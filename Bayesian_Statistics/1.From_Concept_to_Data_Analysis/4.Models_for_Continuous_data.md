# 4. Models for Continuous data

## (1) Exponential data
We have covered about the models for discrete data in the last post. Now we're going to talk about the 'continuous' case. The first one is exponential data.
</br>
</br>
Gamma distribution is conjugate for an exponential likelihood. Actually, gammas are conjugate for a number of different things.
Let's proove why gamma distribution is conjugate for an exponential likelihood.
</br>
</br>
Y follows an exponential distribution with parameter lambda.
</br>
</br>
<a href="https://www.codecogs.com/eqnedit.php?latex=Y&space;\sim&space;Exp(\lambda)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?Y&space;\sim&space;Exp(\lambda)" title="Y \sim Exp(\lambda)" /></a>
</br>
</br>
Let the prior distribution follow a gamma distribution
</br>
</br>
<a href="https://www.codecogs.com/eqnedit.php?latex=f(\lambda)&space;\sim&space;\tau(\alpha,\beta)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?f(\lambda)&space;\sim&space;\tau(\alpha,\beta)" title="f(\lambda) \sim \tau(\alpha,\beta)" /></a>
</br>
</br>
And the posterior distribution would look like this.
</br>
</br>
<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{align*}&space;f(\lambda|y)&\propto&space;f(y|\lambda)f(\lambda)\\&space;&\propto&space;\lambda&space;e^{-\lambda&space;y}\lambda^{\alpha-1}e^{-\beta\lambda}\\&space;&\lambda^{(a&plus;1)-1}e^{-(\beta&plus;y)\lambda}&space;\end{align*}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{align*}&space;f(\lambda|y)&\propto&space;f(y|\lambda)f(\lambda)\\&space;&\propto&space;\lambda&space;e^{-\lambda&space;y}\lambda^{\alpha-1}e^{-\beta\lambda}\\&space;&\lambda^{(a&plus;1)-1}e^{-(\beta&plus;y)\lambda}&space;\end{align*}" title="\begin{align*} f(\lambda|y)&\propto f(y|\lambda)f(\lambda)\\ &\propto \lambda e^{-\lambda y}\lambda^{\alpha-1}e^{-\beta\lambda}\\ &\lambda^{(a+1)-1}e^{-(\beta+y)\lambda} \end{align*}" /></a>
</br>
</br>
<a href="https://www.codecogs.com/eqnedit.php?latex=\lambda|y&space;\sim&space;\tau(\alpha&plus;1,\beta&plus;y)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\lambda|y&space;\sim&space;\tau(\alpha&plus;1,\beta&plus;y)" title="\lambda|y \sim \tau(\alpha+1,\beta+y)" /></a>
</br>
</br>
We can see that the posterior distribution also follows a gamma distribution.
</br>
</br>
### Example.
Let's put this into an example with a time waiting a bus. Say that the prior follows a distribution Gamma(100,1000), which means taht the prior mean is 1/10 ( usually wait 10 minutes for a bus to arrive ). And say we're looking at plus or minus 0.02 as a possible range for this parameter ( 0.1-0.02 ~ 0.1+0.02 ). Suppose we have just waited for 12 minutes for a bus to arrive. If we put this result in to the equation above and calculate the posterior, the posterior distribution will follow (100+1,1000+12). As a result, the posterior mean will be 101/1012, which is almost 1/10.02 . We can say that this one data does not have a big impact in here.
</br>
</br>

## (2) Normal data
Normal distribution is conjugate for itself! I will not make a proof of this, since it can be prooved as the same way as above.
</br>

### a. Normal likelihood with variance KNOWN
Let's talk about the case when the variance of the normal likelihood is 'known'.</br>
Suppose the Xi's follow the normal distribution like below
</br>
</br>
<a href="https://www.codecogs.com/eqnedit.php?latex=X_i&space;\sim&space;N(\mu,&space;\sigma_{0}^2)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?X_i&space;\sim&space;N(\mu,&space;\sigma_{0}^2)" title="X_i \sim N(\mu, \sigma_{0}^2)" /></a>
</br>
</br>
And let the prior distribution follow a normal distribution, with a mean m0 and standard deviation s0.
</br>
</br>
<a href="https://www.codecogs.com/eqnedit.php?latex=\mu&space;\sim&space;N(m_0,&space;s_0^2)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mu&space;\sim&space;N(m_0,&space;s_0^2)" title="\mu \sim N(m_0, s_0^2)" /></a>
</br>
</br>
Then the posterior distribution would look like this. 
</br>
</br>
<a href="https://www.codecogs.com/eqnedit.php?latex=f(\mu|x)&space;\sim&space;f(x|\mu)f(\mu)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?f(\mu|x)&space;\sim&space;f(x|\mu)f(\mu)" title="f(\mu|x) \sim f(x|\mu)f(\mu)" /></a>
</br>
<a href="https://www.codecogs.com/eqnedit.php?latex=\mu|x&space;\sim&space;N(\frac{\frac{n\bar{x}}{\sigma_{0}^2}&plus;\frac{m_o}{s_o^2}}{\frac{n}{\sigma_o^2}&plus;\frac{1}{s_o^2}},\frac{1}{\frac{n}{\sigma_o^2}&plus;\frac{1}{s_o^2}})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mu|x&space;\sim&space;N(\frac{\frac{n\bar{x}}{\sigma_{0}^2}&plus;\frac{m_o}{s_o^2}}{\frac{n}{\sigma_o^2}&plus;\frac{1}{s_o^2}},\frac{1}{\frac{n}{\sigma_o^2}&plus;\frac{1}{s_o^2}})" title="\mu|x \sim N(\frac{\frac{n\bar{x}}{\sigma_{0}^2}+\frac{m_o}{s_o^2}}{\frac{n}{\sigma_o^2}+\frac{1}{s_o^2}},\frac{1}{\frac{n}{\sigma_o^2}+\frac{1}{s_o^2}})" /></a>
</br>
</br>
As a result, the posterior mean would look like this. This is an weighted average of the 'prior mean' and the 'data mean'. And the effective sample size for this prior is the ratio of the vairance of the data, to the variance of the prior.
</br>
</br>
<a href="https://www.codecogs.com/eqnedit.php?latex=\frac{n}{n&plus;\frac{\sigma_o^2}{s_o^2}}\bar{x}&space;&plus;&space;\frac{\frac{\sigma_o^2}{s_o^2}}{n&plus;\frac{\sigma_o^2}{s_o^2}}m_o" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{n}{n&plus;\frac{\sigma_o^2}{s_o^2}}\bar{x}&space;&plus;&space;\frac{\frac{\sigma_o^2}{s_o^2}}{n&plus;\frac{\sigma_o^2}{s_o^2}}m_o" title="\frac{n}{n+\frac{\sigma_o^2}{s_o^2}}\bar{x} + \frac{\frac{\sigma_o^2}{s_o^2}}{n+\frac{\sigma_o^2}{s_o^2}}m_o" /></a>
</br>
</br>
We can notice that as the variance of the prior gets larger, the less weight it gets(less information in it), and the smaller variance it has, the more weight it gets(more information in it). It makes sense!


</br>

### b. Normal likelihood with variance UNKNOWN
