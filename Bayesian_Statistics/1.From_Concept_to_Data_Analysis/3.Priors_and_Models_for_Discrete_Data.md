# 3. Prior and Models for Discrete Data

## (1) Priors
We have seen that we can use prior belief when calculating the posterior probability in Bayesian Inference. So, how do we choose a prior?
A useful concept in terms of choosing priors is that of calibration.

Let's take an example. (ref : https://www.coursera.org/learn/bayesian-statistics )</br>
Suppose you are tasked with eliciting a prior distribution for θ, the proportion of taxpayers who file their returns after the deadline. After speaking with several tax experts, but before collecting data, you are reasonably confident that \thetaθ is greater than 0.05, but less than 0.20.
</br>
<img src="https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/Crg8OhhdEea3RQoRNEpMkw_3b9af2a12375baf616035c778dd5f719_l6.1_ivq1_a.svg?expiry=1582934400000&hmac=mOcllYOWUlribu1TS-ssxW8UzFjjOi7aFqdKIy66fIo" width="550" /> </br>
The prior distribution above most accurately reflects theses prior beliefs about θ. This prior assigns approximately 95% of the prior probability to the interval (0.05, 0.20). It is a strongly informative prior, but it is consistent with our prior beliefs.
</br>

### a) prior predictive distribution
prior predictive distribution is a distribution of data, which we think wi will obtain before actually see the data. For example, if we believe the "coin is a fair coin" and toss the coin 100 times, then the prior distribution of coming up heads(tails) would look like this.
</br>
<img src="https://qph.fs.quoracdn.net/main-qimg-c7fcbfcbf859ecb148cdfe8dcf436be3" width="550" /> </br>
https://qph.fs.quoracdn.net/main-qimg-c7fcbfcbf859ecb148cdfe8dcf436be3
</br>
And this would be the equation how it would look like.
</br>
<a href="https://www.codecogs.com/eqnedit.php?latex=P(y)&space;=&space;\int&space;P(y,\theta)d\theta&space;=&space;\int&space;P(y|\theta)\times&space;P(\theta)d\theta" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(y)&space;=&space;\int&space;P(y,\theta)d\theta&space;=&space;\int&space;P(y|\theta)\times&space;P(\theta)d\theta" title="P(y) = \int P(y,\theta)d\theta = \int P(y|\theta)\times P(\theta)d\theta" /></a>


### b) posterior predictive distribution
posterior predictive distribution is a distribution of data we would expect to obtain if we repeat the experiment after we have seen some data from the current experiment. For example, we have tossed a coin several times as an experiment, and only 3 times out of 10 times had the coin came up with head. Then, we are more likely to have a distribution with a bit right-skewed.
</br>
<img src="https://i.stack.imgur.com/YCbPX.png" width="550" /> </br>
https://i.stack.imgur.com/YCbPX.png
</br>
And this would be the equation how it would look like.
</br>
<a href="https://www.codecogs.com/eqnedit.php?latex=P(y'|y)&space;=&space;\int&space;P(y',\theta|y)d\theta&space;=&space;\int&space;P(y'|\theta,y)\timesP(\theta\y)d\theta" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(y'|y)&space;=&space;\int&space;P(y',\theta|y)d\theta&space;=&space;\int&space;P(y'|\theta,y)\timesP(\theta\y)d\theta" title="P(y'|y) = \int P(y',\theta|y)d\theta = \int P(y'|\theta,y)\timesP(\theta\y)d\theta" /></a>
