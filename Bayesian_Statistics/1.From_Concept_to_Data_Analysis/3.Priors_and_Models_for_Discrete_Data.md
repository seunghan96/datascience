# 3. Prior and Models for Discrete Data

## (1) Priors
We have seen that we can use prior belief when calculating the posterior probability in Bayesian Inference. So, how do we choose a prior?
A useful concept in terms of choosing priors is that of calibration.

Let's take an example. (ref : https://www.coursera.org/learn/bayesian-statistics )</br>
Suppose you are tasked with eliciting a prior distribution for θ, the proportion of taxpayers who file their returns after the deadline. After speaking with several tax experts, but before collecting data, you are reasonably confident that \thetaθ is greater than 0.05, but less than 0.20.
</br>
<img src="https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/Crg8OhhdEea3RQoRNEpMkw_3b9af2a12375baf616035c778dd5f719_l6.1_ivq1_a.svg?expiry=1582934400000&hmac=mOcllYOWUlribu1TS-ssxW8UzFjjOi7aFqdKIy66fIo" width="450" /> </br>
The prior distribution above most accurately reflects theses prior beliefs about θ. This prior assigns approximately 95% of the prior probability to the interval (0.05, 0.20). It is a strongly informative prior, but it is consistent with our prior beliefs.
</br>
</br>

### a) prior predictive distribution
prior predictive distribution is a distribution of data, which we think wi will obtain before actually see the data. For example, if we believe the "coin is a fair coin" and toss the coin 100 times, then the prior distribution of coming up heads(tails) would look like this.
</br>
</br>
<img src="https://qph.fs.quoracdn.net/main-qimg-c7fcbfcbf859ecb148cdfe8dcf436be3" width="450" /> </br>
https://qph.fs.quoracdn.net/main-qimg-c7fcbfcbf859ecb148cdfe8dcf436be3
</br>
</br>
And this would be the equation how it would look like.
</br>
<a href="https://www.codecogs.com/eqnedit.php?latex=P(y)&space;=&space;\int&space;P(y,\theta)d\theta&space;=&space;\int&space;P(y|\theta)\times&space;P(\theta)d\theta" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(y)&space;=&space;\int&space;P(y,\theta)d\theta&space;=&space;\int&space;P(y|\theta)\times&space;P(\theta)d\theta" title="P(y) = \int P(y,\theta)d\theta = \int P(y|\theta)\times P(\theta)d\theta" /></a>
</br>
</br>

### b) posterior predictive distribution
posterior predictive distribution is a distribution of data we would expect to obtain if we repeat the experiment after we have seen some data from the current experiment. For example, we have tossed a coin several times as an experiment, and only 3 times out of 10 times had the coin came up with head. Then, we are more likely to have a distribution with a bit right-skewed.
</br>
<img src="https://i.stack.imgur.com/YCbPX.png" width="400" /> </br>
https://i.stack.imgur.com/YCbPX.png
</br>
</br>
And this would be the equation how it would look like.
</br>
</br>
<a href="https://www.codecogs.com/eqnedit.php?latex=P(y'|y)&space;=&space;\int&space;P(y',\theta|y)d\theta&space;=&space;\int&space;P(y'|\theta,y)\timesP(\theta\y)d\theta" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(y'|y)&space;=&space;\int&space;P(y',\theta|y)d\theta&space;=&space;\int&space;P(y'|\theta,y)\timesP(\theta\y)d\theta" title="P(y'|y) = \int P(y',\theta|y)d\theta = \int P(y'|\theta,y)\timesP(\theta\y)d\theta" /></a>
</br>
</br>
</br>

## (2) Bernoulli / Binomial data
### a) Bernoulli / binomial likelihood with uniform prior
We get a beta posterior when we use uniform prior for Bernoulli likelihood.
</br>
The Bernoulli likelihood is 
<a href="https://www.codecogs.com/eqnedit.php?latex=f(y|\theta)&space;=&space;\theta^{\sum&space;y_i}&space;(1-\theta)^{n-\sum&space;y_i}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?f(y|\theta)&space;=&space;\theta^{\sum&space;y_i}&space;(1-\theta)^{n-\sum&space;y_i}" title="f(y|\theta) = \theta^{\sum y_i} (1-\theta)^{n-\sum y_i}" /></a>,
</br>
and if it has an uniform prior, we can say that
<a href="https://www.codecogs.com/eqnedit.php?latex=f(\theta)&space;=&space;I_{{0\leq&space;\theta\leq&space;1}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?f(\theta)&space;=&space;I_{{0\leq&space;\theta\leq&space;1}}" title="f(\theta) = I_{{0\leq \theta\leq 1}}" /></a>
</br>
</br>
With these two, we can calculate the posterior probability. And this is how it will look like.
</br>
</br>
<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{align*}&space;f(\theta|y)&=\frac{f(y|\theta)f(\theta)}{\int&space;f(y|\theta)f(\theta)d\theta}\\&space;&=\frac&space;{\theta^{\sum&space;y_i}(1-\theta)^{n-\sum&space;y_i}I_{0\leq&space;\theta\leq&space;1}}{\int_{0}^{1}\theta^{\sum&space;y_i}(1-\theta)^{n-\sum&space;y_i}I_{0\leq&space;\theta\leq&space;1}d\theta}\\&space;&=\frac{\tau&space;(n&plus;2)}{\tau&space;(\sum&space;y_i&space;&plus;1)\tau&space;(n-&space;\sum&space;y_i&space;&plus;1)}\theta^{\sum&space;y_i}(1-\theta)^{n-\sum&space;y_i}I_{{{0\leq&space;\theta\leq&space;1}}}&space;\end{align*}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{align*}&space;f(\theta|y)&=\frac{f(y|\theta)f(\theta)}{\int&space;f(y|\theta)f(\theta)d\theta}\\&space;&=\frac&space;{\theta^{\sum&space;y_i}(1-\theta)^{n-\sum&space;y_i}I_{0\leq&space;\theta\leq&space;1}}{\int_{0}^{1}\theta^{\sum&space;y_i}(1-\theta)^{n-\sum&space;y_i}I_{0\leq&space;\theta\leq&space;1}d\theta}\\&space;&=\frac{\tau&space;(n&plus;2)}{\tau&space;(\sum&space;y_i&space;&plus;1)\tau&space;(n-&space;\sum&space;y_i&space;&plus;1)}\theta^{\sum&space;y_i}(1-\theta)^{n-\sum&space;y_i}I_{{{0\leq&space;\theta\leq&space;1}}}&space;\end{align*}" title="\begin{align*} f(\theta|y)&=\frac{f(y|\theta)f(\theta)}{\int f(y|\theta)f(\theta)d\theta}\\ &=\frac {\theta^{\sum y_i}(1-\theta)^{n-\sum y_i}I_{0\leq \theta\leq 1}}{\int_{0}^{1}\theta^{\sum y_i}(1-\theta)^{n-\sum y_i}I_{0\leq \theta\leq 1}d\theta}\\ &=\frac{\tau (n+2)}{\tau (\sum y_i +1)\tau (n- \sum y_i +1)}\theta^{\sum y_i}(1-\theta)^{n-\sum y_i}I_{{{0\leq \theta\leq 1}}} \end{align*}" /></a>
</br>
</br>
So we can say that this posterior proability follows the following beta distribution!
</br>
</br>
<a href="https://www.codecogs.com/eqnedit.php?latex=\theta|y&space;~&space;Beta(\sum&space;y_i&plus;1,&space;n-\sum&space;y_i&space;&plus;1)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\theta|y&space;~&space;Beta(\sum&space;y_i&plus;1,&space;n-\sum&space;y_i&space;&plus;1)" title="\theta|y ~ Beta(\sum y_i+1, n-\sum y_i +1)" /></a>
</br>
</br>
</br>

### b) Conjugate Priors
A family of distributions is referred to as conjugate if when you use a member of that family as a prior, you get another member of that family as your posterior.
</br>
For example, any beta distribution is conjuagate for the Bernoulli distribution, which means that any beta prior will give a beta
posterior. The case with the distribution with uniform prior, that we just saw in the above, is just one case of the below. ( uniform distribution is just one case of beta (1,1) )
<img src="https://miro.medium.com/max/3190/1*xjRaB2R2A3aDS8RstiErMQ.png" width="650" /> </br>
https://miro.medium.com/max/3190/1*xjRaB2R2A3aDS8RstiErMQ.png

Why do we use conjuagte priors?
It's because they make the calculation much more simpler. When we are working out with posteriors, and face an intractable integral in the denominator ( hard to recognize the form ), we get difficulty sovling this. That is why we use conjugat families, which allows us to get closed form solutions!
</br>
</br>

### c) Posterior mean and effective sample size
The posterior mean and effective sample size looks like below.
- Bernoulli likelihood
- prior : <a href="https://www.codecogs.com/eqnedit.php?latex=Beta(\alpha,&space;\beta)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?Beta(\alpha,&space;\beta)" title="Beta(\alpha, \beta)" /></a>
- posterior : <a href="https://www.codecogs.com/eqnedit.php?latex=Beta(\alpha&space;&plus;&space;\sum&space;y_i,&space;\beta&space;&plus;&space;n&space;-&space;\sum&space;y_i)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?Beta(\alpha&space;&plus;&space;\sum&space;y_i,&space;\beta&space;&plus;&space;n&space;-&space;\sum&space;y_i)" title="Beta(\alpha + \sum y_i, \beta + n - \sum y_i)" /></a>

The mean of prior beta is <a href="https://www.codecogs.com/eqnedit.php?latex=\frac{\alpha}{\alpha&space;&plus;&space;\beta}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{\alpha}{\alpha&space;&plus;&space;\beta}" title="\frac{\alpha}{\alpha + \beta}" /></a>, </br>
</br>

and the mean of posterior is 
</br>

<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{align*}&space;\frac{\alpha&plus;\sum&space;y_i}{\alpha&space;&plus;\sum&space;y_i&plus;&space;\beta&plus;n-\sum&space;y_i}&space;&=&space;\frac{\alpha&plus;\beta}{\alpha&plus;\beta&plus;n}\cdot&space;\frac{\alpha}{\alpha&plus;\beta}&plus;\frac{n}{\alpha&plus;\beta&plus;n}\cdot&space;\frac{\sum&space;y_i}{n}\\&space;&=prior\;weight&space;\times&space;prior\;mean&space;&plus;&space;data\;weight\times&space;data\;mean&space;\end{align*}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{align*}&space;\frac{\alpha&plus;\sum&space;y_i}{\alpha&space;&plus;\sum&space;y_i&plus;&space;\beta&plus;n-\sum&space;y_i}&space;&=&space;\frac{\alpha&plus;\beta}{\alpha&plus;\beta&plus;n}\cdot&space;\frac{\alpha}{\alpha&plus;\beta}&plus;\frac{n}{\alpha&plus;\beta&plus;n}\cdot&space;\frac{\sum&space;y_i}{n}\\&space;&=prior\;weight&space;\times&space;prior\;mean&space;&plus;&space;data\;weight\times&space;data\;mean&space;\end{align*}" title="\begin{align*} \frac{\alpha+\sum y_i}{\alpha +\sum y_i+ \beta+n-\sum y_i} &= \frac{\alpha+\beta}{\alpha+\beta+n}\cdot \frac{\alpha}{\alpha+\beta}+\frac{n}{\alpha+\beta+n}\cdot \frac{\sum y_i}{n}\\ &=prior\;weight \times prior\;mean + data\;weight\times data\;mean \end{align*}" /></a>
</br>
</br>
According to the equation above, we can say the effective sample size of the prior for beta prior on Bernoulli(or binomial) likelihood is alpha + beta. This size gives you an idea of how much data is needed to make sure that the prior doesn't have much influence on your posterior. ( depends on the relative size difference between 'alpha+beta' and 'n'. see the numerator of prior weight & data weight )
</br>
</br>
It also implies that sequential anlaysis is possible. Let's say we used 1 to n data as an prior today. After more observation tomorrow, we can use the new data n+1 to n+m as a new prior. The prior becomes 1~n+m. We can make an update like this every time we get a new data! We can just use our 'previous posterior' as a 'new prior', using Bayes' Theorem.
So in the Bayesian paradigm, this consistent whether we're doing sequential updates or a whole batch update. This is impossible in the case of frequentists paradigm.
</br>
</br>

## (3) Poisson data
We have seen that any beta distribution is conjuagate for the Bernoulli distribution. So, what distribution is conjugate for Poisson distribution? The answer is "Gamma distribution".

### a) Conjugate Priors
This is a proof, why gamma distribution is conjugate for the Poisson distribution, which means that any gamma prior will give a gamma posterior.
</br>
<img src="https://slideplayer.com/slide/5100086/16/images/9/Using+conjugate+priors+%28cont.%29.jpg" width="650" /> </br>
https://slideplayer.com/slide/5100086/16/images/9/Using+conjugate+priors+%28cont.%29.jpg
</br>
</br>

### b) Posterior mean and effective sample size
The posterior mean and effective sample size looks like below.
- Poisson likelihood
- prior : <a href="https://www.codecogs.com/eqnedit.php?latex=\tau&space;(\alpha,\beta)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\tau&space;(\alpha,\beta)" title="\tau (\alpha,\beta)" /></a>
- posterior : <a href="https://www.codecogs.com/eqnedit.php?latex=\tau&space;(\alpha&plus;\sum&space;y_i,\beta&plus;n)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\tau&space;(\alpha&plus;\sum&space;y_i,\beta&plus;n)" title="\tau (\alpha+\sum y_i,\beta+n)" /></a>

The mean of prior gamma is </br> 
<a href="https://www.codecogs.com/eqnedit.php?latex=\frac&space;{\alpha}{\beta}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac&space;{\alpha}{\beta}" title="\frac {\alpha}{\beta}" /></a>
</br>
and the mean of posterior gamma is 
</br><a href="https://www.codecogs.com/eqnedit.php?latex=\begin{align*}&space;\frac&space;{\alpha&space;&plus;&space;\sum&space;y_i}{\beta&space;&plus;n}&space;&=&space;\frac{\beta}{\beta&plus;n}\cdot\frac{\alpha}{\beta}&plus;\frac{n}{\beta&plus;n}\cdot\frac{\sum&space;y_i}{n}\\&space;&=prior\;weight&space;\times&space;prior\;mean&space;&plus;&space;data\;weight\times&space;data\;mean&space;\end{align*}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{align*}&space;\frac&space;{\alpha&space;&plus;&space;\sum&space;y_i}{\beta&space;&plus;n}&space;&=&space;\frac{\beta}{\beta&plus;n}\cdot\frac{\alpha}{\beta}&plus;\frac{n}{\beta&plus;n}\cdot\frac{\sum&space;y_i}{n}\\&space;&=prior\;weight&space;\times&space;prior\;mean&space;&plus;&space;data\;weight\times&space;data\;mean&space;\end{align*}" title="\begin{align*} \frac {\alpha + \sum y_i}{\beta +n} &= \frac{\beta}{\beta+n}\cdot\frac{\alpha}{\beta}+\frac{n}{\beta+n}\cdot\frac{\sum y_i}{n}\\ &=prior\;weight \times prior\;mean + data\;weight\times data\;mean \end{align*}" /></a>
</br>
</br>
According to the equation above, we can say the effective sample size of the prior for gamma prior on poisson likelihood is beta. This size gives you an idea of how much data is needed to make sure that the prior doesn't have much influence on your posterior. ( depends on the relative size difference between 'beta' and 'n'. see the numerator of prior weight & data weight )
