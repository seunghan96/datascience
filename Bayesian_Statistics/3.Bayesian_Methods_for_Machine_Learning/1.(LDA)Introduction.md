# LDA ( Latent Dirichlet Allocation)
## 1) Introduction

### a. Topic Modeling
<img src="https://miro.medium.com/max/2796/1*jpytbqadO3FtdIyOjx2_yg.png" width="550" /> </br>
https://miro.medium.com/max/2796/1*jpytbqadO3FtdIyOjx2_yg.png
</br>

A topic model is a type of statistical model for discovering the abstract "topics" that occur in a collection of documents. 
Topic modeling is a frequently used text-mining tool for discovery of hidden semantic structures in a text body. (wikipedia)
</br>

Topic modeling treats document as a 'distribution' of topics. For example, let's take a famous book 'The Adventures of Sherlock Homes'. We can say
that this book is composed of three topics, (60%)detective + (30%)adventure + (10%)horror. We can make this as a vector form like (0.6, 0.3, 0.1).
Then if an another book called "Sherlock Homes and his friends" has a vector form (0.5,0.2,0.3), 
it means that this book is composed of (50%)detective + (20%)adventure + (30%)horror.
</br>
</br>

### b. Similarity & Distance
After we have found the distribution of topic for some books or documnets (as a vector form), we can measure how those two books are similar or different in the aspect of topic.
The commonly used measures for similarity & distance are 'Euclidean distance' and 'Cosine similarity'. I will not cover about them, as you might all know.
</br>

#### Euclidean Distance
<img src="https://i.stack.imgur.com/RtnTY.jpg" width="550" /> </br>
https://i.stack.imgur.com/RtnTY.jpg
</br>

#### Cosine Similarity
<img src="https://miro.medium.com/max/1900/1*pZMivuSRUnOhp2iu7g-RTg.png" width="550" /> </br>
https://miro.medium.com/max/1900/1*pZMivuSRUnOhp2iu7g-RTg.png
</br>
</br>

### c. Dirichlet Distribution
#### [ Intro ]
It is a distribution over vector theta, and is parameterized by parameter alpha (which is also a vector). It has a form like this.
</br>
<a href="https://www.codecogs.com/eqnedit.php?latex=Dir(\theta|\alpha)&space;=&space;\frac{1}{B(\alpha)}\prod_{i=1}^{K}\;&space;\theta_k^{\alpha_{k}-1}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?Dir(\theta|\alpha)&space;=&space;\frac{1}{B(\alpha)}\prod_{i=1}^{K}\;&space;\theta_k^{\alpha_{k}-1}" title="Dir(\theta|\alpha) = \frac{1}{B(\alpha)}\prod_{i=1}^{K}\; \theta_k^{\alpha_{k}-1}" /></a>
</br>
The constraints are as below
- <a href="https://www.codecogs.com/eqnedit.php?latex=\sum_{k}\theta_k&space;=&space;1" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\sum_{k}\theta_k&space;=&space;1" title="\sum_{k}\theta_k = 1" /></a>
- <a href="https://www.codecogs.com/eqnedit.php?latex=\theta_k&space;\geq&space;0" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\theta_k&space;\geq&space;0" title="\theta_k \geq 0" /></a>
</br>

One easy way to interpert this is a traingle like below. (in the case of 3-dim vector )
<img src="https://miro.medium.com/max/1163/1*Pepqn_v-WZC9iJXtyA-tQQ.png" width="550" /> </br>
https://miro.medium.com/max/1163/1*Pepqn_v-WZC9iJXtyA-tQQ.png

#### [ Statistics ]
- Mean </br></br>
<a href="https://www.codecogs.com/eqnedit.php?latex=E[\theta_i]&space;=&space;\frac{\alpha_i}{\alpha_0}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?E[\theta_i]&space;=&space;\frac{\alpha_i}{\alpha_0}" title="E[\theta_i] = \frac{\alpha_i}{\alpha_0}" /></a> </br> </br>
<a href="https://www.codecogs.com/eqnedit.php?latex=\alpha_0&space;=&space;\sum_{k=1}^{K}\alpha_k" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\alpha_0&space;=&space;\sum_{k=1}^{K}\alpha_k" title="\alpha_0 = \sum_{k=1}^{K}\alpha_k" /></a>

- Covariance </br></br>
<a href="https://www.codecogs.com/eqnedit.php?latex=Cov(\theta_i,\theta_j)&space;=&space;\frac{\alpha_i\;\alpha_0[i=j]-\alpha_i\;&space;\alpha_j}{\alpha_0^2(\alpha_0&plus;1)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?Cov(\theta_i,\theta_j)&space;=&space;\frac{\alpha_i\;\alpha_0[i=j]-\alpha_i\;&space;\alpha_j}{\alpha_0^2(\alpha_0&plus;1)}" title="Cov(\theta_i,\theta_j) = \frac{\alpha_i\;\alpha_0[i=j]-\alpha_i\; \alpha_j}{\alpha_0^2(\alpha_0+1)}" /></a>

- note! when k=2, it is same as 'beta distribution'

- Dirichlet prior is a conjugate to the 'multinomial likelihood' </br></br>
<a href="https://www.codecogs.com/eqnedit.php?latex=P(X|\theta)&space;=&space;\frac{n!}{x_1!..x_k!}\theta_1^{x_1}...\theta_l^{x_k}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(X|\theta)&space;=&space;\frac{n!}{x_1!..x_k!}\theta_1^{x_1}...\theta_l^{x_k}" title="P(X|\theta) = \frac{n!}{x_1!..x_k!}\theta_1^{x_1}...\theta_l^{x_k}" /></a> </br> </br>

<a href="https://www.codecogs.com/eqnedit.php?latex=p(\theta)=Dir(\theta|\alpha)&space;=&space;\frac{1}{B(\alpha)}\prod_{i=1}^{K}\;&space;\theta_k^{\alpha_{k}-1}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?p(\theta)=Dir(\theta|\alpha)&space;=&space;\frac{1}{B(\alpha)}\prod_{i=1}^{K}\;&space;\theta_k^{\alpha_{k}-1}" title="p(\theta)=Dir(\theta|\alpha) = \frac{1}{B(\alpha)}\prod_{i=1}^{K}\; \theta_k^{\alpha_{k}-1}" /></a> </br> </br>

<a href="https://www.codecogs.com/eqnedit.php?latex=p(\theta|X)&space;\propto&space;\prod_{k=1}^{K}\theta_k^{\alpha_k&space;&plus;&space;x_k&space;-1}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?p(\theta|X)&space;\propto&space;\prod_{k=1}^{K}\theta_k^{\alpha_k&space;&plus;&space;x_k&space;-1}" title="p(\theta|X) \propto \prod_{k=1}^{K}\theta_k^{\alpha_k + x_k -1}" /></a> </br> </br>

<a href="https://www.codecogs.com/eqnedit.php?latex=p(\theta|X)&space;=&space;Dir(\theta|\begin{pmatrix}&space;...\\&space;\alpha_k&space;&plus;&space;x_k\\&space;...&space;\end{pmatrix})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?p(\theta|X)&space;=&space;Dir(\theta|\begin{pmatrix}&space;...\\&space;\alpha_k&space;&plus;&space;x_k\\&space;...&space;\end{pmatrix})" title="p(\theta|X) = Dir(\theta|\begin{pmatrix} ...\\ \alpha_k + x_k\\ ... \end{pmatrix})" /></a> </br> </br>
