# Variational Inferece

## 2. Variational EM

### a. EM : reminder
We try to maximize the (marginal) log likelihood. To do this, we derive a variational lower bound (=L(theta,q)) and try to maximize this lower bound. We do this on an iterative way, with E step and M step.
</br>

<a href="https://www.codecogs.com/eqnedit.php?latex=logp(X|\theta)&space;\geq&space;L(\theta,q)&space;=&space;E_{q(T)}\;log&space;\frac{p(X,T|\theta)}{q(T)}\;dT" target="_blank"><img src="https://latex.codecogs.com/gif.latex?logp(X|\theta)&space;\geq&space;L(\theta,q)&space;=&space;E_{q(T)}\;log&space;\frac{p(X,T|\theta)}{q(T)}\;dT" title="logp(X|\theta) \geq L(\theta,q) = E_{q(T)}\;log \frac{p(X,T|\theta)}{q(T)}\;dT" /></a>
</br>
#### E-step 
- maximize lower bound ( with respect to q )
- maximization of lower bound = minmizing the KL divergence between "q" & "posterior distribution" </br> </br>
<a href="https://www.codecogs.com/eqnedit.php?latex=L(\theta,q)&space;\rightarrow&space;\underset{q}{max}&space;\;\;&space;\Leftrightarrow&space;\;\;&space;KL[q(T)||p(T|X,\theta)]&space;\rightarrow&space;\underset{q}{min}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?L(\theta,q)&space;\rightarrow&space;\underset{q}{max}&space;\;\;&space;\Leftrightarrow&space;\;\;&space;KL[q(T)||p(T|X,\theta)]&space;\rightarrow&space;\underset{q}{min}" title="L(\theta,q) \rightarrow \underset{q}{max} \;\; \Leftrightarrow \;\; KL[q(T)||p(T|X,\theta)] \rightarrow \underset{q}{min}" /></a>

#### M-step
- maximize the expected value of logarithm of the joint distribution  </br> </br>
<a href="https://www.codecogs.com/eqnedit.php?latex=L(\theta,q)&space;\rightarrow&space;\underset{\theta}{max}&space;\;\;&space;\Leftrightarrow&space;\;\;&space;E_{q(T)}logp(X,T|\theta)&space;\rightarrow&space;\underset{\theta}{min}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?L(\theta,q)&space;\rightarrow&space;\underset{\theta}{max}&space;\;\;&space;\Leftrightarrow&space;\;\;&space;E_{q(T)}logp(X,T|\theta)&space;\rightarrow&space;\underset{\theta}{min}" title="L(\theta,q) \rightarrow \underset{\theta}{max} \;\; \Leftrightarrow \;\; E_{q(T)}logp(X,T|\theta) \rightarrow \underset{\theta}{min}" /></a>
</br>

### b. E-step
we could get q(t) like the below, using the full posterior.  </br> </br>
<a href="https://www.codecogs.com/eqnedit.php?latex=q(T)&space;=&space;p(T|X,\theta)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?q(T)&space;=&space;p(T|X,\theta)" title="q(T) = p(T|X,\theta)" /></a>
<br>

But for many cases, we can not compute for posterior exactly. So if we use variational Inference in this E-step part, then it will be much easier to compute! </br> </br>
<a href="https://www.codecogs.com/eqnedit.php?latex=KL[q(T)||p(T|X,\theta)]&space;\rightarrow&space;\underset{q\in&space;Q}{min}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?KL[q(T)||p(T|X,\theta)]&space;\rightarrow&space;\underset{q\in&space;Q}{min}" title="KL[q(T)||p(T|X,\theta)] \rightarrow \underset{q\in Q}{min}" /></a>
</br>
<img src="https://www.researchgate.net/publication/257870092/figure/fig2/AS:297341622996994@1447903175594/The-variational-E-M-algorithm.png" width="550" /> </br>
https://www.researchgate.net/publication/257870092

### c. Summary
Let's compare the accuracy and the speed of diverse methods.

Accuracy 
- Full Inference > Mean Field > EM algorithm > Variational EM
</br>

Speed 
- Full Inference < Mean Field < EM algorithm < Variational EM
