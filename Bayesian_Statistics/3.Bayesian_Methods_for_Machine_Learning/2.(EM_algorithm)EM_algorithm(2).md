# EM Algorithm 
## 3) EM algorithm (2)
We have seen that EM algorithm consists of two parts.
</br>

<img src="https://stillbreeze.github.io/images/em.jpg" width="550" /> </br>
https://stillbreeze.github.io/images/em.jpg
</br>

- E-step : fix theta, maximize with respect to q ,maximize the lower bound with respect to q </br> </br>
<a href="https://www.codecogs.com/eqnedit.php?latex=q^{k&plus;1}&space;=&space;\underset{q}{argmax}L(\theta^k,q)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?q^{k&plus;1}&space;=&space;\underset{q}{argmax}L(\theta^k,q)" title="q^{k+1} = \underset{q}{argmax}L(\theta^k,q)" /></a>

- M-step : fix q, maximize the lower bound with respect to theta </br> </br>
<a href="https://www.codecogs.com/eqnedit.php?latex=\theta^{k&plus;1}&space;=&space;\underset{\theta}{argmax}L(\theta,q^{k&plus;1})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\theta^{k&plus;1}&space;=&space;\underset{\theta}{argmax}L(\theta,q^{k&plus;1})" title="\theta^{k+1} = \underset{\theta}{argmax}L(\theta,q^{k+1})" /></a>

Let's talk about each step more in detal
</br>

### a. E-step (in EM algorithm)
It tries to maximize the lower bound with respect to q ( while fixing theta ). That is same as minimizing the "gap between lower bound and log likelihood". The gap between these two can be expressed as a 'KL-divergence'.
</br>
</br>

<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{align*}&space;GAP&space;&=&space;logp(X|\theta)-L(\theta,q)&space;\\&space;&=&space;\sum_{i=1}^{N}logp(X_i|\theta)-\sum_{i=1}^{N}\sum_{c=1}^{3}q(t_i=c)log\frac{p(X_i,t_i=c|\theta)}{q(t_i=c)}\\&space;&=&space;\sum_{i=1}^{N}(logp(X_i|\theta)\sum_{c=1}^{3}q(t_i=c)-\sum_{c=1}^{3}q(t_i=c)log\frac{p(X_i,t_i=c|\theta)}{q(t_i=c)}\\&space;&=\sum_{i=1}^{N}\sum_{c=1}^{3}q(t_i=c)(logp(X_i|\theta)-log\frac{p(X_i,t_i=c|\theta)}{q(t_i=c)})\\&space;&=\sum_{i=1}^{N}\sum_{c=1}^{3}q(t_i=c)log\frac{p(X_i|\theta)q(t_i=c)}{p(X_i,t_i=c|\theta)}&space;\\&space;&=\sum_{i=1}^{N}\sum_{c=1}^{3}q(t_i=c)log\frac{q(t_i=c)}{p(t_i=c|X_i,\theta)}\\&space;&=KL(q(t_i)||p(t_i=c,X_i,\theta)))&space;\end{align*}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{align*}&space;GAP&space;&=&space;logp(X|\theta)-L(\theta,q)&space;\\&space;&=&space;\sum_{i=1}^{N}logp(X_i|\theta)-\sum_{i=1}^{N}\sum_{c=1}^{3}q(t_i=c)log\frac{p(X_i,t_i=c|\theta)}{q(t_i=c)}\\&space;&=&space;\sum_{i=1}^{N}(logp(X_i|\theta)\sum_{c=1}^{3}q(t_i=c)-\sum_{c=1}^{3}q(t_i=c)log\frac{p(X_i,t_i=c|\theta)}{q(t_i=c)}\\&space;&=\sum_{i=1}^{N}\sum_{c=1}^{3}q(t_i=c)(logp(X_i|\theta)-log\frac{p(X_i,t_i=c|\theta)}{q(t_i=c)})\\&space;&=\sum_{i=1}^{N}\sum_{c=1}^{3}q(t_i=c)log\frac{p(X_i|\theta)q(t_i=c)}{p(X_i,t_i=c|\theta)}&space;\\&space;&=\sum_{i=1}^{N}\sum_{c=1}^{3}q(t_i=c)log\frac{q(t_i=c)}{p(t_i=c|X_i,\theta)}\\&space;&=KL(q(t_i)||p(t_i=c,X_i,\theta)))&space;\end{align*}" title="\begin{align*} GAP &= logp(X|\theta)-L(\theta,q) \\ &= \sum_{i=1}^{N}logp(X_i|\theta)-\sum_{i=1}^{N}\sum_{c=1}^{3}q(t_i=c)log\frac{p(X_i,t_i=c|\theta)}{q(t_i=c)}\\ &= \sum_{i=1}^{N}(logp(X_i|\theta)\sum_{c=1}^{3}q(t_i=c)-\sum_{c=1}^{3}q(t_i=c)log\frac{p(X_i,t_i=c|\theta)}{q(t_i=c)}\\ &=\sum_{i=1}^{N}\sum_{c=1}^{3}q(t_i=c)(logp(X_i|\theta)-log\frac{p(X_i,t_i=c|\theta)}{q(t_i=c)})\\ &=\sum_{i=1}^{N}\sum_{c=1}^{3}q(t_i=c)log\frac{p(X_i|\theta)q(t_i=c)}{p(X_i,t_i=c|\theta)} \\ &=\sum_{i=1}^{N}\sum_{c=1}^{3}q(t_i=c)log\frac{q(t_i=c)}{p(t_i=c|X_i,\theta)}\\ &=KL(q(t_i)||p(t_i=c,X_i,\theta))) \end{align*}" /></a>
</br>

In summary, to maximize the lower bound, we minimize the KL-divergence above.
