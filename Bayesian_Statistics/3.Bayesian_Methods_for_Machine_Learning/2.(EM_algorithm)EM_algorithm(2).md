# EM Algorithm 
## 3) EM algorithm (2)
We have seen that EM algorithm consists of two parts.
</br>

<img src="https://stillbreeze.github.io/images/em.jpg" width="550" /> </br>
https://stillbreeze.github.io/images/em.jpg
</br>

- E-step : fix theta, maximize with respect to q ,maximize the lower bound with respect to q </br> </br>
<a href="https://www.codecogs.com/eqnedit.php?latex=q^{k&plus;1}&space;=&space;\underset{q}{argmax}L(\theta^k,q)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?q^{k&plus;1}&space;=&space;\underset{q}{argmax}L(\theta^k,q)" title="q^{k+1} = \underset{q}{argmax}L(\theta^k,q)" /></a>

- M-step : fix q, maximize the lower bound with respect to theta </br> </br>
<a href="https://www.codecogs.com/eqnedit.php?latex=\theta^{k&plus;1}&space;=&space;\underset{\theta}{argmax}L(\theta,q^{k&plus;1})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\theta^{k&plus;1}&space;=&space;\underset{\theta}{argmax}L(\theta,q^{k&plus;1})" title="\theta^{k+1} = \underset{\theta}{argmax}L(\theta,q^{k+1})" /></a>

Let's talk about each step more in detal
</br>
</br>

### a. E-step (in EM algorithm)
It tries to maximize the lower bound with respect to q ( while fixing theta ). That is same as minimizing the "gap between lower bound and log likelihood". The gap between these two can be expressed as a 'KL-divergence'.
</br>
</br>

<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{align*}&space;GAP&space;&=&space;logp(X|\theta)-L(\theta,q)&space;\\&space;&=&space;\sum_{i=1}^{N}logp(X_i|\theta)-\sum_{i=1}^{N}\sum_{c=1}^{3}q(t_i=c)log\frac{p(X_i,t_i=c|\theta)}{q(t_i=c)}\\&space;&=&space;\sum_{i=1}^{N}(logp(X_i|\theta)\sum_{c=1}^{3}q(t_i=c)-\sum_{c=1}^{3}q(t_i=c)log\frac{p(X_i,t_i=c|\theta)}{q(t_i=c)}\\&space;&=\sum_{i=1}^{N}\sum_{c=1}^{3}q(t_i=c)(logp(X_i|\theta)-log\frac{p(X_i,t_i=c|\theta)}{q(t_i=c)})\\&space;&=\sum_{i=1}^{N}\sum_{c=1}^{3}q(t_i=c)log\frac{p(X_i|\theta)q(t_i=c)}{p(X_i,t_i=c|\theta)}&space;\\&space;&=\sum_{i=1}^{N}\sum_{c=1}^{3}q(t_i=c)log\frac{q(t_i=c)}{p(t_i=c|X_i,\theta)}\\&space;&=KL(q(t_i)||p(t_i=c,X_i,\theta)))&space;\end{align*}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{align*}&space;GAP&space;&=&space;logp(X|\theta)-L(\theta,q)&space;\\&space;&=&space;\sum_{i=1}^{N}logp(X_i|\theta)-\sum_{i=1}^{N}\sum_{c=1}^{3}q(t_i=c)log\frac{p(X_i,t_i=c|\theta)}{q(t_i=c)}\\&space;&=&space;\sum_{i=1}^{N}(logp(X_i|\theta)\sum_{c=1}^{3}q(t_i=c)-\sum_{c=1}^{3}q(t_i=c)log\frac{p(X_i,t_i=c|\theta)}{q(t_i=c)}\\&space;&=\sum_{i=1}^{N}\sum_{c=1}^{3}q(t_i=c)(logp(X_i|\theta)-log\frac{p(X_i,t_i=c|\theta)}{q(t_i=c)})\\&space;&=\sum_{i=1}^{N}\sum_{c=1}^{3}q(t_i=c)log\frac{p(X_i|\theta)q(t_i=c)}{p(X_i,t_i=c|\theta)}&space;\\&space;&=\sum_{i=1}^{N}\sum_{c=1}^{3}q(t_i=c)log\frac{q(t_i=c)}{p(t_i=c|X_i,\theta)}\\&space;&=KL(q(t_i)||p(t_i=c,X_i,\theta)))&space;\end{align*}" title="\begin{align*} GAP &= logp(X|\theta)-L(\theta,q) \\ &= \sum_{i=1}^{N}logp(X_i|\theta)-\sum_{i=1}^{N}\sum_{c=1}^{3}q(t_i=c)log\frac{p(X_i,t_i=c|\theta)}{q(t_i=c)}\\ &= \sum_{i=1}^{N}(logp(X_i|\theta)\sum_{c=1}^{3}q(t_i=c)-\sum_{c=1}^{3}q(t_i=c)log\frac{p(X_i,t_i=c|\theta)}{q(t_i=c)}\\ &=\sum_{i=1}^{N}\sum_{c=1}^{3}q(t_i=c)(logp(X_i|\theta)-log\frac{p(X_i,t_i=c|\theta)}{q(t_i=c)})\\ &=\sum_{i=1}^{N}\sum_{c=1}^{3}q(t_i=c)log\frac{p(X_i|\theta)q(t_i=c)}{p(X_i,t_i=c|\theta)} \\ &=\sum_{i=1}^{N}\sum_{c=1}^{3}q(t_i=c)log\frac{q(t_i=c)}{p(t_i=c|X_i,\theta)}\\ &=KL(q(t_i)||p(t_i=c,X_i,\theta))) \end{align*}" /></a>
</br>

As KL divergence means the difference between the two distributions, we should set q like the below (make two distributions the same! )
</br>
</br>
<a href="https://www.codecogs.com/eqnedit.php?latex=q^{k&plus;1}(t_i)&space;=&space;p(t_i|x_i,\theta^k)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?q^{k&plus;1}(t_i)&space;=&space;p(t_i|x_i,\theta^k)" title="q^{k+1}(t_i) = p(t_i|x_i,\theta^k)" /></a>
</br>

In summary, to maximize the lower bound, we minimize the KL-divergence above.
</br>
</br>

### b. M-step (in EM algorithm)
On M-step, we want to maximize this lower bound with respect to theta. It can be expressed as below.
</br>
</br>

<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{align*}&space;L(\theta,q)&=\sum_{i}\sum_{c}q(t_i=c)log\frac{p(x_i,t_i=c|\theta)}{q(t_i=c)}\\&space;&=\sum_{i}\sum_{c}q(t_i=c)logp(x_i,t_i=c|\theta)&space;-&space;\sum_{i}\sum_{c}q(t_i=c)logq(t_i=c)\\&space;&=E_qlogp(X,T|\theta)&space;&plus;&space;const&space;\end{align*}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{align*}&space;L(\theta,q)&=\sum_{i}\sum_{c}q(t_i=c)log\frac{p(x_i,t_i=c|\theta)}{q(t_i=c)}\\&space;&=\sum_{i}\sum_{c}q(t_i=c)logp(x_i,t_i=c|\theta)&space;-&space;\sum_{i}\sum_{c}q(t_i=c)logq(t_i=c)\\&space;&=E_qlogp(X,T|\theta)&space;&plus;&space;const&space;\end{align*}" title="\begin{align*} L(\theta,q)&=\sum_{i}\sum_{c}q(t_i=c)log\frac{p(x_i,t_i=c|\theta)}{q(t_i=c)}\\ &=\sum_{i}\sum_{c}q(t_i=c)logp(x_i,t_i=c|\theta) - \sum_{i}\sum_{c}q(t_i=c)logq(t_i=c)\\ &=E_qlogp(X,T|\theta) + const \end{align*}" /></a>
</br> 

As you can see, <a href="https://www.codecogs.com/eqnedit.php?latex=E_qlogp(X,T|\theta)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?E_qlogp(X,T|\theta)" title="E_qlogp(X,T|\theta)" /></a> is (usually) a concave function with respect to theta, which is easy to optimize!
</br>
</br>

### c. Summary of EM algorithm 
[1] method for training Latent Variable Models 
</br>

[2] Handles missing data 
</br>

[3] sequence of simple tasks instead of one hard task 
</br>

[4] guarantees to converge ( local or global ) 
</br>

[5] helps with complicated parameter constraints
</br>

[4] E & M step </br>
In E-step, we maximize the lower bound with respect to q, and in the M-step, we maximize the lower bound with respect to theta. The mathematical expression will be like the below.
</br>
</br>
- E step : <a href="https://www.codecogs.com/eqnedit.php?latex=q^{k&plus;1}(t_i)&space;=&space;p(t_i|x_i,\theta^k)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?q^{k&plus;1}(t_i)&space;=&space;p(t_i|x_i,\theta^k)" title="q^{k+1}(t_i) = p(t_i|x_i,\theta^k)" /></a>
</br>

- M step : <a href="https://www.codecogs.com/eqnedit.php?latex=\theta^{k&plus;1}&space;=&space;\underset{\theta}{argmax}E_{q^{k&plus;1}}\;logp(X,T|\theta)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\theta^{k&plus;1}&space;=&space;\underset{\theta}{argmax}E_{q^{k&plus;1}}\;logp(X,T|\theta)" title="\theta^{k+1} = \underset{\theta}{argmax}E_{q^{k+1}}\;logp(X,T|\theta)" /></a>
