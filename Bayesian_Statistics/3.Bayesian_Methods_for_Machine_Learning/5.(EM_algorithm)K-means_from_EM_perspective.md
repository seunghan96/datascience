# EM Algorithm 
## 5) K-means from EM perspective

You might have heard of, or already know about K-means clusteirng, which is the most widely known (hard) clustering method.
</br>

The algorithm of K-means is as below ( I will not explain about it )
</br>

<img src="https://stanford.edu/~cpiech/cs221/img/kmeansMath.png" width="550" /> </br>
https://stanford.edu/~cpiech/cs221/img/kmeansMath.png
</br>

Actually, K-means is also one part of EM algorithm. Can you notice from the pseudocode above?
</br>
</br>

### a. From GMM to K-means
fix covariance to be identical 
- <a href="https://www.codecogs.com/eqnedit.php?latex=\Sigma_c&space;=&space;1" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Sigma_c&space;=&space;1" title="\Sigma_c = 1" /></a>

fix weights of the prior to be uniform
- <a href="https://www.codecogs.com/eqnedit.php?latex=\pi_c&space;=&space;\frac{1}{number\;of\;Gaussians}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\pi_c&space;=&space;\frac{1}{number\;of\;Gaussians}" title="\pi_c = \frac{1}{number\;of\;Gaussians}" /></a>

The, that's all! If we make those two assumptions, the density of each data point, given that we know the cluster, will look like this.
</br>

<a href="https://www.codecogs.com/eqnedit.php?latex=p(x_i|t_i&space;=&space;c,\theta)&space;=&space;\frac{1}{Z}exp(-0.5||x_i-\mu_c||^2)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?p(x_i|t_i&space;=&space;c,\theta)&space;=&space;\frac{1}{Z}exp(-0.5||x_i-\mu_c||^2)" title="p(x_i|t_i = c,\theta) = \frac{1}{Z}exp(-0.5||x_i-\mu_c||^2)" /></a>
</br>
</br>

### b. E-step of K-means 
q function of k-means is a delta function, like below.
</br>

<a href="https://www.codecogs.com/eqnedit.php?latex=q^{k&plus;1}(t_i)&space;=&space;\left\{\begin{matrix}&space;1&space;\;\;\;&space;if\;t_i=c_i\\&space;0&space;\;\;&space;otherwise&space;\end{matrix}\right." target="_blank"><img src="https://latex.codecogs.com/gif.latex?q^{k&plus;1}(t_i)&space;=&space;\left\{\begin{matrix}&space;1&space;\;\;\;&space;if\;t_i=c_i\\&space;0&space;\;\;&space;otherwise&space;\end{matrix}\right." title="q^{k+1}(t_i) = \left\{\begin{matrix} 1 \;\;\; if\;t_i=c_i\\ 0 \;\; otherwise \end{matrix}\right." /></a>
</br>

c_i, which is a cluster for each data point i, is the c that makes the density of each data point the maximum.
</br>

<a href="https://www.codecogs.com/eqnedit.php?latex=c_i&space;=&space;\underset{c}{argmax}\;p(t_i=c|x_i,\theta)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?c_i&space;=&space;\underset{c}{argmax}\;p(t_i=c|x_i,\theta)" title="c_i = \underset{c}{argmax}\;p(t_i=c|x_i,\theta)" /></a>
</br>

But as you can see in <a href="https://www.codecogs.com/eqnedit.php?latex=p(x_i|t_i&space;=&space;c,\theta)&space;=&space;\frac{1}{Z}exp(-0.5||x_i-\mu_c||^2)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?p(x_i|t_i&space;=&space;c,\theta)&space;=&space;\frac{1}{Z}exp(-0.5||x_i-\mu_c||^2)" title="p(x_i|t_i = c,\theta) = \frac{1}{Z}exp(-0.5||x_i-\mu_c||^2)" /></a>,
we can change c_i like the below.
</br>

<a href="https://www.codecogs.com/eqnedit.php?latex=c_i&space;=&space;\underset{c}{argmax}\;p(t_i=c|x_i,\theta)&space;=&space;\underset{c}{argmin}||x_i-\mu_c||^2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?c_i&space;=&space;\underset{c}{argmax}\;p(t_i=c|x_i,\theta)&space;=&space;\underset{c}{argmin}||x_i-\mu_c||^2" title="c_i = \underset{c}{argmax}\;p(t_i=c|x_i,\theta) = \underset{c}{argmin}||x_i-\mu_c||^2" /></a>
</br>

This is exactly the same as the K-means algorithm!
</br>
</br>

### c. M-step of K-means 
remember, M-step of GMM was maximizing the following expression.
</br>

<a href="https://www.codecogs.com/eqnedit.php?latex=\underset{\mu}{max}\sum_{i=1}^{N}E_{q(t_i)}logP(x_i,t_i|\mu)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\underset{\mu}{max}\sum_{i=1}^{N}E_{q(t_i)}logP(x_i,t_i|\mu)" title="\underset{\mu}{max}\sum_{i=1}^{N}E_{q(t_i)}logP(x_i,t_i|\mu)" /></a>
</br>

And after the optimization, we found out that mu_c is
</br>

<a href="https://www.codecogs.com/eqnedit.php?latex=\mu_c&space;=&space;\frac{\sum_{i=1}^{N}(q(t_i=c)\cdot&space;x_i)}{\sum_{i=1}^{N}q(t_i=c)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mu_c&space;=&space;\frac{\sum_{i=1}^{N}(q(t_i=c)\cdot&space;x_i)}{\sum_{i=1}^{N}q(t_i=c)}" title="\mu_c = \frac{\sum_{i=1}^{N}(q(t_i=c)\cdot x_i)}{\sum_{i=1}^{N}q(t_i=c)}" /></a>
</br>

In the equation above, let q function be a delta function
</br>

<a href="https://www.codecogs.com/eqnedit.php?latex=q^{k&plus;1}(t_i)&space;=&space;\left\{\begin{matrix}&space;1&space;\;\;\;&space;if\;t_i=c_i\\&space;0&space;\;\;&space;otherwise&space;\end{matrix}\right." target="_blank"><img src="https://latex.codecogs.com/gif.latex?q^{k&plus;1}(t_i)&space;=&space;\left\{\begin{matrix}&space;1&space;\;\;\;&space;if\;t_i=c_i\\&space;0&space;\;\;&space;otherwise&space;\end{matrix}\right." title="q^{k+1}(t_i) = \left\{\begin{matrix} 1 \;\;\; if\;t_i=c_i\\ 0 \;\; otherwise \end{matrix}\right." /></a>
</br>

Then, we can get the following equation for mu_c, which is exactly the same as the k-means algorithm!
</br>

<a href="https://www.codecogs.com/eqnedit.php?latex=\mu_c&space;=&space;\frac{\sum_{i=1}^{N}(q(t_i=c)\cdot&space;x_i)}{\sum_{i=1}^{N}q(t_i=c)}&space;=&space;\frac{\sum_{i:c_i=c}x_i}{\&hash;{i:c_i=c}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mu_c&space;=&space;\frac{\sum_{i=1}^{N}(q(t_i=c)\cdot&space;x_i)}{\sum_{i=1}^{N}q(t_i=c)}&space;=&space;\frac{\sum_{i:c_i=c}x_i}{\&hash;{i:c_i=c}}" title="\mu_c = \frac{\sum_{i=1}^{N}(q(t_i=c)\cdot x_i)}{\sum_{i=1}^{N}q(t_i=c)} = \frac{\sum_{i:c_i=c}x_i}{\#{i:c_i=c}}" /></a>
</br>
</br>

### d. Summary
- K-means is actually EM for GMM, with fixed covariance matrices I
- It is a simplified E-step with delta function

#### K-means is faster but less flexible than GMM!
