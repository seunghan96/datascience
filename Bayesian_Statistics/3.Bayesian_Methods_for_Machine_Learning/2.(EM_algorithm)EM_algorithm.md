# EM Algorithm 
## 2) EM algorithm

### a. Jensen's inequality & KL-divergence
#### [ Jensen's inequality ]
- convex / concave
- relates the value of a convex/convex function of an integral to the integral of the convex/convae function
</br>
in the case of concave function..
</br>

If <a href="https://www.codecogs.com/eqnedit.php?latex=f(\alpha&space;a&space;&plus;&space;(1-\alpha)b)&space;\geq&space;\alpha&space;f(a)&space;&plus;&space;(1-\alpha)f(b)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?f(\alpha&space;a&space;&plus;&space;(1-\alpha)b)&space;\geq&space;\alpha&space;f(a)&space;&plus;&space;(1-\alpha)f(b)" title="f(\alpha a + (1-\alpha)b) \geq \alpha f(a) + (1-\alpha)f(b)" /></a>
</br>

Then Jensen's inequality is <a href="https://www.codecogs.com/eqnedit.php?latex=f(E_{p(t)}t)\geq&space;E_{p(t)}f(t)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?f(E_{p(t)}t)\geq&space;E_{p(t)}f(t)" title="f(E_{p(t)}t)\geq E_{p(t)}f(t)" /></a>
</br>

( a, b is a any two data point. alpha is a weight between 0 and 1 )
</br>
</br>
<img src="https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcSjRIhCTLP24BF3xD5LKRsG5Lff15ue6KjtU9gA0cSEhJTYTlQV" width="550" /> </br>
https://encrypted-tbn0.gstatic.com/
</br>
</br>

#### [ KL-divergence ]
 KL-divergence( Kullbackâ€“Leibler divergence), which is also called relative entropy, is a measure of how one probability distribution is different from another probability distribution. 
</br>

For a discrete probability distribution, the KL-divergence of Q from P is
</br>

<img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4958785faae58310ca5ab69de1310e3aafd12b32" width="300" /> </br>
</br>

and for a continuouse case, it is
</br>
<img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/756dd25036c5da76a59e58a001f3196e059f537d" width="300" /> </br>
</br>

<img src="https://cs-cheatsheet.readthedocs.io/en/latest/_images/kl_divergence.png" width="550" /> </br>
https://cs-cheatsheet.readthedocs.io/en/latest/_images/kl_divergence.png
</br>

The three properties of KL-divergence are
</br>
1. KL(p|q) is not always same with KL(q|p)
2. KL(p|p) = 0
3. KL(p|q) is always more or equal to 0
</br>
</br>

### b. EM algorithm (Expectation-Maximization algorithm)
This is the general form of Expectation Maximization. We take logarithm of the likelihood for easier calculation. ( We'll take an example with the number of clusters=3 )
</br>
</br>
<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{align*}&space;logp(X|\theta)&space;&=&space;\sum_{i=1}^{N}logp(x_i|\theta)&space;\\&space;&=&space;\sum_{i=1}^{N}log&space;\sum_{c=1}^{3}\frac{q(t_i=c)}{q(t_i=c)}p(x_i,t_i=c|\theta)\\&space;&\geq&space;\sum_{i=1}^{N}\sum_{c=1}^{3}q(t_i=c)log\frac{p(x_i,t_i=c|\theta)}{q(t_i=c)}\\&space;&=L(\theta,q)&space;\end{align*}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{align*}&space;logp(X|\theta)&space;&=&space;\sum_{i=1}^{N}logp(x_i|\theta)&space;\\&space;&=&space;\sum_{i=1}^{N}log&space;\sum_{c=1}^{3}\frac{q(t_i=c)}{q(t_i=c)}p(x_i,t_i=c|\theta)\\&space;&\geq&space;\sum_{i=1}^{N}\sum_{c=1}^{3}q(t_i=c)log\frac{p(x_i,t_i=c|\theta)}{q(t_i=c)}\\&space;&=L(\theta,q)&space;\end{align*}" title="\begin{align*} logp(X|\theta) &= \sum_{i=1}^{N}logp(x_i|\theta) \\ &= \sum_{i=1}^{N}log \sum_{c=1}^{3}\frac{q(t_i=c)}{q(t_i=c)}p(x_i,t_i=c|\theta)\\ &\geq \sum_{i=1}^{N}\sum_{c=1}^{3}q(t_i=c)log\frac{p(x_i,t_i=c|\theta)}{q(t_i=c)}\\ &=L(\theta,q) \end{align*}" /></a>
</br>

#### [ Interpretation ]
We build a lower bound by using Jensen's Inequality. Actually, we instead of directly maximizing the likelihood, we can maximize the lower bound instead. But instead of using only one lower bound, we can use a family of lower bounds, and choose the one that suits best at each iteration. 
</br>

<img src="https://people.duke.edu/~ccc14/sta-663/_images/EMAlgorithm_19_0.png" width="450" /> </br>
https://people.duke.edu/~ccc14/sta-663/
</br>
</br>

The best lower bound among the lower bound family is chosen with the following expression. ( The best possible bound is that log(p(x|theta)) and L(theta,q) attach )
</br>
<a href="https://www.codecogs.com/eqnedit.php?latex=logp(X|\theta)\geq&space;L(\theta,q)&space;\;\;&space;for&space;\;&space;any&space;\;&space;q" target="_blank"><img src="https://latex.codecogs.com/gif.latex?logp(X|\theta)\geq&space;L(\theta,q)&space;\;\;&space;for&space;\;&space;any&space;\;&space;q" title="logp(X|\theta)\geq L(\theta,q) \;\; for \; any \; q" /></a>
</br>
</br>
<a href="https://www.codecogs.com/eqnedit.php?latex=q^{k&plus;1}&space;=&space;\underset{q}{argmax}L(\theta^k,q)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?q^{k&plus;1}&space;=&space;\underset{q}{argmax}L(\theta^k,q)" title="q^{k+1} = \underset{q}{argmax}L(\theta^k,q)" /></a>
</br>
</br>
