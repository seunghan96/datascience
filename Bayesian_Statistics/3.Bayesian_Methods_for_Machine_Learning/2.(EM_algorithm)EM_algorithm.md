# EM Algorithm 
## 2) EM algorithm

### a. Jensen's inequality & KL-divergence
#### [ Jensen's inequality ]
- convex / concave
- relates the value of a convex/convex function of an integral to the integral of the convex/convae function
</br>
in the case of concave function..
</br>

If <a href="https://www.codecogs.com/eqnedit.php?latex=f(\alpha&space;a&space;&plus;&space;(1-\alpha)b)&space;\geq&space;\alpha&space;f(a)&space;&plus;&space;(1-\alpha)f(b)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?f(\alpha&space;a&space;&plus;&space;(1-\alpha)b)&space;\geq&space;\alpha&space;f(a)&space;&plus;&space;(1-\alpha)f(b)" title="f(\alpha a + (1-\alpha)b) \geq \alpha f(a) + (1-\alpha)f(b)" /></a>
</br>

Then Jensen's inequality is <a href="https://www.codecogs.com/eqnedit.php?latex=f(E_{p(t)}t)\geq&space;E_{p(t)}f(t)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?f(E_{p(t)}t)\geq&space;E_{p(t)}f(t)" title="f(E_{p(t)}t)\geq E_{p(t)}f(t)" /></a>
</br>

( a, b is a any two data point. alpha is a weight between 0 and 1 )
</br>
</br>
<img src="https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcSjRIhCTLP24BF3xD5LKRsG5Lff15ue6KjtU9gA0cSEhJTYTlQV" width="550" /> </br>
https://encrypted-tbn0.gstatic.com/
</br>
</br>

#### [ KL-divergence ]
 KL-divergence( Kullbackâ€“Leibler divergence), which is also called relative entropy, is a measure of how one probability distribution is different from another probability distribution. 
</br>

For a discrete probability distribution, the KL-divergence of Q from P is
</br>

<img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4958785faae58310ca5ab69de1310e3aafd12b32" width="300" /> </br>
</br>

and for a continuouse case, it is
</br>
<img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/756dd25036c5da76a59e58a001f3196e059f537d" width="300" /> </br>
</br>

<img src="https://cs-cheatsheet.readthedocs.io/en/latest/_images/kl_divergence.png" width="550" /> </br>
https://cs-cheatsheet.readthedocs.io/en/latest/_images/kl_divergence.png
