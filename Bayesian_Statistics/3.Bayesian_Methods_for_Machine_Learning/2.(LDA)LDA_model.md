# LDA ( Latent Dirichlet Allocation)
## 2) LDA model

### a. LDA Model
Goal : find a probabilistic model of a corpus that assigns high probability to members of the corpus ( & to other “similar” documents )

This is how the model looks like.
</br>

<a href="https://www.codecogs.com/eqnedit.php?latex=p(W,Z,\Theta)&space;=&space;\prod_{d=1}^{D}p(\theta_d)\prod_{n=1}^{N_d}p(z_{dn}|\theta_d)p(w_{dn}|z_{dn})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?p(W,Z,\Theta)&space;=&space;\prod_{d=1}^{D}p(\theta_d)\prod_{n=1}^{N_d}p(z_{dn}|\theta_d)p(w_{dn}|z_{dn})" title="p(W,Z,\Theta) = \prod_{d=1}^{D}p(\theta_d)\prod_{n=1}^{N_d}p(z_{dn}|\theta_d)p(w_{dn}|z_{dn})" /></a>

#### [ Interpretation ]

- <a href="https://www.codecogs.com/eqnedit.php?latex=\prod_{d=1}^{D}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\prod_{d=1}^{D}" title="\prod_{d=1}^{D}" /></a> : for each document ( ex. d=3 : document 3 )
- <a href="https://www.codecogs.com/eqnedit.php?latex=p(\theta_d)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?p(\theta_d)" title="p(\theta_d)" /></a> : generate topic probabilities ( ex. (0.5,0.2,0.3) )
- <a href="https://www.codecogs.com/eqnedit.php?latex=\prod_{n=1}^{N_d}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\prod_{n=1}^{N_d}" title="\prod_{n=1}^{N_d}" /></a> : for each word ( ex. N4 : word 4 )
- <a href="https://www.codecogs.com/eqnedit.php?latex=p(z_{dn}|\theta_d)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?p(z_{dn}|\theta_d)" title="p(z_{dn}|\theta_d)" /></a> : select topic ( ex. with the probability vector theta )
- <a href="https://www.codecogs.com/eqnedit.php?latex=p(w_{dn}|z_{dn})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?p(w_{dn}|z_{dn})" title="p(w_{dn}|z_{dn})" /></a> : select word from topic

Very Intuitive! 

Let's see the distributions more carefully. </br>
The probability of theta is a dirichlet distribution with parameter alpha
- <a href="https://www.codecogs.com/eqnedit.php?latex=p(\theta_d)&space;\sim&space;Dir(\alpha)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?p(\theta_d)&space;\sim&space;Dir(\alpha)" title="p(\theta_d) \sim Dir(\alpha)" /></a>

The probability of topics (given theta) will be the component of theta.
- <a href="https://www.codecogs.com/eqnedit.php?latex=p(z_{dn}|\theta_d)&space;=&space;\theta_{dz_{dn}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?p(z_{dn}|\theta_d)&space;=&space;\theta_{dz_{dn}}" title="p(z_{dn}|\theta_d) = \theta_{dz_{dn}}" /></a>

To select the words, we need to know the probability of each words in the topic. 
(can find in the matrix Phi! row_num : Z_dn & col_num : w_dn )
- <a href="https://www.codecogs.com/eqnedit.php?latex=\Phi_{z_{dn}w_{dn}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Phi_{z_{dn}w_{dn}}" title="\Phi_{z_{dn}w_{dn}}" /></a>

We have to find the matrix Phi in the expression above. There are two constraints.
- <a href="https://www.codecogs.com/eqnedit.php?latex=\Phi_{tw}&space;\geq&space;0" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Phi_{tw}&space;\geq&space;0" title="\Phi_{tw} \geq 0" /></a>
- <a href="https://www.codecogs.com/eqnedit.php?latex=\sum_{w}\Phi_{tw}&space;=1" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\sum_{w}\Phi_{tw}&space;=1" title="\sum_{w}\Phi_{tw} =1" /></a>

#### [ Summary ]
Known 
- W data

Unknown 
- <a href="https://www.codecogs.com/eqnedit.php?latex=\Phi" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Phi" title="\Phi" /></a>  ( parameters, distribution over words for each topic 
- Z( latent variables, topic of each word )
- <a href="https://www.codecogs.com/eqnedit.php?latex=\Theta" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Theta" title="\Theta" /></a> ( latent variables, distribution over topics for each document )
</br>

### (2) E-step & M-step Overview
Goal : train the model by finding the optimal values of phi! ( by maximizing the likelihood )

If we take logarithm for the posterior distribution, it will seem like this.
</br>

<a href="https://www.codecogs.com/eqnedit.php?latex=logP(\theta,Z,W)&space;=&space;\sum_{d=1}^{D}\begin{bmatrix}&space;\;\sum_{t-1}^{T}(\alpha_t-1)\log\theta_{dt}&space;&plus;&space;\sum_{n=1}^{N_d}\sum_{t=1}^{T}[z_{dn}=t](log\theta_{dt}&plus;log\varphi&space;_{tw_{dn}})&space;\end{bmatrix}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?logP(\theta,Z,W)&space;=&space;\sum_{d=1}^{D}\begin{bmatrix}&space;\;\sum_{t-1}^{T}(\alpha_t-1)\log\theta_{dt}&space;&plus;&space;\sum_{n=1}^{N_d}\sum_{t=1}^{T}[z_{dn}=t](log\theta_{dt}&plus;log\varphi&space;_{tw_{dn}})&space;\end{bmatrix}" title="logP(\theta,Z,W) = \sum_{d=1}^{D}\begin{bmatrix} \;\sum_{t-1}^{T}(\alpha_t-1)\log\theta_{dt} + \sum_{n=1}^{N_d}\sum_{t=1}^{T}[z_{dn}=t](log\theta_{dt}+log\varphi _{tw_{dn}}) \end{bmatrix}" /></a> </br> 
( erasing the constant )
</br>
</br>

We will use EM algorithm to find this distribution.

E step
- <a href="https://www.codecogs.com/eqnedit.php?latex=KL(q(\theta)q(Z)\;||\;p(\theta,Z|W))\rightarrow&space;\underset{q(\theta),q(Z)}{min}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?KL(q(\theta)q(Z)\;||\;p(\theta,Z|W))\rightarrow&space;\underset{q(\theta),q(Z)}{min}" title="KL(q(\theta)q(Z)\;||\;p(\theta,Z|W))\rightarrow \underset{q(\theta),q(Z)}{min}" /></a>

M step
- <a href="https://www.codecogs.com/eqnedit.php?latex=E_{q(\theta)q(T)}log(P(\theta,Z,w))\rightarrow&space;\underset{\phi}{max}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?E_{q(\theta)q(T)}log(P(\theta,Z,w))\rightarrow&space;\underset{\phi}{max}" title="E_{q(\theta)q(T)}log(P(\theta,Z,w))\rightarrow \underset{\phi}{max}" /></a>
</br>
</br>
