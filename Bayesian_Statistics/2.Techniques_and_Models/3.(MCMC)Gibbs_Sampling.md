# 2. Markov Chain Monte Carlo (MCMC)
about Markov Chains : https://d3c33hcgiwev3.cloudfront.net/_adadc80290e52a99b282ca9d7c1a41ee_background_MarkovChains.html?Expires=1583020800&Signature=UJdh6PpuE3m5EvICzH476NP5PxgoQ81DO~rCGk7a7OQcAQ-gnEjYFVSNyYoFJP2427rmBkXVLCiPdzzOWDKToMHFkzMjICyFz2QIOL0Jw0qXS-4NDXiTyeFPU~RfVeM347ZuYEkhgUqpJgMsjclK11baUhZYtMmH2g97mdMki~E_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A

## (2) Gibbs Sampling
### a. Multiple parameter sampling & Full conditional distributions
What about the case when there are multiple parameters? That is when we use "Gibb Sampling".
</br>

The algorithm is like below. ( Let's assume with a case with 2 parameters, theta & phi )
</br>

**[Step 1]** Initialize  <a href="https://www.codecogs.com/eqnedit.php?latex=\theta_0,&space;\phi_0" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\theta_0,&space;\phi_0" title="\theta_0, \phi_0" /></a>
</br>

**[Step 2]** for i=1,...,m repeat: </br>
a) using <a href="https://www.codecogs.com/eqnedit.php?latex=\phi_0" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\phi_0" title="\phi_0" /></a>, draw <a href="https://www.codecogs.com/eqnedit.php?latex=\theta_i&space;\sim&space;p(\theta|\phi_{i-1},y)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\theta_i&space;\sim&space;p(\theta|\phi_{i-1},y)" title="\theta_i \sim p(\theta|\phi_{i-1},y)" /></a> </br>
b) using <a href="https://www.codecogs.com/eqnedit.php?latex=\theta_i" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\theta_i" title="\theta_i" /></a>, draw <a href="https://www.codecogs.com/eqnedit.php?latex=\phi_i&space;\sim&space;p(\phi|\theta_{i},y)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\phi_i&space;\sim&space;p(\phi|\theta_{i},y)" title="\phi_i \sim p(\phi|\theta_{i},y)" /></a>

With these two draws, one cycle of the Gibbs sampler is completed! Quite Simple!
</br>
</br>
<img src="https://miro.medium.com/max/2060/0*6QwmVaLCHiJQjbqs.png" width="550" /> </br>
https://miro.medium.com/max/2060/0*6QwmVaLCHiJQjbqs.png
</br>
</br>

### b. Conditionally conjugate prior example with Normal Likelihood
Let's take an example with the case of normal likelihood with unknwon mean & variance. And the parameters (mu and sigma) follows the distribution below.
</br>

<a href="https://www.codecogs.com/eqnedit.php?latex=y_i&space;|&space;\mu,&space;\sigma^2&space;\sim&space;N(\mu,\sigma^2)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y_i&space;|&space;\mu,&space;\sigma^2&space;\sim&space;N(\mu,\sigma^2)" title="y_i | \mu, \sigma^2 \sim N(\mu,\sigma^2)" /></a>
</br>

<a href="https://www.codecogs.com/eqnedit.php?latex=\mu&space;\sim&space;N(\mu_o,&space;\sigma_o^2)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mu&space;\sim&space;N(\mu_o,&space;\sigma_o^2)" title="\mu \sim N(\mu_o, \sigma_o^2)" /></a>
</br>

<a href="https://www.codecogs.com/eqnedit.php?latex=\sigma^2&space;\sim&space;\tau^{-1}(\alpha,\beta)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\sigma^2&space;\sim&space;\tau^{-1}(\alpha,\beta)" title="\sigma^2 \sim \tau^{-1}(\alpha,\beta)" /></a>
</br>
</br>

Then the posterior distribution can be expressed like this :
</br>

<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{align*}&space;p(\mu,\sigma^2|y_1,...y_n)&space;&\propto&space;p(y1,...,yn|\mu,\sigma^2)p(\mu)p(\sigma^2)\\&space;&\propto&space;(\sigma^2)^{-n/2}&space;exp[-\frac{1}{2\sigma^2}\sum_{i=1}^{n}(y_i-\mu)^2]exp[-\frac{1}{2\sigma^2}\sum_{i=1}^{n}(\mu-\mu_o)^2](\sigma^2)^{-(\alpha&plus;1)}exp[\frac{-\beta_o}{\sigma^2}]&space;\end{align*}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{align*}&space;p(\mu,\sigma^2|y_1,...y_n)&space;&\propto&space;p(y1,...,yn|\mu,\sigma^2)p(\mu)p(\sigma^2)\\&space;&\propto&space;(\sigma^2)^{-n/2}&space;exp[-\frac{1}{2\sigma^2}\sum_{i=1}^{n}(y_i-\mu)^2]exp[-\frac{1}{2\sigma^2}\sum_{i=1}^{n}(\mu-\mu_o)^2](\sigma^2)^{-(\alpha&plus;1)}exp[\frac{-\beta_o}{\sigma^2}]&space;\end{align*}" title="\begin{align*} p(\mu,\sigma^2|y_1,...y_n) &\propto p(y1,...,yn|\mu,\sigma^2)p(\mu)p(\sigma^2)\\ &\propto (\sigma^2)^{-n/2} exp[-\frac{1}{2\sigma^2}\sum_{i=1}^{n}(y_i-\mu)^2]exp[-\frac{1}{2\sigma^2}\sum_{i=1}^{n}(\mu-\mu_o)^2](\sigma^2)^{-(\alpha+1)}exp[\frac{-\beta_o}{\sigma^2}] \end{align*}" /></a>
</br>
</br>
</br>

First, we'll look at mu. So we assume sigma is a known constant. Then the expression would be like below.
</br>

<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{align*}&space;p(\mu|y_1,...y_n)&space;&\propto&space;p(\mu,\sigma^2|y_1,..,y_n)\\&space;&\propto&space;exp[-\frac{1}{2}(\frac{\sum_{i=1}^{n}(y_i-\mu)^2}{\sigma^2}&plus;\frac{(\mu-\mu_o)^2}{\sigma_o^2})]\\&space;&\propto&space;N(\mu|\frac{n\bar{y}/\sigma^2&space;&plus;\mu_o/\sigma_o^2}{n/\sigma^2&plus;1/\sigma_o^2},&space;\frac{1}{n/\sigma^2&space;&plus;&space;1/\sigma_o^2})&space;\end{align*}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{align*}&space;p(\mu|y_1,...y_n)&space;&\propto&space;p(\mu,\sigma^2|y_1,..,y_n)\\&space;&\propto&space;exp[-\frac{1}{2}(\frac{\sum_{i=1}^{n}(y_i-\mu)^2}{\sigma^2}&plus;\frac{(\mu-\mu_o)^2}{\sigma_o^2})]\\&space;&\propto&space;N(\mu|\frac{n\bar{y}/\sigma^2&space;&plus;\mu_o/\sigma_o^2}{n/\sigma^2&plus;1/\sigma_o^2},&space;\frac{1}{n/\sigma^2&space;&plus;&space;1/\sigma_o^2})&space;\end{align*}" title="\begin{align*} p(\mu|y_1,...y_n) &\propto p(\mu,\sigma^2|y_1,..,y_n)\\ &\propto exp[-\frac{1}{2}(\frac{\sum_{i=1}^{n}(y_i-\mu)^2}{\sigma^2}+\frac{(\mu-\mu_o)^2}{\sigma_o^2})]\\ &\propto N(\mu|\frac{n\bar{y}/\sigma^2 +\mu_o/\sigma_o^2}{n/\sigma^2+1/\sigma_o^2}, \frac{1}{n/\sigma^2 + 1/\sigma_o^2}) \end{align*}" /></a>
</br>

Then, we'll look at sigma. So we assume mu is a known constant. Then the expression would be like below.
</br>

<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{align*}&space;p(\sigma|y_1,...y_n)&space;&\propto&space;p(\mu,\sigma^2|y_1,..,y_n)\\&space;&\propto&space;\tau^{-1}(\sigma^2|\alpha&space;&plus;&space;\frac{n}{2},&space;\beta_o&plus;\frac{\sum_{i=1}^{n}(y_i-\mu)^2}{2})&space;\end{align*}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{align*}&space;p(\sigma|y_1,...y_n)&space;&\propto&space;p(\mu,\sigma^2|y_1,..,y_n)\\&space;&\propto&space;\tau^{-1}(\sigma^2|\alpha&space;&plus;&space;\frac{n}{2},&space;\beta_o&plus;\frac{\sum_{i=1}^{n}(y_i-\mu)^2}{2})&space;\end{align*}" title="\begin{align*} p(\sigma|y_1,...y_n) &\propto p(\mu,\sigma^2|y_1,..,y_n)\\ &\propto \tau^{-1}(\sigma^2|\alpha + \frac{n}{2}, \beta_o+\frac{\sum_{i=1}^{n}(y_i-\mu)^2}{2}) \end{align*}" /></a>
</br>
</br>

Two distributions expressed above, provide the basis of a Gibbs sampler to simulate from a Markov chain, whose stationary distribution is the full posterior distribution for mu and sigma squared. The only thing we have to do is to alternate draws between these mu and sigma, using the most recent draw of one parameter to update the other one.
