# 2. Markov Chain Monte Carlo (MCMC)
about Markov Chains : https://d3c33hcgiwev3.cloudfront.net/_adadc80290e52a99b282ca9d7c1a41ee_background_MarkovChains.html?Expires=1583020800&Signature=UJdh6PpuE3m5EvICzH476NP5PxgoQ81DO~rCGk7a7OQcAQ-gnEjYFVSNyYoFJP2427rmBkXVLCiPdzzOWDKToMHFkzMjICyFz2QIOL0Jw0qXS-4NDXiTyeFPU~RfVeM347ZuYEkhgUqpJgMsjclK11baUhZYtMmH2g97mdMki~E_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A

## (2) Gibbs Sampling
### a. Multiple parameter sampling & Full conditional distributions
What about the case when there are multiple parameters? That is when we use "Gibb Sampling".
</br>

The algorithm is like below. ( Let's assume with a case with 2 parameters, theta & phi )
</br>

**[Step 1]** Initialize  <a href="https://www.codecogs.com/eqnedit.php?latex=\theta_0,&space;\phi_0" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\theta_0,&space;\phi_0" title="\theta_0, \phi_0" /></a>
</br>

**[Step 2]** for i=1,...,m repeat: </br>
a) using <a href="https://www.codecogs.com/eqnedit.php?latex=\phi_0" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\phi_0" title="\phi_0" /></a>, draw <a href="https://www.codecogs.com/eqnedit.php?latex=\theta_i&space;\sim&space;p(\theta|\phi_{i-1},y)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\theta_i&space;\sim&space;p(\theta|\phi_{i-1},y)" title="\theta_i \sim p(\theta|\phi_{i-1},y)" /></a> </br>
b) using <a href="https://www.codecogs.com/eqnedit.php?latex=\theta_i" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\theta_i" title="\theta_i" /></a>, draw <a href="https://www.codecogs.com/eqnedit.php?latex=\phi_i&space;\sim&space;p(\phi|\theta_{i},y)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\phi_i&space;\sim&space;p(\phi|\theta_{i},y)" title="\phi_i \sim p(\phi|\theta_{i},y)" /></a>

With these two draws, one cycle of the Gibbs sampler is completed! Quite Simple!
</br>
</br>
<img src="https://miro.medium.com/max/2060/0*6QwmVaLCHiJQjbqs.png" width="550" /> </br>
https://miro.medium.com/max/2060/0*6QwmVaLCHiJQjbqs.png
</br>
</br>

### b. Conditionally conjugate prior example with Normal Likelihood
