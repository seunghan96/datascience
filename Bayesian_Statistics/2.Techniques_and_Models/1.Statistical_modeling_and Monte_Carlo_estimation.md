# 1. Statistical modeling and Monte Carlo estimation

## (1) Statistical Modeling
Statistical models : to imitates & approximates "data generating process"
</br>
<img src="https://i.ytimg.com/vi/yQhTtdq_y9M/maxresdefault.jpg" width="550" /> </br>
https://i.ytimg.com/vi/yQhTtdq_y9M/maxresdefault.jpg
</br>

### a. Four objectives of statistical models
1) **Quantify Uncertainty**
- ex Q) the probability of xxxx is 57%. How sure are you about the number '57'?
- ex A) confidence interval (51,63) with 99% confidence

2) **Inference**
- ex) We only know 10% of people who supported the candidate. What if we extend to the total population?

3) Measure support for hypothesis
- Polling example - hypothesis that the candidate is more popular with men than with women. 50% of women and 52% of men favored the candidate. Is thig strong
enought to support the hypothesis?

4) Prediction
- Model which does not produce realistic predictions will be useless! Very important
- Have same objective as Machine Learning.
</br>
If Machine Learning focuses on the 4) Prediction, Statistical models strive to balance these four objectives!
</br>
</br>

### b. Modeling Process
- 1 ) Understand the problem
- 2 ) Plan & Collect data
- 3 ) Explore data ( + visualization )
- 4 ) Postulate model
- 5 ) Fit model
- 6 ) Check Model
- 7 ) Iterate 4)~6)
- 8 ) Use model
</br>
</br>

## (2) Bayesian Modeling
### a. Components of Bayesian models (review)
1) likelihood : <a href="https://www.codecogs.com/eqnedit.php?latex=P(y|\theta)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(y|\theta)" title="P(y|\theta)" /></a></br>
2) prior : <a href="https://www.codecogs.com/eqnedit.php?latex=P(\theta)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(\theta)" title="P(\theta)" /></a></br>
3) posterior : <a href="https://www.codecogs.com/eqnedit.php?latex=P(\theta|y)&space;=&space;\frac{P(\theta,y)}{P(y)}=&space;\frac{P(\theta,y)}{\int&space;P(\theta,y)d\theta}=&space;\frac{P(y|\theta)P(\theta)}{\int&space;P(y|\theta)P(\theta)d\theta}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(\theta|y)&space;=&space;\frac{P(\theta,y)}{P(y)}=&space;\frac{P(\theta,y)}{\int&space;P(\theta,y)d\theta}=&space;\frac{P(y|\theta)P(\theta)}{\int&space;P(y|\theta)P(\theta)d\theta}" title="P(\theta|y) = \frac{P(\theta,y)}{P(y)}= \frac{P(\theta,y)}{\int P(\theta,y)d\theta}= \frac{P(y|\theta)P(\theta)}{\int P(y|\theta)P(\theta)d\theta}" /></a>
</br>
And before fitting a model, we need to specify these components.

### b. Posterior derivation
There can be a model with multiple levels (hierarchical). 
That is, a prior follows a certain distribution, and the parameter of that distribution follows another distribution, ... and so on.
</br>
Let's take a look at this expression.

<a href="https://www.codecogs.com/eqnedit.php?latex=X_i|\mu,\sigma^2&space;\sim&space;N(\mu,&space;\sigma^2)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?X_i|\mu,\sigma^2&space;\sim&space;N(\mu,&space;\sigma^2)" title="X_i|\mu,\sigma^2 \sim N(\mu, \sigma^2)" /></a>
</br>
</br>
<a href="https://www.codecogs.com/eqnedit.php?latex=\mu|\sigma^2&space;\sim&space;N(m,&space;\frac{\sigma^2}{w})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mu|\sigma^2&space;\sim&space;N(m,&space;\frac{\sigma^2}{w})" title="\mu|\sigma^2 \sim N(m, \frac{\sigma^2}{w})" /></a>
</br>
</br>
<a href="https://www.codecogs.com/eqnedit.php?latex=\sigma^2&space;\sim&space;\tau^{-1}(\alpha,\beta)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\sigma^2&space;\sim&space;\tau^{-1}(\alpha,\beta)" title="\sigma^2 \sim \tau^{-1}(\alpha,\beta)" /></a>
</br>
</br>
We can derive a posterior like below!
</br>
</br>
<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{align*}&space;P(X_1,...X_n,\mu,\sigma^2)&=P(X_1,..X_n|\mu,\sigma^2)P(\mu|\sigma^2)P(\sigma^2)\\&space;&=\prod_{i=1}^{n}[N(X_i|\mu,\sigma^2)]\times&space;N(\mu|m,\frac{\sigma^2}{w})\times\tau^{-1}(\sigma^2|\alpha,\beta)\\&space;&\sim&space;P(\mu,\sigma^2|X_1...X_n)&space;\end{align*}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{align*}&space;P(X_1,...X_n,\mu,\sigma^2)&=P(X_1,..X_n|\mu,\sigma^2)P(\mu|\sigma^2)P(\sigma^2)\\&space;&=\prod_{i=1}^{n}[N(X_i|\mu,\sigma^2)]\times&space;N(\mu|m,\frac{\sigma^2}{w})\times\tau^{-1}(\sigma^2|\alpha,\beta)\\&space;&\sim&space;P(\mu,\sigma^2|X_1...X_n)&space;\end{align*}" title="\begin{align*} P(X_1,...X_n,\mu,\sigma^2)&=P(X_1,..X_n|\mu,\sigma^2)P(\mu|\sigma^2)P(\sigma^2)\\ &=\prod_{i=1}^{n}[N(X_i|\mu,\sigma^2)]\times N(\mu|m,\frac{\sigma^2}{w})\times\tau^{-1}(\sigma^2|\alpha,\beta)\\ &\sim P(\mu,\sigma^2|X_1...X_n) \end{align*}" /></a>
</br>
</br>

## (3) Monte Carlo Estimation
### a. Monte Carlo Estimation
Monte Carlo estimation refers to simulating hypothetical draws from a probability distribution. It is a method to calculate important quantities of that distribution, such as mean, variance. 
</br>
In Monte Carlo estimation, it simulates a large number of draws to approximate the value which is calculated by the integral.
</br>
You can understand easily with the following expression.
</br>
</br>
There is a function of theta like <a href="https://www.codecogs.com/eqnedit.php?latex=h(\theta)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?h(\theta)" title="h(\theta)" /></a>
</br>
To calculate the mean of this function, <a href="https://www.codecogs.com/eqnedit.php?latex=E[h(\theta)]" target="_blank"><img src="https://latex.codecogs.com/gif.latex?E[h(\theta)]" title="E[h(\theta)]" /></a>, we have to take an integral like the below.
</br>
</br>
<a href="https://www.codecogs.com/eqnedit.php?latex=\int&space;h(\theta)p(\theta)d\theta" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\int&space;h(\theta)p(\theta)d\theta" title="\int h(\theta)p(\theta)d\theta" /></a>
</br>
</br>
But instead of using integral, by sampling a large sample, we can approximate this like below. The average of these samples converges in probability to the true mean, by the Law of Large Numbers.
</br>
</br>
<a href="https://www.codecogs.com/eqnedit.php?latex=E[h(\theta)]&space;=&space;\int&space;h(\theta)p(\theta)d\theta&space;\approx&space;\frac&space;{1}{m}&space;\sum_{i=1}^{m}h(\theta_i^m)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?E[h(\theta)]&space;=&space;\int&space;h(\theta)p(\theta)d\theta&space;\approx&space;\frac&space;{1}{m}&space;\sum_{i=1}^{m}h(\theta_i^m)" title="E[h(\theta)] = \int h(\theta)p(\theta)d\theta \approx \frac {1}{m} \sum_{i=1}^{m}h(\theta_i^m)" /></a>
</br>
</br>

### b. Monte Carlo error and marginalization

#### [ error ] 
The sample mean, theta star bar, will follow the distribution below.
</br>
</br>
<a href="https://www.codecogs.com/eqnedit.php?latex=\bar{\theta^*}&space;\sim&space;N(E(\theta),&space;\frac{Var(\theta)}{m})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\bar{\theta^*}&space;\sim&space;N(E(\theta),&space;\frac{Var(\theta)}{m})" title="\bar{\theta^*} \sim N(E(\theta), \frac{Var(\theta)}{m})" /></a>
</br>
</br>
And the sample variance can be calculated like this.
</br>
</br>
<a href="https://www.codecogs.com/eqnedit.php?latex=\widehat{Var(\theta)}&space;=&space;\frac{1}{m}\sum_{i=1}^{m}(\theta_i^*-\bar{\theta^*})^2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\widehat{Var(\theta)}&space;=&space;\frac{1}{m}\sum_{i=1}^{m}(\theta_i^*-\bar{\theta^*})^2" title="\widehat{Var(\theta)} = \frac{1}{m}\sum_{i=1}^{m}(\theta_i^*-\bar{\theta^*})^2" /></a>
</br>
</br>
And if we divide the sample variance and take a square root, this will be a standard error.
</br>
</br>
<a href="https://www.codecogs.com/eqnedit.php?latex=\sqrt{\frac{\widehat{Var(\theta)}}{m}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\sqrt{\frac{\widehat{Var(\theta)}}{m}}" title="\sqrt{\frac{\widehat{Var(\theta)}}{m}}" /></a>
</br>
</br>

#### [ marginalization ]
Let's say Y follows a Binomial distribution, with n=10 and parameter phi.</br>
And phi follows a Beta distribution with alpha=2,beta=2.
</br>
</br>
<a href="https://www.codecogs.com/eqnedit.php?latex=y|\phi&space;\sim&space;Bin(10,\phi)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y|\phi&space;\sim&space;Bin(10,\phi)" title="y|\phi \sim Bin(10,\phi)" /></a>
</br>
</br>
<a href="https://www.codecogs.com/eqnedit.php?latex=\phi&space;\sim&space;Beta(2,2)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\phi&space;\sim&space;Beta(2,2)" title="\phi \sim Beta(2,2)" /></a>
</br>
</br>
The followings are the steps of simulation.
- Step 1) sample phi* from a beta distribution
- Step 2) given, phi*, draw yi* ~ Bin(10,phi*)
- Then you get a sample (pair) of (phi*,yi*)
</br>
One major advantage of this (MC simulation) is that you can easily calculate the marginal distribution.
In the case of integral, it might have been hard to marginalize out phi, but in this case, using the sample pair,
you just discard all the phi's and use yi's.
