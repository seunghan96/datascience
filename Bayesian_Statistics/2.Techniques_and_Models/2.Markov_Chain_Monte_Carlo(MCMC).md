# 2. Markov Chain Monte Carlo (MCMC)
about Markov Chains : https://d3c33hcgiwev3.cloudfront.net/_adadc80290e52a99b282ca9d7c1a41ee_background_MarkovChains.html?Expires=1583020800&Signature=UJdh6PpuE3m5EvICzH476NP5PxgoQ81DO~rCGk7a7OQcAQ-gnEjYFVSNyYoFJP2427rmBkXVLCiPdzzOWDKToMHFkzMjICyFz2QIOL0Jw0qXS-4NDXiTyeFPU~RfVeM347ZuYEkhgUqpJgMsjclK11baUhZYtMmH2g97mdMki~E_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A

## (1) Metropolis-Hastings
### a. Algorithm
Let's say our target distribution is p(theta), but we only know it up to proportionality. Let that g(theta).
</br>
</br>
<a href="https://www.codecogs.com/eqnedit.php?latex=p(\theta)&space;\propto&space;g(\theta)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?p(\theta)&space;\propto&space;g(\theta)" title="p(\theta) \propto g(\theta)" /></a>
</br>
</br>
These are the steps of Metropolis-Hastings Algorithms.
</br>
</br>
**Step 1)** Select initial value <a href="https://www.codecogs.com/eqnedit.php?latex=\theta_0" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\theta_0" title="\theta_0" /></a>
</br>
</br>
**Step 2)** for i=1....m, repeat: </br>
a) Draw candidate <a href="https://www.codecogs.com/eqnedit.php?latex=\theta^*&space;\sim&space;q(\theta^*|\theta_{i-1})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\theta^*&space;\sim&space;q(\theta^*|\theta_{i-1})" title="\theta^* \sim q(\theta^*|\theta_{i-1})" /></a>
</br>
</br>
b) <a href="https://www.codecogs.com/eqnedit.php?latex=\alpha&space;=&space;\frac{g(\theta^*)/q(\theta^*|\theta_{i-1})}{g(\theta_{i-1})/q(\theta_{i-1}|\theta^*)}&space;=&space;\frac{g(\theta^*)q(\theta_{i-1}|\theta^*)}{g(\theta_{i-1})q(\theta^*|\theta_{i-1})}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\alpha&space;=&space;\frac{g(\theta^*)/q(\theta^*|\theta_{i-1})}{g(\theta_{i-1})/q(\theta_{i-1}|\theta^*)}&space;=&space;\frac{g(\theta^*)q(\theta_{i-1}|\theta^*)}{g(\theta_{i-1})q(\theta^*|\theta_{i-1})}" title="\alpha = \frac{g(\theta^*)/q(\theta^*|\theta_{i-1})}{g(\theta_{i-1})/q(\theta_{i-1}|\theta^*)} = \frac{g(\theta^*)q(\theta_{i-1}|\theta^*)}{g(\theta_{i-1})q(\theta^*|\theta_{i-1})}" /></a>
</br>
</br>
c) if alpha >= 1, accept theta* and set theta_i <- theta* </br>
   if 0<alpha<1, accept theta* and set theta_i <- theta* with probability alpha </br>
                 reject theta* and set theta_i <- theta* with probability 1-alpha
</br>

#### Interpretation
So what is the meaning of "alpha" in the algorithm above? </br>
It is the Posterior Probability of theta_new, divided by posterior probability of theta_(t-1). This can be calculated by the product of 
prior and the likelihood. Then you might understand why the algorithm is like the above.
</br>

If alpha is greater than 1 ( = the proposed move to the candidate is advantageous ) move there! </br>
If it is between 0 and 1 ( = not advantageous ) move there with probability alpha! </br>
This is also a Markov chain, because the decision to move on or not only depends on the current situation!
</br>
</br>
<img src="https://www.researchgate.net/publication/279248766/figure/fig8/AS:668369330126848@1536363074045/Illustration-of-Metropolis-Hastings-M-H-algorithm-explained-in-Figure-1.ppm" width="550" /> </br>
https://www.researchgate.net/publication/
</br>
</br>

### b. Demonstration

## (2) Gibbs Sampling

## (3) Assessing Convergence
