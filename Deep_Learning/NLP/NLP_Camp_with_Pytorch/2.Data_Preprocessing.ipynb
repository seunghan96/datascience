{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참고 : https://github.com/kh-kim/nlp_with_pytorch_examples/tree/master/chapter-04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, fileinput,re\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) 문장 단위 분절"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. 원래 문장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_kor = open(\"Kor_input.txt\",\"r\",encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'자연어처리는 인공지능의 한 줄기 입니다. 시퀀스 투 시퀀스의 등장 이후로 딥러닝을 활용한 자연어처리는 새로운 전기를 맞이하게 되었습니다. 문장을 받아 단순히 수치로 나타내던 시절을 넘어, 원하는대로 문장을 만들어낼 수 있게 된 것 입니다. 이에따라 이전까지 큰 변화가 없었던 자연어처리 분야의 연구는 폭발적으로 늘어나기 시작하여, 곧 기계번역 시스템은 신경망에 의해 정복 당하였습니다. 또한, attention 기법의 고도화로 전이학습이 발전하면서, QA 문제도 사람보다 정확한 수준이 되었습니다.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_kor.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. 수정 문장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "자연어처리는 인공지능의 한 줄기 입니다.\n",
      "시퀀스 투 시퀀스의 등장 이후로 딥러닝을 활용한 자연어처리는 새로운 전기를 맞이하게 되었습니다.\n",
      "문장을 받아 단순히 수치로 나타내던 시절을 넘어, 원하는대로 문장을 만들어낼 수 있게 된 것 입니다.\n",
      "이에따라 이전까지 큰 변화가 없었던 자연어처리 분야의 연구는 폭발적으로 늘어나기 시작하여, 곧 기계번역 시스템은 신경망에 의해 정복 당하였습니다.\n",
      "또한, attention 기법의 고도화로 전이학습이 발전하면서, QA 문제도 사람보다 정확한 수준이 되었습니다.\n"
     ]
    }
   ],
   "source": [
    "for line in open(\"Kor_input.txt\",\"r\",encoding='utf-8'):\n",
    "    if line.strip() != \"\":\n",
    "        line = re.sub(r'([a-z])\\.([A-Z])', r'\\1. \\2', line.strip())\n",
    "        sentences = sent_tokenize(line.strip())\n",
    "        \n",
    "        for s in sentences :\n",
    "            if s!=\"\":\n",
    "                sys.stdout.write(s+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) 영어 분절\n",
    "- 쉼표, 마침표, 인용부호 등을 띄워 줘야!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. 원래 문장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_eng = open(\"Eng_input.txt\",\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Natural language processing is one of biggest stream in artificial intelligence, and it becomes very popular after seq2seq's invention. However, in order to make a strong A.I., there are still many challenges remain. I believe that we can breakthrough these barriers to get strong artificial intelligence.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_eng.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. 수정 문장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.moses import MosesTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural language processing is one of biggest stream in artificial intelligence , and it becomes very popular after seq2seq 's invention . However , in order to make a strong A.I . , there are still many challenges remain . I believe that we can breakthrough these barriers to get strong artificial intelligence .\n"
     ]
    }
   ],
   "source": [
    "t = MosesTokenizer()\n",
    "\n",
    "for line in open(\"Eng_input.txt\",\"r\"):\n",
    "    if line.strip() != \"\":\n",
    "        tokens = t.tokenize(line.strip(), escape=False)\n",
    "        sys.stdout.write(' '.join(tokens) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) TorchText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * 3가지 case\n",
    "- case 1 ) X : corpus & Y : class ( 텍스트 분류, 감성 분석 )\n",
    "- case 2 ) X : corpus ( 언어 모델 )\n",
    "- case 3 ) X : corpus & Y : corpus ( 기계번역, 요약, 질의응답 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### case 1 ) X : corpus & Y : class ( 텍스트 분류, 감성 분석 )\n",
    "- 코퍼스 & 레이블 읽기\n",
    "- 한 줄에서, text & class가 탭('\\t')으로 구분된 데이터의 입력을 받는 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader(object):\n",
    "    def __init__(self, train_fn, valid_fn, \n",
    "                 batch_size=64, device=-1,\n",
    "                 max_vocab=999999, min_freq=1, \n",
    "                 use_eos=False, shuffle=True):\n",
    "        super(DataLoader,self).__init__()\n",
    "        \n",
    "        # define field of input file ( consists of 2 fields(columns) )\n",
    "        # X ( corpus )\n",
    "        self.text = data.Field(use_vocab=True,\n",
    "                               batch_first=True,\n",
    "                               include_lengths=False,\n",
    "                               eos_token='<EOS>' if use_eos else None)\n",
    "        # Y ( class )\n",
    "        self.label = data.Field(sequential=False,\n",
    "                               use_vocab=True,\n",
    "                               unk_token=None)\n",
    "        \n",
    "        # 2 columns will be delimited by TAB\n",
    "        train, valid = data.TabularDataset.splits(path='',\n",
    "                                                 train=train_fn, validation=valid_fn, format='tsv',\n",
    "                                                  fields=[ ('label', self.label),\n",
    "                                                         ('text',self.text)])\n",
    "        \n",
    "        # train & valid iterator\n",
    "        self.train_iter, self.valid_iter = data.BucketIterator.splits((train,valid), \n",
    "                                                                      batch_size=batch_size,\n",
    "                                                                     device='cuda:%d' % device if device>=0 else 'cpu',\n",
    "                                                                     shuffle=shuffle,\n",
    "                                                                     sort_key=lambda x : len(x.text),\n",
    "                                                                     sort_within_batch=True)\n",
    "        \n",
    "        # make vocabulary for label & text\n",
    "        # ( = making mapping table between words & indice )\n",
    "        self.label.build_vocab(train)\n",
    "        self.text.build_vocab(train, max_size=max_vocab,min_freq=min_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### case 2 ) X : corpus ( 언어 모델 )\n",
    "- 코퍼스 읽기\n",
    "- 한 라인이 text로만 이루어진 경우 ( 주로 언어모델 훈련시킬 때 )\n",
    "- 언어 모델(Language Model, LM) : 언어라는 현상을 모델링하고자 단어 시퀀스(또는 문장)에 확률을 할당(assign)하는 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data,datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD, BOS, EOS  = 1,2,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self,train_fn,valid_fn,\n",
    "                 batch_size=64,device='cpu',\n",
    "                 max_vocab=99999999, max_length=255,\n",
    "                 fix_length=None,use_bos=True,use_eos=True,\n",
    "                 shuffle=True):\n",
    "        super(DataLoader,self).__init__()\n",
    "        \n",
    "        # X (corpus)\n",
    "        self.text = data.Field(sequential=True,\n",
    "                              use_vocab=True,\n",
    "                              batch_first=True,\n",
    "                               include_lengths=True,\n",
    "                               fix_length=fix_length,\n",
    "                               init_token='<BOS>' if use_bos else None,\n",
    "                               eos_token='<EOS>' if use_eos else None)\n",
    "        \n",
    "        train = LanguageModelDataset(path=train_fn,\n",
    "                                    fields=[('text', self,text)],\n",
    "                                    max_length=max_length)\n",
    "        valid = LanguageModelDataset(path=valid_fn,\n",
    "                                    fields=[('text', self.text)],\n",
    "                                    max_length=max_length)\n",
    "        \n",
    "        self.train_iter = data.BucketIterator(train,\n",
    "                                             batch_size=batch_size,\n",
    "                                             device='cuda:%d' % device if device >= 0 else 'cpu',\n",
    "                                             shuffle=False,\n",
    "                                             sort_key = lambda x: -len(x.text),\n",
    "                                             sort_within_batch=True)\n",
    "        self.text.build_vocab(train, max_size=max_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModelDataset(data.Dataset):\n",
    "    def __init__(self, path, fields, max_length=None, **kwargs):\n",
    "        if not isinstance(fields[0], (tuple,list)):\n",
    "            fields = [('text', fields[0])]\n",
    "            \n",
    "        examples = []\n",
    "        with open(path) as f :\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if max_length and max_length < len(line.split()):\n",
    "                    continue\n",
    "                if line != '':\n",
    "                    examples.append(data.Example.fromlist([line],fields))\n",
    "            \n",
    "        super(LangaugeModelDataset, self).__init__(examples, fields, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
