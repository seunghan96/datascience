{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [ Karate Dataset Classification with MLP ]\n",
    "- train ratio : 0.1, 0.3, 0.5, 0.7\n",
    "- metric : accuracy, precision, recall, F1-score\n",
    "- dataset : Karate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Importing libraries & dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev = pd.read_csv('embedded_vector.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.040150</td>\n",
       "      <td>0.527334</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.111660</td>\n",
       "      <td>0.387415</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.535404</td>\n",
       "      <td>0.009845</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.725362</td>\n",
       "      <td>0.163119</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.165507</td>\n",
       "      <td>-0.431127</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          x1        x2  class\n",
       "30  0.040150  0.527334      0\n",
       "27 -0.111660  0.387415      0\n",
       "23 -0.535404  0.009845      0\n",
       "17  0.725362  0.163119      1\n",
       "26 -0.165507 -0.431127      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = ev[['X','Y','Color']]\n",
    "data.columns = ['x1','x2','class']\n",
    "data = data.sample(frac=1) # to shuffle\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Define Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic functions\n",
    "- 1) train_test_split\n",
    "- 2) standard scaler\n",
    "- 3) transpose & matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(data,test_ratio):\n",
    "    data.iloc[:,[0,1]] = standard_scaler(data.iloc[:,[0,1]])\n",
    "    test_index = np.random.choice(len(data),int(len(data)*test_ratio),replace=False)\n",
    "    train = data[~data.index.isin(test_index)]\n",
    "    test = data[data.index.isin(test_index)]\n",
    "    \n",
    "    train_X = np.array(train)[:,[0,1]]\n",
    "    train_y = np.array(train)[:,[2]].flatten()\n",
    "    train_y = np.column_stack((1-train_y,train_y))\n",
    "    \n",
    "    test_X = np.array(test)[:,[0,1]]\n",
    "    test_y = np.array(test)[:,[2]].flatten()\n",
    "    test_y = np.column_stack((1-test_y,test_y))\n",
    "    return train_X,train_y, test_X,test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_scaler(x):\n",
    "    mean = np.mean(x)\n",
    "    std = np.std(x)\n",
    "    return (x-mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _t(X):\n",
    "    return np.transpose(X)\n",
    "\n",
    "def _m(A,B):\n",
    "    return np.matmul(A,B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions\n",
    "- 1) Sigmoid\n",
    "- 2) Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.last_o = 1\n",
    "    \n",
    "    def __call__(self,X):\n",
    "        self.last_o = 1/(1+np.exp(-X))\n",
    "        return self.last_o\n",
    "    \n",
    "    def grad(self):\n",
    "        return self.last_o*(1-self.last_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        self.last_o = 1\n",
    "        \n",
    "    def __call__(self,X):\n",
    "        e_x = np.exp(X-np.max(X))\n",
    "        self.last_o = e_x / e_x.sum()\n",
    "        return self.last_o\n",
    "    \n",
    "    def grad(self):\n",
    "        return self.last_o*(1-self.last_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogLoss:\n",
    "    def __init__(self):\n",
    "        self.dh = 1\n",
    "        self.last_diff = 1\n",
    "    \n",
    "    def __call__(self,y,yhat):\n",
    "        self.last_diff = yhat-y\n",
    "        total_loss = np.mean(y*np.log(yhat+(1e-5)) + (1-y)*np.log(1-yhat+(1e-5)))\n",
    "        return -total_loss\n",
    "    \n",
    "    def grad(self):\n",
    "        return self.last_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron :\n",
    "    def __init__(self,W,b,activation):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.act= activation()\n",
    "        \n",
    "        self.dW = np.zeros_like(self.W)  \n",
    "        self.db = np.zeros_like(self.b)\n",
    "        self.dh = np.zeros_like(_t(self.W)) \n",
    "        \n",
    "        self.last_x = np.zeros((self.W.shape[0])) \n",
    "        self.last_h = np.zeros((self.W.shape[1]))\n",
    "        \n",
    "    def __call__(self,x):\n",
    "        self.last_x = x\n",
    "        self.last_h = _m(_t(self.W),x) + self.b\n",
    "        output = self.act(self.last_h)\n",
    "        return output\n",
    "    \n",
    "    def grad(self): \n",
    "        grad = self.act.grad()*self.W\n",
    "        return grad\n",
    "    \n",
    "    def grad_W(self,dh): \n",
    "        grad = np.ones_like(self.W) \n",
    "        grad_a = self.act.grad()   # dh/du     \n",
    "        for j in range(grad.shape[1]):\n",
    "            grad[:,j] = dh[j] * grad_a[j] * self.last_x     # previous gradient * dh/du * du/dW\n",
    "        return grad\n",
    "        \n",
    "    def grad_b(self,dh) : # dh/db = dh/du * du/db\n",
    "        grad = dh * self.act.grad() * 1  # previous gradient * dh/du * du/db\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self,input_num,output_num,hidden_depth,num_neuron, activation=Sigmoid, activation2=Softmax): \n",
    "        def init_var(in_,out_):\n",
    "            weight = np.random.normal(0,0.1,(in_,out_))\n",
    "            bias = np.zeros((out_,))\n",
    "            return weight,bias\n",
    "           \n",
    "    ## 1-1. Hidden Layer\n",
    "        self.sequence = list() # lists to put neurons\n",
    "        W,b = init_var(input_num,num_neuron)\n",
    "        self.sequence.append(Neuron(W,b,activation)) # b ->0 ( no bias term in input-hidden layer )\n",
    "    \n",
    "        for _ in range(hidden_depth-1):\n",
    "            W,b = init_var(num_neuron,num_neuron)\n",
    "            self.sequence.append(Neuron(W,b,activation)) # default : Sigmoid\n",
    "    \n",
    "    ## 1-2. Output Layer\n",
    "        W,b = init_var(num_neuron,output_num)\n",
    "        self.sequence.append(Neuron(W,b,activation2)) # default : Softmax\n",
    "    \n",
    "    def __call__(self,x):\n",
    "        for layer in self.sequence:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def calc_grad(self,loss_fun):\n",
    "        loss_fun.dh = loss_fun.grad()\n",
    "        self.sequence.append(loss_fun)\n",
    "        \n",
    "        for i in range(len(self.sequence)-1, 0, -1):\n",
    "            L1 = self.sequence[i]\n",
    "            L0 = self.sequence[i-1]\n",
    "            \n",
    "            L0.dh = _m(L0.grad(), L1.dh)\n",
    "            L0.dW = L0.grad_W(L1.dh)\n",
    "            L0.db = L0.grad_b(L1.dh)\n",
    "            \n",
    "        self.sequence.remove(loss_fun)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GD(nn,x,y,loss_fun,lr=0.01):\n",
    "    loss = loss_fun(nn(x),y) # 1) FEED FORWARD\n",
    "    nn.calc_grad(loss_fun) # 2) BACK PROPAGATION\n",
    "    \n",
    "    for layer in nn.sequence: # Update Equation\n",
    "        layer.W += -lr*layer.dW\n",
    "        layer.b += -lr*layer.db    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_10, train_y_10, test_X_10, test_y_10 = train_test_split(data,0.9)\n",
    "train_X_30, train_y_30, test_X_30, test_y_30 = train_test_split(data,0.7)\n",
    "train_X_50, train_y_50, test_X_50, test_y_50 = train_test_split(data,0.5)\n",
    "train_X_70, train_y_70, test_X_70, test_y_70 = train_test_split(data,0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### too Small Dataset for MLP! use simple structures!\n",
    "- 2 hidden layers\n",
    "- 2 neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 cases\n",
    "- case 1) train 10%\n",
    "- case 2) train 30%\n",
    "- case 3) train 50%\n",
    "- case 4) train 70%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### case 1) train 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : Loss 5.202577693765131\n",
      "Epoch 2 : Loss 5.162600926727661\n",
      "Epoch 3 : Loss 5.119638289523182\n",
      "Epoch 4 : Loss 5.073500658648799\n",
      "Epoch 5 : Loss 5.023994627350589\n",
      "Epoch 6 : Loss 4.970924459117038\n",
      "Epoch 7 : Loss 4.914094617906786\n",
      "Epoch 8 : Loss 4.853312967184703\n",
      "Epoch 9 : Loss 4.788394727938221\n",
      "Epoch 10 : Loss 4.719167275694655\n",
      "Epoch 11 : Loss 4.645475834787615\n",
      "Epoch 12 : Loss 4.567190090940786\n",
      "Epoch 13 : Loss 4.4842116868603\n",
      "Epoch 14 : Loss 4.396482486881406\n",
      "Epoch 15 : Loss 4.303993394548755\n",
      "Epoch 16 : Loss 4.206793383355597\n"
     ]
    }
   ],
   "source": [
    "NeuralNet_10 = NN(2,2,1,2,activation=Sigmoid, activation2=Softmax) # input_num, output_num, hidden_depth, num_layers\n",
    "loss_fun = LogLoss()\n",
    "EPOCH = 16\n",
    "\n",
    "loss_per_epoch_10 = []\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    for i in range(train_X_10.shape[0]):\n",
    "        loss = GD(NeuralNet_10,train_X_10[i],train_y_10[i],loss_fun,0.1)\n",
    "    loss_per_epoch_10.append(loss)\n",
    "    print('Epoch {} : Loss {}'.format(epoch+1, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### case 2) train 30%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : Loss 6.0077145086115955\n",
      "Epoch 2 : Loss 6.630605222126901\n",
      "Epoch 3 : Loss 7.337995824266561\n",
      "Epoch 4 : Loss 8.068775394156157\n",
      "Epoch 5 : Loss 8.743128112430927\n",
      "Epoch 6 : Loss 9.304895772567551\n",
      "Epoch 7 : Loss 9.741686649276499\n",
      "Epoch 8 : Loss 10.070497595170835\n",
      "Epoch 9 : Loss 10.316627312127249\n",
      "Epoch 10 : Loss 10.502595760212468\n",
      "Epoch 11 : Loss 10.645389170469928\n",
      "Epoch 12 : Loss 10.757047753537647\n",
      "Epoch 13 : Loss 10.845951130795202\n",
      "Epoch 14 : Loss 10.91794245181227\n",
      "Epoch 15 : Loss 10.977142189508676\n",
      "Epoch 16 : Loss 11.0265005801676\n"
     ]
    }
   ],
   "source": [
    "NeuralNet_30 = NN(2,2,1,2,activation=Sigmoid, activation2=Softmax) # input_num, output_num, hidden_depth, num_layers\n",
    "loss_fun = LogLoss()\n",
    "EPOCH = 16\n",
    "\n",
    "loss_per_epoch_30 = []\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    for i in range(train_X_30.shape[0]):\n",
    "        loss = GD(NeuralNet_30,train_X_30[i],train_y_30[i],loss_fun,0.1)\n",
    "    loss_per_epoch_30.append(loss)\n",
    "    print('Epoch {} : Loss {}'.format(epoch+1, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### case 3) train 50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : Loss 5.579502528297192\n",
      "Epoch 2 : Loss 5.415466911261049\n",
      "Epoch 3 : Loss 5.191549080005368\n",
      "Epoch 4 : Loss 4.8903099305855235\n",
      "Epoch 5 : Loss 4.496429630649346\n",
      "Epoch 6 : Loss 4.007020168297924\n",
      "Epoch 7 : Loss 3.4461621822682185\n",
      "Epoch 8 : Loss 2.8693657934303136\n",
      "Epoch 9 : Loss 2.3412858038140443\n",
      "Epoch 10 : Loss 1.9023428560870312\n",
      "Epoch 11 : Loss 1.5588897932557657\n",
      "Epoch 12 : Loss 1.2970318661308449\n",
      "Epoch 13 : Loss 1.0978804736451946\n",
      "Epoch 14 : Loss 0.9448709131012745\n",
      "Epoch 15 : Loss 0.8254531429805213\n",
      "Epoch 16 : Loss 0.73062726760545\n"
     ]
    }
   ],
   "source": [
    "NeuralNet_50 = NN(2,2,1,2,activation=Sigmoid, activation2=Softmax) # input_num, output_num, hidden_depth, num_layers\n",
    "loss_fun = LogLoss()\n",
    "EPOCH = 16\n",
    "\n",
    "loss_per_epoch_50 = []\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    for i in range(train_X_50.shape[0]):\n",
    "        loss = GD(NeuralNet_50,train_X_50[i],train_y_50[i],loss_fun,0.1)\n",
    "    loss_per_epoch_50.append(loss)\n",
    "    print('Epoch {} : Loss {}'.format(epoch+1, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### case 4) train 70%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : Loss 4.898567335894803\n",
      "Epoch 2 : Loss 4.058881620390265\n",
      "Epoch 3 : Loss 3.0708254147867633\n",
      "Epoch 4 : Loss 2.187728069268146\n",
      "Epoch 5 : Loss 1.5616007602003\n",
      "Epoch 6 : Loss 1.159184638676766\n",
      "Epoch 7 : Loss 0.9003590294018504\n",
      "Epoch 8 : Loss 0.7273030175086146\n",
      "Epoch 9 : Loss 0.606219333725357\n",
      "Epoch 10 : Loss 0.5179050523121949\n",
      "Epoch 11 : Loss 0.4511672046828421\n",
      "Epoch 12 : Loss 0.399216673960884\n",
      "Epoch 13 : Loss 0.3577633372026356\n",
      "Epoch 14 : Loss 0.32399144597221563\n",
      "Epoch 15 : Loss 0.2959893482892117\n",
      "Epoch 16 : Loss 0.27241995779724454\n"
     ]
    }
   ],
   "source": [
    "NeuralNet_70 = NN(2,2,1,2,activation=Sigmoid, activation2=Softmax) # input_num, output_num, hidden_depth, num_layers\n",
    "loss_fun = LogLoss()\n",
    "EPOCH = 16\n",
    "\n",
    "loss_per_epoch_70 = []\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    for i in range(train_X_70.shape[0]):\n",
    "        loss = GD(NeuralNet_70,train_X_70[i],train_y_70[i],loss_fun,0.1)\n",
    "    loss_per_epoch_70.append(loss)\n",
    "    print('Epoch {} : Loss {}'.format(epoch+1, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model,test_X):\n",
    "    preds = []\n",
    "    for i in range(test_X.shape[0]):\n",
    "        pred_result = np.argmax(model(test_X[i]))\n",
    "        preds.append(pred_result)\n",
    "    return np.array(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) prediction result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred10 = predict(NeuralNet_10,test_X_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred30 = predict(NeuralNet_30,test_X_30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred50 = predict(NeuralNet_50,test_X_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred70 = predict(NeuralNet_70,test_X_70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Metrics(pred,actual):\n",
    "    TP,TN,FP,FN = 0,0,0,0\n",
    "    for i in range(len(pred)):\n",
    "        if pred[i]*actual[i]==1:\n",
    "            TP +=1\n",
    "        elif pred[i]>actual[i]:\n",
    "            FP +=1\n",
    "        elif pred[i]<actual[i]:\n",
    "            FN +=1\n",
    "        else:\n",
    "            TN +=1\n",
    "    \n",
    "    accuracy = (TP+TN) / (TP+TN+FP+FN)\n",
    "    precision = TP / (TP+FP)\n",
    "    recall = TP / (TP+FN)\n",
    "    F1_score = 2*(precision*recall)/(precision+recall)\n",
    "    return accuracy,precision,recall,F1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset 10%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5, 0.5, 1.0, 0.6666666666666666)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Training Dataset 10%')\n",
    "actual_class_10 = (1-test_y_10)[:,0]\n",
    "Metrics(pred10,actual_class_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset 30%\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-85a77cf46c3e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Training Dataset 30%'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mactual_class_30\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mtest_y_30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mMetrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred30\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactual_class_30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-50-e072fbce0fd7>\u001b[0m in \u001b[0;36mMetrics\u001b[1;34m(pred, actual)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTP\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mTN\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTP\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mTN\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mFP\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mFN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mprecision\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTP\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTP\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mFP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0mrecall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTP\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTP\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mFN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mF1_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprecision\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mrecall\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprecision\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mrecall\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "print('Training Dataset 30%')\n",
    "actual_class_30 = (1-test_y_30)[:,0]\n",
    "Metrics(pred30,actual_class_30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset 50%\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-5fe11d17b73b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Training Dataset 50%'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mactual_class_50\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mtest_y_50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mMetrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactual_class_50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-50-e072fbce0fd7>\u001b[0m in \u001b[0;36mMetrics\u001b[1;34m(pred, actual)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTP\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mTN\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTP\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mTN\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mFP\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mFN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mprecision\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTP\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTP\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mFP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0mrecall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTP\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTP\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mFN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mF1_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprecision\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mrecall\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprecision\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mrecall\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "print('Training Dataset 50%')\n",
    "actual_class_50 = (1-test_y_50)[:,0]\n",
    "Metrics(pred50,actual_class_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset 70%\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-d58beb7babb1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Training Dataset 70%'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mactual_class_70\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mtest_y_70\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mMetrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred70\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactual_class_70\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-50-e072fbce0fd7>\u001b[0m in \u001b[0;36mMetrics\u001b[1;34m(pred, actual)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTP\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mTN\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTP\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mTN\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mFP\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mFN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mprecision\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTP\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTP\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mFP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0mrecall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTP\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTP\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mFN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mF1_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprecision\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mrecall\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprecision\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mrecall\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "print('Training Dataset 70%')\n",
    "actual_class_70 = (1-test_y_70)[:,0]\n",
    "Metrics(pred70,actual_class_70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### can check that the model is not trained well, because of the size of the dataset! ( only 34 data in total )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
