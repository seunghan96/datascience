# Negative Sampling
이 방법은 hierarchical softmax보다 그 아이디어가 더 직관적이다. 한마디로 요약하자면, 한번의 iteration 마다 업데이트 해야 하는 output vector가 너무 많을 때, 그 중 일부만을 sample해서 업데이트 하는 방식이다. 예를 들어, 20000개의 단어가 있다고 하자. 기존의 방식대로 하자면, 이 20000개의 단어로 softmax를 사용하여 확률값을 내지만(too expensive computing cost), 이 방법 같은 경우에는 N개만을 샘플하여 N개의 binary classifier를 생성한다. 이는 보다 훨씬 더  빠른 연산을 가능하게 한다!
</br></br>
### Negative?
Negative Sampling에서 말하는 Negative는 무엇일까? 간단한 예를 들어보자. </br><
(철수,착하다)라는 word pair의 네트워크를 훈련할 때, 예측 대상의 label (actual output)은 '착하다'를 원 핫 인코딩한 벡터이다. 단어가 30000가지 있다고 하면, '착하다' 위치에 해당하는 인덱스만 1이고 나머지 29999개는 0이 된다. 여기서 0이 negative (반대로 1이 positive)라고 할 수 있다.
그래서 Negative Sampling을 한다는 것은 이 29999개의 negative들 중 일부만을 샘플링한다는 것이다. ( 당연히 정답인 positive도 sampling한다 )
일반적으로, 작은 데이터셋에서는 5에서 20개, 큰 데이터 셋에서는 2에서 5개를 샘플링한다고 한다. </br></br>
연산량이 얼마나 크게 줄어드는지 다음 예시를 통해 확인해볼 수 있다.</br>
10000개의 단어에 대해 100dimension을 가진 weight matrix가 있다고 해보자. 기존의 방식대로 한다면, parameter수는 100만개(10000x100=1000000)
가 된다. 하지만, 만약
negative sampling을 통해 5개의 negative을 샘플링(positive 1개 포함해서 total 6개 샘플링)했다면 , 그 parameter 수는 600개(6x100)이 된다. 파라미터 수를 0.06%로 크게 줄일 수 있다. </br></br>

### How to Sample
그렇다면 V-1개(V = vocabulary의 총 단어 수)의 negative들 중, 어떻게 샘플링할지도 다양한 방법이 있다. 대표적으로 word2vec에서는 unigram distribution으로 이러한
negative들을 샘플링한다. 즉, 더 자주 등장하는 (빈도 수가 높은) 단어가 샘플링 될 확률이 더 높다. 단어 A가 샘플로 뽑힐 확률은, corpus내에서 단어 A가 등장하는 횟수에서 전체 corpus의 단어 수를 나눈 것과 같다. </br></br>
Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and Dean, J. (2013) Distributed representations of words and phrases and their compositionality
에서는, 이 unigram distribution에 3/4제곱을 할 때 가장 좋은 성능을 보임을 알려준다.</br>
<a href="https://www.codecogs.com/eqnedit.php?latex=P(w_{i})&space;=&space;\frac{f(w_{i})^{3/4}}{\sum_{n}^{j=0}(f(w_{j})^{3/4})}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(w_{i})&space;=&space;\frac{f(w_{i})^{3/4}}{\sum_{n}^{j=0}(f(w_{j})^{3/4})}" title="P(w_{i}) = \frac{f(w_{i})^{3/4}}{\sum_{n}^{j=0}(f(w_{j})^{3/4})}" /></a> </br></br>

word2vec에서는 좋은 성능을 내는 것으로 알려진 다음과 같은 Error function을 사용하여 training을 한다. </br></br>
<a href="https://www.codecogs.com/eqnedit.php?latex=E&space;=&space;-log\sigma(v'_{w_{O}}^Th)&space;-&space;\sum_{w_{j}\in&space;W_{neg}}^{\m}&space;log\sigma(v'_{w_{j}}^Th)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?E&space;=&space;-log\sigma(v'_{w_{O}}^Th)&space;-&space;\sum_{w_{j}\in&space;W_{neg}}^{\m}&space;log\sigma(v'_{w_{j}}^Th)" title="E = -log\sigma(v'_{w_{O}}^Th) - \sum_{w_{j}\in W_{neg}}^{\m} log\sigma(v'_{w_{j}}^Th)" /></a>
</br></br>
여기서 wO는 output word(positive sample), <a href="https://www.codecogs.com/eqnedit.php?latex=v'_{w_{O}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?v'_{w_{O}}" title="v'_{w_{O}}" /></a>는 output vector, h는 hidden layer의 output vector를 나타낸다. ( CBOW의 h : <a href="https://www.codecogs.com/eqnedit.php?latex=\frac{1}{C}\sum_{c=1}^{C}v_{w_{c}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{1}{C}\sum_{c=1}^{C}v_{w_{c}}" title="\frac{1}{C}\sum_{c=1}^{C}v_{w_{c}}" /></a>, Skip-Gram의 h : <a href="https://www.codecogs.com/eqnedit.php?latex=v_{w_{I}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?v_{w_{I}}" title="v_{w_{I}}" /></a> ) <a href="https://www.codecogs.com/eqnedit.php?latex=W_{neg}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?W_{neg}" title="W_{neg}" /></a>는 negative 샘플들의 단어 집합이다. </br></br>

### Updating Equation
우선 E를 output unit <a href="https://www.codecogs.com/eqnedit.php?latex=w_{j}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?w_{j}" title="w_{j}" /></a>가 net input으로 들어갔을 때의 output값으로 미분을 하면, 다음과 같다. </br></br>
<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{align*}&space;\frac{\partial&space;E}{\partial&space;v'^T_{w_{j}}h}&space;&=&space;\left\{\begin{matrix}&space;\sigma(v'^T_{w_{j}}h)-1&space;\:\:\:\:\:\:\:if&space;\:\:w_{j}=w_{O}\\&space;\sigma(v'^T_{w_{j}}h)\:\:\:\:\:\:\:\:\:\:\:if&space;\:\:w_{j}\in&space;W_{neg}&space;\end{matrix}\right.&space;\\&space;&=\sigma(v'^T_{w_{j}}h)&space;-&space;t_{j}&space;\end{align*}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{align*}&space;\frac{\partial&space;E}{\partial&space;v'^T_{w_{j}}h}&space;&=&space;\left\{\begin{matrix}&space;\sigma(v'^T_{w_{j}}h)-1&space;\:\:\:\:\:\:\:if&space;\:\:w_{j}=w_{O}\\&space;\sigma(v'^T_{w_{j}}h)\:\:\:\:\:\:\:\:\:\:\:if&space;\:\:w_{j}\in&space;W_{neg}&space;\end{matrix}\right.&space;\\&space;&=\sigma(v'^T_{w_{j}}h)&space;-&space;t_{j}&space;\end{align*}" title="\begin{align*} \frac{\partial E}{\partial v'^T_{w_{j}}h} &= \left\{\begin{matrix} \sigma(v'^T_{w_{j}}h)-1 \:\:\:\:\:\:\:if \:\:w_{j}=w_{O}\\ \sigma(v'^T_{w_{j}}h)\:\:\:\:\:\:\:\:\:\:\:if \:\:w_{j}\in W_{neg} \end{matrix}\right. \\ &=\sigma(v'^T_{w_{j}}h) - t_{j} \end{align*}" /></a> </br>
여기서 <a href="https://www.codecogs.com/eqnedit.php?latex=t_{j}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?t_{j}" title="t_{j}" /></a>는 단어 <a href="https://www.codecogs.com/eqnedit.php?latex=w_{j}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?t_{j}" title="w_{j}" /></a>의 label로, positive sample일 경우 1, negative sample일 경우 0이다. </br></br>
그 다음으로, E를 단어 <a href="https://www.codecogs.com/eqnedit.php?latex=w_{j}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?w_{j}" title="w_{j}" /></a>의 output vector로 미분하면 다음과 같다. </br></br>
<a href="https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&space;E}{\partial&space;v'_{w_{j}}}&space;=&space;\frac{\partial&space;E}{\partial&space;v'^T_{w_{j}}h}\cdot\frac{\partial&space;v'^T_{w_{j}}h}{\partial&space;v'_{w_{j}}}&space;=&space;(\sigma(v'^T_{w_{j}}h)-t_{j})h" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{\partial&space;E}{\partial&space;v'_{w_{j}}}&space;=&space;\frac{\partial&space;E}{\partial&space;v'^T_{w_{j}}h}\cdot\frac{\partial&space;v'^T_{w_{j}}h}{\partial&space;v'_{w_{j}}}&space;=&space;(\sigma(v'^T_{w_{j}}h)-t_{j})h" title="\frac{\partial E}{\partial v'_{w_{j}}} = \frac{\partial E}{\partial v'^T_{w_{j}}h}\cdot\frac{\partial v'^T_{w_{j}}h}{\partial v'_{w_{j}}} = (\sigma(v'^T_{w_{j}}h)-t_{j})h" /></a> </br></br>
위를 통해 updating equation은 다음과 같이 나옴을 알 수 있다.</br></br>
<a href="https://www.codecogs.com/eqnedit.php?latex=v'_{w_{j}}^{new}=&space;v'_{w_{j}}^{old}-\eta(\sigma(v'^T_{w_{j}}h)-t_{j})h)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?v'_{w_{j}}^{new}=&space;v'_{w_{j}}^{old}-\eta(\sigma(v'^T_{w_{j}}h)-t_{j})h)" title="v'_{w_{j}}^{new}= v'_{w_{j}}^{old}-\eta(\sigma(v'^T_{w_{j}}h)-t_{j})h)" /></a> </br></br>
이 update 식은 모든 단어가 아닌, 단지 샘플링된 단어들에만 적용되기 때문에 연산량이 크게 줄어들 수 있다.</br>
따라서, E를 hidden layer의 output에 대해 미분하면, 다음과 같은 식이 완성된다. </br></br>
<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{align*}&space;\frac{\partial&space;E}{\partial&space;h}&space;&=&space;\sum_{w_{j}&space;\in&space;\{W_{O}\}\bigcup&space;W_{neg}}^{\&space;}\frac{\partial&space;E}{\partial&space;v'^T_{w_{j}}h}\cdot\frac{\partial&space;v'^T_{w_{j}}h}{\partial&space;h}&space;\\&space;&=&space;\sum_{w_{j}&space;\in&space;\{W_{O}\}\bigcup&space;W_{neg}}^{\&space;}\sigma(v'^T_{w_{j}}h)-t_{j})v'_{w_{j}}&space;:=EH&space;\end{align*}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{align*}&space;\frac{\partial&space;E}{\partial&space;h}&space;&=&space;\sum_{w_{j}&space;\in&space;\{W_{O}\}\bigcup&space;W_{neg}}^{\&space;}\frac{\partial&space;E}{\partial&space;v'^T_{w_{j}}h}\cdot\frac{\partial&space;v'^T_{w_{j}}h}{\partial&space;h}&space;\\&space;&=&space;\sum_{w_{j}&space;\in&space;\{W_{O}\}\bigcup&space;W_{neg}}^{\&space;}\sigma(v'^T_{w_{j}}h)-t_{j})v'_{w_{j}}&space;:=EH&space;\end{align*}" title="\begin{align*} \frac{\partial E}{\partial h} &= \sum_{w_{j} \in \{W_{O}\}\bigcup W_{neg}}^{\ }\frac{\partial E}{\partial v'^T_{w_{j}}h}\cdot\frac{\partial v'^T_{w_{j}}h}{\partial h} \\ &= \sum_{w_{j} \in \{W_{O}\}\bigcup W_{neg}}^{\ }\sigma(v'^T_{w_{j}}h)-t_{j})v'_{w_{j}} :=EH \end{align*}" /></a> </br></br>
CBOW의 경우 위 식의 EH를 그대로 updating equation에 집어 넣으면 되고, Skip-Gram같은 경우에는 모든 context word에 대해서 적용하여 EH value들을 합한 다음 집어넣으면 된다.

