{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [ Football Dataset Classification with Logistic Regression ]\n",
    "- embedded with LINE (first-order proximity / negative sampling )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Libraries & Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev = pd.read_csv('[Football]Embedded_with_FirstOrder.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev = ev.drop(ev.columns[0],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(115, 11)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.865126</td>\n",
       "      <td>0.732464</td>\n",
       "      <td>0.654681</td>\n",
       "      <td>-0.280288</td>\n",
       "      <td>-0.416516</td>\n",
       "      <td>0.779290</td>\n",
       "      <td>1.989182</td>\n",
       "      <td>0.944528</td>\n",
       "      <td>0.758910</td>\n",
       "      <td>0.924716</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.315168</td>\n",
       "      <td>-1.665299</td>\n",
       "      <td>-0.984810</td>\n",
       "      <td>1.077798</td>\n",
       "      <td>0.511267</td>\n",
       "      <td>0.939566</td>\n",
       "      <td>1.635527</td>\n",
       "      <td>-0.366913</td>\n",
       "      <td>-0.451699</td>\n",
       "      <td>1.780345</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.569846</td>\n",
       "      <td>-0.199044</td>\n",
       "      <td>1.784970</td>\n",
       "      <td>0.186517</td>\n",
       "      <td>2.154936</td>\n",
       "      <td>-0.550533</td>\n",
       "      <td>-0.937430</td>\n",
       "      <td>0.107572</td>\n",
       "      <td>1.074133</td>\n",
       "      <td>0.326420</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.832763</td>\n",
       "      <td>0.221549</td>\n",
       "      <td>-0.575225</td>\n",
       "      <td>-0.686977</td>\n",
       "      <td>-1.096524</td>\n",
       "      <td>0.453152</td>\n",
       "      <td>-0.012188</td>\n",
       "      <td>0.983878</td>\n",
       "      <td>0.942373</td>\n",
       "      <td>0.570720</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.117742</td>\n",
       "      <td>0.502898</td>\n",
       "      <td>0.749028</td>\n",
       "      <td>0.396632</td>\n",
       "      <td>-0.188808</td>\n",
       "      <td>0.286299</td>\n",
       "      <td>1.366271</td>\n",
       "      <td>-0.073079</td>\n",
       "      <td>-0.786144</td>\n",
       "      <td>-0.255506</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.865126  0.732464  0.654681 -0.280288 -0.416516  0.779290  1.989182   \n",
       "1 -0.315168 -1.665299 -0.984810  1.077798  0.511267  0.939566  1.635527   \n",
       "2 -0.569846 -0.199044  1.784970  0.186517  2.154936 -0.550533 -0.937430   \n",
       "3  0.832763  0.221549 -0.575225 -0.686977 -1.096524  0.453152 -0.012188   \n",
       "4  0.117742  0.502898  0.749028  0.396632 -0.188808  0.286299  1.366271   \n",
       "\n",
       "          7         8         9  Label  \n",
       "0  0.944528  0.758910  0.924716      7  \n",
       "1 -0.366913 -0.451699  1.780345      0  \n",
       "2  0.107572  1.074133  0.326420      2  \n",
       "3  0.983878  0.942373  0.570720      3  \n",
       "4 -0.073079 -0.786144 -0.255506      7  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6     13\n",
       "9     12\n",
       "3     12\n",
       "2     11\n",
       "11    10\n",
       "8     10\n",
       "4     10\n",
       "0      9\n",
       "7      8\n",
       "1      8\n",
       "10     7\n",
       "5      5\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ev['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=42,k_neighbors=2)\n",
    "k = sm.fit_sample(ev.iloc[:,0:10],ev.iloc[:,10])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev2 = pd.DataFrame(k[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev2['Label'] = k[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev2 = ev2.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11    13\n",
       "10    13\n",
       "9     13\n",
       "8     13\n",
       "7     13\n",
       "6     13\n",
       "5     13\n",
       "4     13\n",
       "3     13\n",
       "2     13\n",
       "1     13\n",
       "0     13\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ev2['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train & test split\n",
    "- proportional to each 'class' (1~12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMOTE (X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_index1 = ev.groupby('Label').apply(lambda x: x.sample(frac=0.3)).index.levels[1]\n",
    "train_index1 = set(np.arange(0,ev.shape[0])) - set(test_index1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1 = ev.loc[train_index1]\n",
    "test1 = ev.loc[test_index1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X1 = np.array(train1.iloc[:,0:10])\n",
    "train_y1 = np.array(train1.iloc[:,10]).flatten()\n",
    "test_X1 = np.array(test1.iloc[:,0:10])\n",
    "test_y1 = np.array(test1.iloc[:,10]).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((80, 10), (35, 10), (80,), (35,))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X1.shape, test_X1.shape, train_y1.shape, test_y1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMOTE (O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_index2 = ev2.groupby('Label').apply(lambda x: x.sample(frac=0.3)).index.levels[1]\n",
    "train_index2 = set(np.arange(0,ev2.shape[0])) - set(test_index2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2 = ev2.loc[train_index2]\n",
    "test2 = ev2.loc[test_index2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X2 = np.array(train2.iloc[:,0:10])\n",
    "train_y2 = np.array(train2.iloc[:,10]).flatten()\n",
    "test_X2 = np.array(test2.iloc[:,0:10])\n",
    "test_y2 = np.array(test2.iloc[:,10]).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((108, 10), (48, 10), (108,), (48,))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X2.shape, test_X2.shape, train_y2.shape, test_y2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Define Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1) matrix multiplication\n",
    "- 2) sigmoid\n",
    "- 3) standard scaler\n",
    "- 4) loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mul(W,b,x):\n",
    "    return np.dot(x,W)+b\n",
    "\n",
    "def sigmoid(x):    \n",
    "    k = 1 / (1 + np.exp(-x+0.0001))\n",
    "    return k[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_scaler(x):\n",
    "    mean = np.mean(x)\n",
    "    std = np.std(x)\n",
    "    return (x-mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(y_hat,y):\n",
    "    total_loss = np.mean(y*np.log(y_hat+0.0001) + (1-y)*np.log(1-y_hat+0.0001))\n",
    "    return -total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test_X,W,b):\n",
    "    result = sigmoid(np.dot(test_X, W) + b)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logreg(x,y,epoch,lr):\n",
    "    W = np.random.rand(x.shape[1],1)\n",
    "    b = np.random.rand(1)\n",
    "    \n",
    "    for ep in range(epoch+1):\n",
    "        Z = mul(W,b,x)\n",
    "        y_hat = sigmoid(Z)\n",
    "        loss = loss_func(y_hat,y)\n",
    "        dw = np.matmul(x.T,y_hat-y)/x.shape[0]\n",
    "        db = np.sum(y_hat-y)\n",
    "        \n",
    "        W = W-lr*dw.reshape(-1,1)\n",
    "        b = b-lr*db\n",
    "        \n",
    "        if ep>0 and ep % 10000 == 0:\n",
    "            print('epoch :',ep,' loss :',loss)\n",
    "    print('------------------------------------------ final loss :',loss,'---')   \n",
    "    return W,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OVR (One-Versus-Rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OVR(train_x,train_y,test_x,test_y,epoch,lr):\n",
    "    pred_result = []\n",
    "    real_result = []\n",
    "    for index in ev['Label'].unique():\n",
    "        train_y2 = (train_y == index).astype(int)        \n",
    "        test_y2 = (test_y == index).astype(int)\n",
    "        \n",
    "        \n",
    "        ''' oversampling with SMOTE in OVR\n",
    "        \n",
    "        sm = SMOTE(random_state=42,k_neighbors=3)\n",
    "        smote_x,smote_y = sm.fit_sample(train_x,train_y2)\n",
    "        \n",
    "        ind = np.arange(smote_x.shape[0])\n",
    "        np.random.shuffle(ind)\n",
    "        \n",
    "        smote_x,smote_y = smote_x[ind],smote_y[ind]\n",
    "        \n",
    "        W,b = logreg(smote_x,smote_y,epoch,lr)\n",
    "        print('------------------------------------------ Classifier ',index,'done---')\n",
    "        \n",
    "        '''\n",
    "        W,b = logreg(train_x,train_y2,epoch,lr)\n",
    "        y_pred = predict(test_x,W,b)\n",
    "        pred_result.append(y_pred)\n",
    "        real_result.append(test_y2)\n",
    "    pred_OH = (pred_result == np.amax(pred_result,axis=0)).astype('int')\n",
    "    act_OH = np.concatenate(real_result).ravel().reshape(ev.iloc[:,-1].nunique(),-1)    \n",
    "    return pred_OH,act_OH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(actual,prediction):\n",
    "    n = actual.shape[0]\n",
    "    conf_mat = np.zeros((n,n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            conf_mat[i][j] += len(np.intersect1d(np.nonzero(actual[i]),np.nonzero(prediction[j])))        \n",
    "    return conf_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1. SMOTE (X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 10000  loss : 0.24007866286417195\n",
      "epoch : 20000  loss : 0.23680118954105106\n",
      "------------------------------------------ final loss : 0.23680118954105106 ---\n",
      "epoch : 10000  loss : 0.1752521261784632\n",
      "epoch : 20000  loss : 0.16416244141006825\n",
      "------------------------------------------ final loss : 0.16416244141006825 ---\n",
      "epoch : 10000  loss : 0.281384114717651\n",
      "epoch : 20000  loss : 0.27806784014081265\n",
      "------------------------------------------ final loss : 0.27806784014081265 ---\n",
      "epoch : 10000  loss : 0.22297134652014933\n",
      "epoch : 20000  loss : 0.21767287570497387\n",
      "------------------------------------------ final loss : 0.21767287570497387 ---\n",
      "epoch : 10000  loss : 0.22175818987175216\n",
      "epoch : 20000  loss : 0.211198716837698\n",
      "------------------------------------------ final loss : 0.211198716837698 ---\n",
      "epoch : 10000  loss : 0.12114989191594872\n",
      "epoch : 20000  loss : 0.11273809885920652\n",
      "------------------------------------------ final loss : 0.11273809885920652 ---\n",
      "epoch : 10000  loss : 0.2649164780890776\n",
      "epoch : 20000  loss : 0.2519730950150898\n",
      "------------------------------------------ final loss : 0.2519730950150898 ---\n",
      "epoch : 10000  loss : 0.22101384088099474\n",
      "epoch : 20000  loss : 0.2078649825932543\n",
      "------------------------------------------ final loss : 0.2078649825932543 ---\n",
      "epoch : 10000  loss : 0.1767867072945053\n",
      "epoch : 20000  loss : 0.1425810221363643\n",
      "------------------------------------------ final loss : 0.1425810221363643 ---\n",
      "epoch : 10000  loss : 0.22525643967274248\n",
      "epoch : 20000  loss : 0.2156514249701329\n",
      "------------------------------------------ final loss : 0.2156514249701329 ---\n",
      "epoch : 10000  loss : 0.11773905299477097\n",
      "epoch : 20000  loss : 0.09568321094024893\n",
      "------------------------------------------ final loss : 0.09568321094024893 ---\n",
      "epoch : 10000  loss : 0.23128075596452985\n",
      "epoch : 20000  loss : 0.22795853056835433\n",
      "------------------------------------------ final loss : 0.22795853056835433 ---\n"
     ]
    }
   ],
   "source": [
    "prediction1,actual1 = OVR(train_X1,train_y1,test_X1,test_y1,20000,0.0025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 2., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 2., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1.],\n",
       "       [0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 2., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_without_smote = confusion_matrix(actual1, prediction1)\n",
    "confusion_without_smote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. SMOTE (O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 10000  loss : 0.2983699795044497\n",
      "epoch : 20000  loss : 0.2611569455527149\n",
      "epoch : 30000  loss : 0.2473366437738021\n",
      "epoch : 40000  loss : 0.24164112733430088\n",
      "epoch : 50000  loss : 0.23881470383005338\n",
      "epoch : 60000  loss : 0.23719708232203154\n",
      "epoch : 70000  loss : 0.23618992626970173\n",
      "epoch : 80000  loss : 0.23553188933471786\n",
      "epoch : 90000  loss : 0.23508881092170372\n",
      "epoch : 100000  loss : 0.23478409834385966\n",
      "------------------------------------------ final loss : 0.23478409834385966 ---\n",
      "epoch : 10000  loss : 0.2400181946008694\n",
      "epoch : 20000  loss : 0.18745882470144323\n",
      "epoch : 30000  loss : 0.15880505308162365\n",
      "epoch : 40000  loss : 0.14176660822950332\n",
      "epoch : 50000  loss : 0.13120364338910293\n",
      "epoch : 60000  loss : 0.12428912713895102\n",
      "epoch : 70000  loss : 0.1194388832204144\n",
      "epoch : 80000  loss : 0.1158095823084796\n",
      "epoch : 90000  loss : 0.11295044593793865\n",
      "epoch : 100000  loss : 0.11060850003748399\n",
      "------------------------------------------ final loss : 0.11060850003748399 ---\n",
      "epoch : 10000  loss : 0.2999803097390051\n",
      "epoch : 20000  loss : 0.2639149114555777\n",
      "epoch : 30000  loss : 0.2446725952739122\n",
      "epoch : 40000  loss : 0.23379872998522944\n",
      "epoch : 50000  loss : 0.22746777609393065\n",
      "epoch : 60000  loss : 0.22369106581188636\n",
      "epoch : 70000  loss : 0.2213713025379446\n",
      "epoch : 80000  loss : 0.21989517760674207\n",
      "epoch : 90000  loss : 0.21891950759605758\n",
      "epoch : 100000  loss : 0.21825060262288792\n",
      "------------------------------------------ final loss : 0.21825060262288792 ---\n",
      "epoch : 10000  loss : 0.29455031245577284\n",
      "epoch : 20000  loss : 0.26564345125146954\n",
      "epoch : 30000  loss : 0.25095026384575914\n",
      "epoch : 40000  loss : 0.24249639463859832\n",
      "epoch : 50000  loss : 0.23716841034005542\n",
      "epoch : 60000  loss : 0.23360593701021243\n",
      "epoch : 70000  loss : 0.23113276614192327\n",
      "epoch : 80000  loss : 0.22937279992261464\n",
      "epoch : 90000  loss : 0.2280983187354759\n",
      "epoch : 100000  loss : 0.2271630848001625\n",
      "------------------------------------------ final loss : 0.2271630848001625 ---\n",
      "epoch : 10000  loss : 0.3548198834773136\n",
      "epoch : 20000  loss : 0.28671006912526875\n",
      "epoch : 30000  loss : 0.249277348920404\n",
      "epoch : 40000  loss : 0.228683775225805\n",
      "epoch : 50000  loss : 0.21714141128012227\n",
      "epoch : 60000  loss : 0.2104198099948162\n",
      "epoch : 70000  loss : 0.20630635570095104\n",
      "epoch : 80000  loss : 0.203656329801001\n",
      "epoch : 90000  loss : 0.20186713234708797\n",
      "epoch : 100000  loss : 0.20061007578940213\n",
      "------------------------------------------ final loss : 0.20061007578940213 ---\n",
      "epoch : 10000  loss : 0.13292111805959692\n",
      "epoch : 20000  loss : 0.11483439477752716\n",
      "epoch : 30000  loss : 0.10402357803414848\n",
      "epoch : 40000  loss : 0.0969316477847122\n",
      "epoch : 50000  loss : 0.09192692327305411\n",
      "epoch : 60000  loss : 0.08818877266003355\n",
      "epoch : 70000  loss : 0.08527045381252621\n",
      "epoch : 80000  loss : 0.08291184267647564\n",
      "epoch : 90000  loss : 0.08095258903322274\n",
      "epoch : 100000  loss : 0.07928894046006787\n",
      "------------------------------------------ final loss : 0.07928894046006787 ---\n",
      "epoch : 10000  loss : 0.3481952743350057\n",
      "epoch : 20000  loss : 0.2879064519008239\n",
      "epoch : 30000  loss : 0.25728219983436634\n",
      "epoch : 40000  loss : 0.24083588231068212\n",
      "epoch : 50000  loss : 0.23140603463888046\n",
      "epoch : 60000  loss : 0.22563980878392964\n",
      "epoch : 70000  loss : 0.22190968000637687\n",
      "epoch : 80000  loss : 0.21938406070985333\n",
      "epoch : 90000  loss : 0.21761203854263705\n",
      "epoch : 100000  loss : 0.21633418543777017\n",
      "------------------------------------------ final loss : 0.21633418543777017 ---\n",
      "epoch : 10000  loss : 0.21266317034523596\n",
      "epoch : 20000  loss : 0.17930406028222048\n",
      "epoch : 30000  loss : 0.16222107154326448\n",
      "epoch : 40000  loss : 0.15248490782314048\n",
      "epoch : 50000  loss : 0.14636718911043783\n",
      "epoch : 60000  loss : 0.14219674031088794\n",
      "epoch : 70000  loss : 0.13916115360760709\n",
      "epoch : 80000  loss : 0.13683515684757225\n",
      "epoch : 90000  loss : 0.1349811000740059\n",
      "epoch : 100000  loss : 0.13345820228775343\n",
      "------------------------------------------ final loss : 0.13345820228775343 ---\n",
      "epoch : 10000  loss : 0.2834878544223111\n",
      "epoch : 20000  loss : 0.24590509223846155\n",
      "epoch : 30000  loss : 0.23321726562913037\n",
      "epoch : 40000  loss : 0.22755168297689163\n",
      "epoch : 50000  loss : 0.22456861010391957\n",
      "epoch : 60000  loss : 0.22283217057783475\n",
      "epoch : 70000  loss : 0.22175305410255083\n",
      "epoch : 80000  loss : 0.22105140386565358\n",
      "epoch : 90000  loss : 0.22058002097684232\n",
      "epoch : 100000  loss : 0.2202555050137814\n",
      "------------------------------------------ final loss : 0.2202555050137814 ---\n",
      "epoch : 10000  loss : 0.38969972975605066\n",
      "epoch : 20000  loss : 0.302637098419762\n",
      "epoch : 30000  loss : 0.25944308523026194\n",
      "epoch : 40000  loss : 0.2352142918299519\n",
      "epoch : 50000  loss : 0.22007405152167986\n",
      "epoch : 60000  loss : 0.20981538612630823\n",
      "epoch : 70000  loss : 0.20241610880226202\n",
      "epoch : 80000  loss : 0.19681442532338628\n",
      "epoch : 90000  loss : 0.1924126924031136\n",
      "epoch : 100000  loss : 0.18885366241819537\n",
      "------------------------------------------ final loss : 0.18885366241819537 ---\n",
      "epoch : 10000  loss : 0.2756251024544689\n",
      "epoch : 20000  loss : 0.22718486773731691\n",
      "epoch : 30000  loss : 0.20643472565578336\n",
      "epoch : 40000  loss : 0.1960051361906093\n",
      "epoch : 50000  loss : 0.1900684757832861\n",
      "epoch : 60000  loss : 0.18636604290448683\n",
      "epoch : 70000  loss : 0.1838893551023386\n",
      "epoch : 80000  loss : 0.18213763841611183\n",
      "epoch : 90000  loss : 0.18084148412847992\n",
      "epoch : 100000  loss : 0.17984649603065841\n",
      "------------------------------------------ final loss : 0.17984649603065841 ---\n",
      "epoch : 10000  loss : 0.24759561724848939\n",
      "epoch : 20000  loss : 0.21656257829847408\n",
      "epoch : 30000  loss : 0.20394407013269233\n",
      "epoch : 40000  loss : 0.19822450449413484\n",
      "epoch : 50000  loss : 0.19510911766714892\n",
      "epoch : 60000  loss : 0.19313480791405166\n",
      "epoch : 70000  loss : 0.1917541088358366\n",
      "epoch : 80000  loss : 0.19072929630772226\n",
      "epoch : 90000  loss : 0.1899402062585381\n",
      "epoch : 100000  loss : 0.18931780789395422\n",
      "------------------------------------------ final loss : 0.18931780789395422 ---\n"
     ]
    }
   ],
   "source": [
    "prediction2,actual2 = OVR(train_X2,train_y2,test_X2,test_y2,100000,0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 2., 0., 0., 1., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0.],\n",
       "       [2., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0.],\n",
       "       [1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 2., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 2.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 3., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 3.]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_with_smote = confusion_matrix(actual2, prediction2)\n",
    "confusion_with_smote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_scores(con,score): \n",
    "    # score = 0 : micro / score =1 : macro / score = 2 : weighted macro\n",
    "    \n",
    "    # (1) Micro F1\n",
    "    if score==0: \n",
    "        return np.diag(con).sum()/con.sum()\n",
    "    rec,pre,f1 = [],[],[]\n",
    "    \n",
    "    for i in range(con.shape[0]):\n",
    "        recall = con[i][i] / con[i].sum()\n",
    "        precision = con[i][i] / con[:,i].sum()\n",
    "        f1_score = 2*recall*precision / (recall+precision)\n",
    "        rec.append(recall)\n",
    "        pre.append(precision)\n",
    "        f1.append(f1_score)\n",
    "    \n",
    "    # (2) Macro F1\n",
    "    if score==1:\n",
    "        return np.average(f1)\n",
    "    \n",
    "    # (3) Weighted Macro F1\n",
    "    elif score==2:\n",
    "        w = [con[x].sum() for x in range(con.shape[0])]\n",
    "        return np.average(f1,weights=w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Micro F1 :',f1_scores(con,0).round(3))\n",
    "print('Macro F1 (Average) :',f1_scores(con,1).round(3))\n",
    "print('Macro F1 (Weighted Average) :',f1_scores(con,2).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
