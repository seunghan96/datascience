plot(silhouette(wk.clusters$cluster, dissimilarity.m))
# prototypical words 뽑아내기
work.clus.proto <- t(cl_prototypes(wk.clusters))
comparison.cloud(work.clus.proto, max.words = 100)
# Spherical K-Means Clustering
wk.dtm <- DocumentTermMatrix(wk.corpus, control=list(weghting=weightTfIdf))
soft.part <- skmeans(wk.dtm, 3, m=1.2, control=list(nruns=5, verbose=T))
barplot(table(soft.part$cluster), main='Spherical k-means')
plotcluster(cmdscale(dist(wk.dtm)), soft.part$cluster)
# 파일 불러오고 dtm 형태로 변환하기
wk.exp <- read.csv('C:\\Users\\samsung\\Desktop\\Bitamin\\Textmining with R\\5장\\1yr_plus_final4.csv', header=T)
wk.source <- VCorpus(VectorSource(wk.exp$text))
wk.corpus <- clean.corpus(wk.source)
wk.dtm <- DocumentTermMatrix(wk.corpus, control=list(weghting=weightTfIdf))
# scale해서 정규화하기
wk.dtm.s <- scale(wk.dtm, scale=T)
# 군집화하기 (k=3)
wk.clusters <- kmeans(wk.dtm.s, 3)
# barplot 그리기
barplot(wk.clusters$size, main='k-means')
clean.corpus <- function(corpus) {
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removeWords, c(stopwords("en"),'customer','service','customers','calls'))
return(corpus)
}
# 파일 불러오고 dtm 형태로 변환하기
wk.exp <- read.csv('C:\\Users\\samsung\\Desktop\\Bitamin\\Textmining with R\\5장\\1yr_plus_final4.csv', header=T)
wk.source <- VCorpus(VectorSource(wk.exp$text))
wk.corpus <- clean.corpus(wk.source)
wk.dtm <- DocumentTermMatrix(wk.corpus, control=list(weghting=weightTfIdf))
# scale해서 정규화하기
wk.dtm.s <- scale(wk.dtm, scale=T)
# 군집화하기 (k=3)
wk.clusters <- kmeans(wk.dtm.s, 3)
# barplot 그리기
barplot(wk.clusters$size, main='k-means')
# document 50개가 각각 어디 cluster에 속하는지 확인하기
# ( 1개는 군집1, 49개는 군집2에 속한다 )
wk.clusters$cluster
# plotcluster
plotcluster(cmdscale(dist(wk.dtm)), wk.clusters$cluster)
# silhouette
dissimilarity.m <- dist(wk.dtm.s)
plot(silhouette(wk.clusters$cluster, dissimilarity.m))
# prototypical words 뽑아내기
work.clus.proto <- t(cl_prototypes(wk.clusters))
comparison.cloud(work.clus.proto, max.words = 100)
# Spherical K-Means Clustering
wk.dtm <- DocumentTermMatrix(wk.corpus, control=list(weghting=weightTfIdf))
soft.part <- skmeans(wk.dtm, 3, m=1.2, control=list(nruns=5, verbose=T))
barplot(table(soft.part$cluster), main='Spherical k-means')
plotcluster(cmdscale(dist(wk.dtm)), soft.part$cluster)
barplot(table(soft.part$cluster), main='Spherical k-means')
plot(silhouette(soft.part))
s.clus.proto <- t(cl_prototypes(soft.part))
comparison.cloud(s.clus.proto, max.words=100)
# top 5 뽑아내기 ( 각각 cluster로 부터 )
sort(s.clus.proto[,1],decreasing=T)[1:5]
sort(s.clus.proto[,2],decreasing=T)[1:5]
sort(s.clus.proto[,3],decreasing=T)[1:5]
# < 3. K-Mediod Clustering >
wk.dtm <- DocumentTermMatrix(wk.corpus, control=list(weghting=weightTfIdf))
wk.mediods <- pamk(wk.dtm, krange=2:4, critout=T)
dissimilarity.m <- dsit(wk.dtm)
dissimilarity.m <- dist(wk.dtm)
plot(silhouette(wk.mediods$pamobject$clustering, dissimilarity.m))
library(fpc)
# < 4. Evaluating Cluster Approaches >
results <- cluster.stats(dist(wk.dtm.s), wk.clusters$cluster, wk.mediods$pamobject$clustering)
results
# Calculating and Exploring String Distance
stringdist('crabapple','apple',method='lcs')
# Calculating and Exploring String Distance
install.packages('stringdist')
# Calculating and Exploring String Distance
library(stringdits)
stringdist('crabapple','apple',method='lcs')
# Calculating and Exploring String Distance
library(stringdist)
stringdist('crabapple','apple',method='lcs')
stringdist('crabapples','apple',mtehod='lcs')
stringdist('crabapples','apple',method='lcs')
# 함수 3가지
# 1) Amatch ( + Ain)
match('apple', c('crabapple','pear'))
amatch('apple',c('crabapple','pear'), maxDist=3, method='dl')
amatch('apple',c('crabapple','pear'), maxDist=4, method='dl')
ain('raspberry',c('berry','pear'), maxDist=4, method='dl')
# 2) Stringdist
stringdist('raspberry',c('berry','pear'), method='hamming')
stringdist('raspberry',c('berry','pear'), method='osa')
# 3) Stringdistmatrix
fruit<- c('crabapple','apple','raspberry')
fruit.dist <- stringdistMatrix(fruit)
fruit.dist <- stringdistmatrix(fruit)
plot(hclust(fruit.dist), labels=fruit)
# LDA Topic Modeling Explained
text <- read.csv('C:\\Users\\samsung\\Desktop\\Bitamin\\Textmining with R\\5장\\guardian.csv')
# LDA Topic Modeling Explained
library(tm)
library(qdap)
library(qdap)
library(lda)
install.packages('lda')
library(lda)
library(GuardianR)
install.packages('GouardianR')
install.packages('pbapply')
install.packages('LDAvis')
install.packages('treemap')
install.packages('car')
# LDA Topic Modeling Explained
library(tm)
library(qdap)
library(lda)
library(GuardianR)
library(pbapply)
library(treemap)
library(car)
library(LDAvis)
library(car)
library(treemap)
# LDA Topic Modeling Explained
library(tm)
library(qdap)
library(lda)
library(GuardianR)
install.packages('GuardianR')
library(GuardianR)
library(pbapply)
library(LDAvis)
library(treemap)
library(car)
options(stringsAsFactors = F)
text <- read.csv('C:\\Users\\samsung\\Desktop\\Bitamin\\Textmining with R\\5장\\guardian.csv')
articles <- iconv(text$body, "latin1",'ASCII',sub="")
articles <-
articles <-
articles <-
articles <- iconv(text$body, "latin1",'ASCII',sub="")
articles <- iconv(text$body, "latin1",'ASCII',sub="")
articles <- gsub('http\\S+\\s*','',articles)
articles <- bracketX(articles, bracket='all')
articles <- gsub("[[:punct:]]","",articles)
articles <- removeNumbers(articles)
articles <- tolower(articles)
articles <- removeWords(articles,c(stopwords('en'), 'pakistan','gmt','england'))
blank.removal <- function(x) {
x <- unlist(strsplit(x,' '))
x <- subset(x, nchar(x)>0)
x <- paste(x,collapse=' ')
}
articles <- pblapply(articles, blank.removal)
# example로 훈련하기
ex.text <- c('Text mining is a     good time', 'Text mining is a good time')
strsplit(ex.text[1],' ')
strsplit(ex.text[2],' ')
char.vec <- unlist(strsplit(ex.text[1],' '))
char.vec
char.vec <- subset(char.vec, nchar(char.vec)>0)
char.vec
char.vec <- paste(char.vec, collapse=' ')
char.vec
# example로 훈련하기 (2)
ex.text <- c('this is a text documnet')
# example로 훈련하기 (2)
ex.text <- c('this is a text document')
ex.text.lex <- lexicalize(ex.text)
ex.text.lex
ex.text <- c('this is a text document','text mining a text document is great')
ex.text.lex <- lexicalize(ex.text)
ex.text.lex
# example 끝내고 다시 본문으로!
documents <- lexicalize(articles)
wc <- word.counts(documents$documents, documents$vocab)
wc
doc.length <- document.lengths(documents$documents)
doc.length
k <- 4
num.iter <- 25
alpha <- 0.02
fit <- lda.collapsed.gibbs.sampler(documents=documents$documents,
K=k,
vocab=documents$vocab,
num.iterations = num.iter,
alpha=alpha,
eta=eta,
initial=NULL,
burnin=0,
compute.log.likelihood = TRUE
)
eta <- 0.02
set.seed(1234)
# LDA 모델 만들기
k <- 4
num.iter <- 25
alpha <- 0.02
eta <- 0.02
set.seed(1234)
fit <- lda.collapsed.gibbs.sampler(documents=documents$documents,
K=k,
vocab=documents$vocab,
num.iterations = num.iter,
alpha=alpha,
eta=eta,
initial=NULL,
burnin=0,
compute.log.likelihood = TRUE
)
# LDA Topic Modeling Explained
library(tm)
library(qdap)
library(GuardianR)
library(lda)
library(pbapply)
library(LDAvis)
library(treemap)
library(car)
fit <- lda.collapsed.gibbs.sampler(documents=documents$documents,
K=k,
vocab=documents$vocab,
num.iterations = num.iter,
alpha=alpha,
eta=eta,
initial=NULL,
burnin=0,
compute.log.likelihood = TRUE
)
options(stringsAsFactors = F)
text <- read.csv('C:\\Users\\samsung\\Desktop\\Bitamin\\Textmining with R\\5장\\guardian.csv')
articles <- iconv(text$body, "latin1",'ASCII',sub="")
articles <- gsub('http\\S+\\s*','',articles)
articles <- bracketX(articles, bracket='all')
articles <- removeNumbers(articles)
articles <- tolower(articles)
articles <- removeWords(articles,c(stopwords('en'), 'pakistan','gmt','england'))
blank.removal <- function(x) {
x <- unlist(strsplit(x,' '))
x <- subset(x, nchar(x)>0)
x <- paste(x,collapse=' ')
}
articles <- pblapply(articles, blank.removal)
strsplit(ex.text[1],' ')
char.vec <- unlist(strsplit(ex.text[1],' '))
char.vec <- subset(char.vec, nchar(char.vec)>0)
char.vec <- paste(char.vec, collapse=' ')
char.vec
# example로 훈련하기 (2)
ex.text <- c('this is a text document')
ex.text.lex
ex.text.lex <- lexicalize(ex.text)
char.vec
char.vec
# example로 훈련하기 (1)
ex.text <- c('Text mining is a     good time', 'Text mining is a good time')
articles <- gsub("[[:punct:]]","",articles)
strsplit(ex.text[2],' ')
ex.text <- c('this is a text document','text mining a text document is great')
ex.text.lex <- lexicalize(ex.text)
ex.text.lex
# example 끝내고 다시 본문으로!
documents <- lexicalize(articles)
wc <- word.counts(documents$documents, documents$vocab)
doc.length <- document.lengths(documents$documents)
# LDA 모델 만들기
k <- 4
num.iter <- 25
alpha <- 0.02
eta <- 0.02
set.seed(1234)
fit <- lda.collapsed.gibbs.sampler(documents=documents$documents,
K=k,
vocab=documents$vocab,
num.iterations = num.iter,
alpha=alpha,
eta=eta,
initial=NULL,
burnin=0,
compute.log.likelihood = TRUE
)
str(fit)
plot(fit$log.likelihoods[1,])
top.topic.words(fit$topics,7,by.score=TRUE)
# 각 주제를 잘 대표하는 문서는 어떤 문서?
top.topic.documents(fit$document_sums,1)
theta <- t(pbapply(fit$document_sums + alpha, 2, function(x) x/sum(x)))
phi <- t(pbapply(t(fit$topics) + eta, 2, function(x) x/sum(x)))
article.json <- createJSON(phi=phi, theta=theta, doc.length=doc.length, vocab=document$vocab, term.frequency = as.vector(wc))
article.json <- createJSON(phi=phi, theta=theta, doc.length=doc.length, vocab=documents$vocab, term.frequency = as.vector(wc))
serVis(article.json)
article.json <- createJSON(phi=phi, theta=theta, doc.length=doc.length, vocab=documents$vocab, term.frequency = as.vector(wc))
# 각 주제를 잘 대표하는 문서는 어떤 문서?
# 주제1은 21번 문서가 잘 나타냄 / 주제2는 ....
top.topic.documents(fit$document_sums,1)
x <- table(x)
doc.assignment <- function(x) {
x <- table(x)
x <- as.matrix(x)
x <- t(x)
x <- max.col(x)
}
fit$assignments[[2]][1:10]
table(fit$assignments[[2]][1:10])
t(as.matrix(table(fit$assignments[[2]][1:10]))
t(as.matrix(table(fit$assignments[[2]][1:10])))
t(as.matrix(table(fit$assignments[[2]][1:10])))
max.col(t(as.matrix(table(fit$assignments[[2]][1:10]))))
assignments <- unlist(pblapply(fit$assignments,doc.assignment))
assignments <- recode(assignments, "1='Cricket1' ; 2='Paris Attacks'; 3='Cricket2', 4='Unknown'")
assignments <- recode(assignments, "1='Cricket1' ; 2='Paris Attacks'; 3='Cricket2'; 4='Unknown'")
article.ref <- seq(1:nrow(text))
article.pol <- polarity(articles)[[1]][3]
article.tree.df <- cbind(article.ref, article.pol, doc.length, assignments)
treemap(article.tree.df, index=c('assignments','article.ref'),
vSize='doc.length', vColor='polarity',type='value',
title='Guardian Articles mentioning Pakistan',
palette=c('red','white','green'))
# Text to Vectors using text2vec
library(data.table)
install.packages('text2vec')
library(text2vec)
library(tm)
text <- fread('Airbnb-boston_only.csv')
text <- fread('C:\\Users\\samsung\\Desktop\\Bitamin\\Textmining with R\\5장\\Airbnb-boston_only\\Airbnb-boston_only.csv')
airbnb <- data.table(review_id = text$review_id,
comments = text$comments,
review_scores_rating = text$review_scores_rating)
airbnb$comments <- removeWords(airbnb$comments, c(stopwords('en'),'Boston'))
airbnb$comments <- removePunctuation(airbnb$comments)
airbnb$comments <- stripWhitespace(airbnb$comments)
airbnb$comments <- removeNumbers(airbnb$comments)
airbnb$comments <- tolower(airbnb$comments)
airbnb$comments <- tolower(airbnb$comments)
airbnb$comments <- tolower(airbnb$comments)
options(stringsAsFactors = F)
text <- fread('C:\\Users\\samsung\\Desktop\\Bitamin\\Textmining with R\\5장\\Airbnb-boston_only\\Airbnb-boston_only.csv')
airbnb <- data.table(review_id = text$review_id,
comments = text$comments,
review_scores_rating = text$review_scores_rating)
airbnb$comments <- removeWords(airbnb$comments, c(stopwords('en'),'Boston'))
airbnb$comments <- removePunctuation(airbnb$comments)
airbnb$comments <- stripWhitespace(airbnb$comments)
airbnb$comments <- removeNumbers(airbnb$comments)
airbnb$comments <- tolower(airbnb$comments)
airbnb$comments
text <- read.csv('C:\\Users\\samsung\\Desktop\\Bitamin\\Textmining with R\\5장\\Airbnb-boston_only\\Airbnb-boston_only.csv')
airbnb <- data.table(review_id = text$review_id,
comments = text$comments,
review_scores_rating = text$review_scores_rating)
airbnb$comments <- removeWords(airbnb$comments, c(stopwords('en'),'Boston'))
airbnb$comments <- removePunctuation(airbnb$comments)
airbnb$comments <- stripWhitespace(airbnb$comments)
airbnb$comments <- removeNumbers(airbnb$comments)
airbnb$comments <- tolower(airbnb$comments)
text <- fread('C:\\Users\\samsung\\Desktop\\Bitamin\\Textmining with R\\5장\\Airbnb-boston_only\\Airbnb-boston_only.csv')
airbnb$comments <- removeWords(airbnb$comments, c(stopwords('en'),'Boston'))
airbnb$comments <- removePunctuation(airbnb$comments)
# 경로 설정
setwd("C:\\Users\\samsung\\Desktop\\Bitamin\\Machine Learning with R")
# 파일 불러오기
wbcd <- read.csv("wisc_bc_data.csv", stringsAsFactors=FALSE)
# 파일 구조 check
str(wbcd)
# 경로 설정
setwd("C:\\Users\\samsung\\Desktop\\Bitamin\\Machine Learning with R\\3장. kNN")
# 파일 불러오기
wbcd <- read.csv("wisc_bc_data.csv", stringsAsFactors=FALSE)
# 파일 구조 check
str(wbcd)
# table 확인하기
table(wbcd$diagnosis)
# target 변수를 factor 형식으로
wbcd$diagnosis <- factor(wbcd$diagnosis, levels=c("B","M"), labels=c("Benign","Malignant"))
wbcd$diagnosis
str(wbcd$diagnosis)
# 비율 확인하기
round(prop.table(table(wbcd$diagnosis))*100, digits=1)
# 비율 확인하기
# ( %로 나타내기 위해 100을 곱함 + 소숫점 1째짜리 까지 )
round(prop.table(table(wbcd$diagnosis))*100, digits=1)
# 더 자세히 들여다보자면...
summary(wbcd[c("radius_mean","area_mean","smoothness_mean")])
# 정규화 시킬 함수 생성
normalize <- function(x) { return ((x-min(x)) / (max(x)-min(x))) }
# 정규화 (한번에) 시키기
str(wbcd)
# ID 빼기
wbcd <- wbcd[-1]
str(wbcd)
# 파일 불러오기
# 여성 유방암 관련 data
wbcd <- read.csv("wisc_bc_data.csv", stringsAsFactors=FALSE)
# 파일 구조 check
str(wbcd)
# ID 빼기
wbcd <- wbcd[-1]
# table 확인하기
table(wbcd$diagnosis)
# target 변수를 factor 형식으로
wbcd$diagnosis <- factor(wbcd$diagnosis, levels=c("B","M"), labels=c("Benign","Malignant"))
str(wbcd$diagnosis)
# 비율 확인하기
# ( %로 나타내기 위해 100을 곱함 + 소숫점 1째짜리 까지 )
round(prop.table(table(wbcd$diagnosis))*100, digits=1)
# 더 자세히 들여다보자면...
# 종양의 3가지 특성에 대해 요약 (반지름, 넓이, 굴곡도)
summary(wbcd[c("radius_mean","area_mean","smoothness_mean")])
# 정규화 시킬 함수 생성
normalize <- function(x) { return ((x-min(x)) / (max(x)-min(x))) }
# 정규화 (한번에) 시키기
str(wbcd)
wbcd_n <- as.data.frame(lapply(wbcd[2:31],normalize))
str(wbcd_n)
# Training & Test 데이터로 나누기
wbcd_train <- wbcd_n[1:469,]
wbcd_test <- wbcd_n[470:569,]
# target 변수 불러들어!
wbcd_train_labels <- wbcd[1:469,1]
wbcd_test_labels <- wbcd[470:569,1]
# Model 만들기 (by Training 데이터)
library(class)
wbcd_test_pred <- knn(train=wbcd_train, test=wbcd_test, cl=wbcd_train_labels, k=21)
# Model 평가하기 (by Test 데이터)
library(gmodels)
CrossTable(x=wbcd_test_labels, y=wbcd_test_pred, prop.chisq=FALSE)
# Model 개선하기 ( normalize함수 말고, scale함수 써서(z-score) )
wbcd_z <- as.data.frame(scale(wbcd[-1]))
wbcd_train <- wbcd_z[1:469,]
wbcd_test <- wbcd_z[470:569,]
wbcd_train_labels <- wbcd[1:469,1]
wbcd_test_labels <- wbcd[470:569,1]
wbcd_test_pred <- knn(train=wbcd_train, test=wbcd_test, cl=wbcd_train_labels, k=21)
CrossTable(x=wbcd_test_labels, y=wbcd_test_pred, prop.chisq=FALSE)
# 경로 설정
setwd("C:\\Users\\samsung\\Desktop\\Bitamin\\Machine Learning with R\\3장. kNN")
# 파일 불러오기
# 여성 유방암 관련 data
wbcd <- read.csv("wisc_bc_data.csv", stringsAsFactors=FALSE)
# 파일 구조 check
str(wbcd)
# ID 빼기
wbcd <- wbcd[-1]
# table 확인하기
table(wbcd$diagnosis)
# target 변수를 factor 형식으로
wbcd$diagnosis <- factor(wbcd$diagnosis, levels=c("B","M"), labels=c("Benign","Malignant"))
str(wbcd$diagnosis)
table(wbcd$diagnosis)
s
str(wbcd$diagnosis)
# 비율 확인하기
# ( %로 나타내기 위해 100을 곱함 + 소숫점 1째짜리 까지 )
round(prop.table(table(wbcd$diagnosis))*100, digits=1)
# 비율 확인하기
# ( %로 나타내기 위해 100을 곱함 + 소숫점 1째짜리 까지 )
round(prop.table(table(wbcd$diagnosis))*100, digits=1)
# 더 자세히 들여다보자면...
# 종양의 3가지 특성에 대해 요약 (반지름, 넓이, 굴곡도)
summary(wbcd[c("radius_mean","area_mean","smoothness_mean")])
# 더 자세히 들여다보자면...
# 종양의 3가지 특성에 대해 요약 (반지름, 넓이, 굴곡도)
summary(wbcd[c("radius_mean","area_mean","smoothness_mean")])
# 정규화 시킬 함수 생성
normalize <- function(x) { return ((x-min(x)) / (max(x)-min(x))) }
# 정규화 (한번에) 시키기
# ( 모든 값들이 0~1사이로 변형되었을것임! )
str(wbcd)
wbcd_n <- as.data.frame(lapply(wbcd[2:31],normalize))
# Training & Test 데이터로 나누기
wbcd_train <- wbcd_n[1:469,]
wbcd_test <- wbcd_n[470:569,]
# target 변수 불러들어!
wbcd_train_labels <- wbcd[1:469,1]
wbcd_test_labels <- wbcd[470:569,1]
str(wbcd_train)
str(wbcd_train_labels)
# Training & Test 데이터로 나누기
wbcd_train <- wbcd_n[1:469,]
wbcd_test <- wbcd_n[470:569,]
# target 변수 불러들어!
wbcd_train_labels <- wbcd[1:469,1]
wbcd_test_labels <- wbcd[470:569,1]
# Model 만들기 (by Training 데이터)
# 'class' library가 knn을 가지고 있음
library(class)
wbcd_test_pred <- knn(train=wbcd_train, test=wbcd_test, cl=wbcd_train_labels, k=21)
# Model 평가하기 (by Test 데이터)
library(gmodels)
CrossTable(x=wbcd_test_labels, y=wbcd_test_pred, prop.chisq=FALSE)
# Model 개선하기 ( normalize함수 말고, scale함수 써서(z-score) )
wbcd_z <- as.data.frame(scale(wbcd[-1]))
wbcd_train <- wbcd_z[1:469,]
wbcd_test <- wbcd_z[470:569,]
wbcd_train_labels <- wbcd[1:469,1]
wbcd_test_labels <- wbcd[470:569,1]
wbcd_test_pred <- knn(train=wbcd_train, test=wbcd_test, cl=wbcd_train_labels, k=21)
CrossTable(x=wbcd_test_labels, y=wbcd_test_pred, prop.chisq=FALSE)
# 경로 설정
setwd("C:\\Users\\samsung\\Desktop\\Bitamin\\Machine Learning with R\\3장. kNN")
# 파일 불러오기
# 여성 유방암 관련 data
wbcd <- read.csv("wisc_bc_data.csv", stringsAsFactors=FALSE)
# 파일 구조 check
str(wbcd)
# ID 빼기
wbcd <- wbcd[-1]
# table 확인하기
table(wbcd$diagnosis)
# ID 빼기
wbcd <- wbcd[-1]
