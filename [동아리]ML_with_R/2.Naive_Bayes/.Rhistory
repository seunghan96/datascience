comments = text$comments,
review_scores_rating = text$review_scores_rating)
airbnb$comments <- removeWords(airbnb$comments, c(stopwords('en'),'Boston'))
airbnb$comments <- removePunctuation(airbnb$comments)
airbnb$comments <- stripWhitespace(airbnb$comments)
airbnb$comments <- removeNumbers(airbnb$comments)
airbnb$comments <- tolower(airbnb$comments)
airbnb$comments <- tolower(airbnb$comments)
airbnb$comments <- tolower(airbnb$comments)
options(stringsAsFactors = F)
text <- fread('C:\\Users\\samsung\\Desktop\\Bitamin\\Textmining with R\\5장\\Airbnb-boston_only\\Airbnb-boston_only.csv')
airbnb <- data.table(review_id = text$review_id,
comments = text$comments,
review_scores_rating = text$review_scores_rating)
airbnb$comments <- removeWords(airbnb$comments, c(stopwords('en'),'Boston'))
airbnb$comments <- removePunctuation(airbnb$comments)
airbnb$comments <- stripWhitespace(airbnb$comments)
airbnb$comments <- removeNumbers(airbnb$comments)
airbnb$comments <- tolower(airbnb$comments)
airbnb$comments
text <- read.csv('C:\\Users\\samsung\\Desktop\\Bitamin\\Textmining with R\\5장\\Airbnb-boston_only\\Airbnb-boston_only.csv')
airbnb <- data.table(review_id = text$review_id,
comments = text$comments,
review_scores_rating = text$review_scores_rating)
airbnb$comments <- removeWords(airbnb$comments, c(stopwords('en'),'Boston'))
airbnb$comments <- removePunctuation(airbnb$comments)
airbnb$comments <- stripWhitespace(airbnb$comments)
airbnb$comments <- removeNumbers(airbnb$comments)
airbnb$comments <- tolower(airbnb$comments)
text <- fread('C:\\Users\\samsung\\Desktop\\Bitamin\\Textmining with R\\5장\\Airbnb-boston_only\\Airbnb-boston_only.csv')
airbnb$comments <- removeWords(airbnb$comments, c(stopwords('en'),'Boston'))
airbnb$comments <- removePunctuation(airbnb$comments)
install.packages("C:\\Users\\samsung\\Desktop\\Bitamin\\Textmining with R\\4장. Sentiment Scoring\\Rstem_0.4-1 (1).tar.gz", repos=NULL, type='source')
install.packages("C:\\Users\\samsung\\Desktop\\Bitamin\\Textmining with R\\4장. Sentiment Scoring\\Rstem_0.4-1 (1).tar.gz", repos=NULL, type='source',fileEncoding="latin1")
Sys.setlocale(category = "LC_ALL", locale = "us")
install.packages("C:\\Users\\samsung\\Desktop\\Bitamin\\Textmining with R\\4장. Sentiment Scoring\\Rstem_0.4-1 (1).tar.gz", repos=NULL, type='source')
library(Rstem)
Sys.setlocale(category = "LC_ALL", locale = "us")
install.packages("C:\\Users\\samsung\\Desktop\\Bitamin\\Textmining with R\\4장. Sentiment Scoring\\Rstem_0.4-1 (1).tar.gz", repos=NULL, type='source')
library(qdap)
# 기본 설정
options(stringsAsFactors = F)
library(tm)
library(qdap)
library(wordcloud)
library(ggplot2)
library(ggthemes)
library(qdap)
# 기본 설정
options(stringsAsFactors = F)
library(tm)
library(qdap)
library(wordcloud)
library(ggplot2)
library(ggthemes)
install.packages("C:\\Users\\samsung\\Desktop\\Bitamin\\Textmining with R\\4장. Sentiment Scoring\\Rstem_0.4-1 (1).tar.gz", repos=NULL, type='source')
install.packages("C:\\Users\\samsung\\Desktop\\Bitamin\\Textmining with R\\4. Sentiment Scoring\\Rstem_0.4-1 (1).tar.gz", repos=NULL, type='source')
install.packages("C:\\Users\\samsung\\Desktop\\Bitamin\\Textmining with R\\4. Sentiment Scoring\\Rstem_0.4-1.tar.gz", repos=NULL, type='source')
library(Rstem)
install.packages("C:\\Users\\samsung\\Desktop\\Bitamin\\Textmining with R\\4. Sentiment Scoring\\Rstem_0.4-1.tar.gz", repos=NULL, type='source')
library(Rstem)
install.packages("C:\\Users\\samsung\\Desktop\\Bitamin\\Textmining with R\\4. Sentiment Scoring\\Rstem_0.4-1.tar.gz", repos=NULL, type="source")
install.packages("C:\\Users\\samsung\\Desktop\\Bitamin\\Textmining with R\\4.SentimentScoring\\Rstem_0.4-1.tar.gz", repos=NULL, type="source")
install.packages("C:\\Users\\samsung\\Desktop\\Bitamin\\TextminingwithR\\4.SentimentScoring\\Rstem_0.4-1.tar.gz", repos=NULL, type="source")
library(Rstem)
install.packages("C:\\Users\\samsung\\Desktop\\Bitamin\\Textmining with R\\4.Sentiment Scoring\\Rstem_0.4-1.tar.gz", repos=NULL, type="source")
install.packages("C:\\Users\\samsung\\Desktop\\Rstem_0.4-1.tar.gz", repos=NULL, type="source")
########
행복한공부를시작해볼까!
#####
print('A')
########
행복한공부를시작해볼까!
#####
print("아리야")
print("미팅잡아줘서")
print("고마워")
library(tm)
library(Matrix)
install.packages("glmnet")
library(glmnet)
library(caret)
library(pROC)
library(ggthemes)
library(ggplot2)
library(arm)
install.packages('arm')
library(arm)
headline.clean <- function(x) {
x <- tolower(x)
x <- removeWords(x, stopwords('en'))
x <- removePunctuation(x)
x <- stripWhitespace(x)
return(x)
}
install.packages("RTextTiiks")
install.packages("RTextTools")
library(RTextTools)
trace("create_matrix", edit=T)
# GLMnet Training
raw.headlines<-readLines('https://raw.githubusercontent.com/kwartler/text_mining/master/all_3k_headlines.csv',
encoding='latin1')[-1]
# GLMnet Training
headlines<-as_tibble(read.csv(url('https://raw.githubusercontent.com/kwartler/text_mining/master/all_3k_headlines.csv'),encoding='latin1'))
# GLMnet Training
raw.headlines<-readLines('https://raw.githubusercontent.com/kwartler/text_mining/master/all_3k_headlines.csv',
encoding='latin1')[-1]
text<-str_split(raw.headlines, 'http{1}', simplify=TRUE)[,1]
url_site_y<-paste('http',str_split(raw.headlines, 'http{1}', simplify=TRUE)[,2],sep='')
url_site_y<-str_split(url_site_y,',',simplify=TRUE)
url<-url_site_y[,1]
site<-url_site_y[,2]
y<-url_site_y[,3]
headlines<-tibble(
headline=text,
url=url,
site=site,
y=y
)
library(tibble)
headlines<-tibble(
headline=text,
url=url,
site=site,
y=y
)
raw.headlines<-readLines('https://raw.githubusercontent.com/kwartler/text_mining/master/all_3k_headlines.csv',
encoding='latin1')[-1]
text<-str_split(raw.headlines, 'http{1}', simplify=TRUE)[,1]
library(stringr)
text<-str_split(raw.headlines, 'http{1}', simplify=TRUE)[,1]
url_site_y<-paste('http',str_split(raw.headlines, 'http{1}', simplify=TRUE)[,2],sep='')
url_site_y<-str_split(url_site_y,',',simplify=TRUE)
url<-url_site_y[,1]
site<-url_site_y[,2]
y<-url_site_y[,3]
headlines<-tibble(
headline=text,
url=url,
site=site,
y=y
)
train <- createDataPartition(headlines$y, p=0.5, list=F)
train.headlines <- headlines[train,]
test.headlines <- headlines[-train,]
clean.train <- headline.clean(train.headlines$headline)
train.dtm <- match.matrix(clean.train, weighting=tm::weightTfIdf())
train.dtm <- match.matrix(clean.train, weighting=tm::weightTfIdf
train.dtm <- match.matrix(clean.train, weighting=tm::weightTfIdf)
train.dtm <- match.matrix(clean.train, weighting=tm::weightTfIdf)
match.matrix <- function(text.col, original.matrix=NULL, weighting=weightTf
match.matrix <- function(text.col, original.matrix=NULL, weighting=weightTf)
{
control <- list(weighting=weighting)
training.col <- sapply(as.vector(text.col, mode='character'),iconv, to='UTF8', sub='byte')
corpus <- VCorpus(VectorSource(training.col))
matrix <- DocumentTermMatrix(corpus, control=control);
if (!is.null(original.matrix)) {
terms <- colnames(original.matrix[,
which(!colnames(original.matrix) %in% colnames(matrix))])
weight <- 0
if (attr(original.matrix, "weighting")[2] == "tfidf")
weight <- 0.000000001
amat <- matrix(weight, nrow=nrow(matrix),
ncol=length(terms))
colnames(amat) <- terms
rownames(amat) <- rownames(matrix)
fixed <- as.DocumentTermMatrix(cbind(matrix[,which(colnames(matrix) %in% colnames(original.matrix))],amat),
weighting=weighting)
matrix <- fixed
}
matrix <- matrix[,sort(clonames(matrix))]
gc()
return(matrix)
}
train.dtm <- match.matrix(clean.train, weighting=tm::weightTfIdf)
match.matrix <- function(text.col, original.matrix=NULL, weighting=weightTf)
{
control <- list(weighting=weighting)
training.col <- sapply(as.vector(text.col, mode='character'),iconv, to='UTF8', sub='byte')
corpus <- VCorpus(VectorSource(training.col))
matrix <- DocumentTermMatrix(corpus, control=control);
if (!is.null(original.matrix)) {
terms <- colnames(original.matrix[,
which(!colnames(original.matrix) %in% colnames(matrix))])
weight <- 0
if (attr(original.matrix, "weighting")[2] == "tfidf")
weight <- 0.000000001
amat <- matrix(weight, nrow=nrow(matrix),
ncol=length(terms))
colnames(amat) <- terms
rownames(amat) <- rownames(matrix)
fixed <- as.DocumentTermMatrix(cbind(matrix[,which(colnames(matrix) %in% colnames(original.matrix))],amat),
weighting=weighting)
matrix <- fixed
}
matrix <- matrix[,sort(colnames(matrix))]
gc()
return(matrix)
}
train.dtm <- match.matrix(clean.train, weighting=tm::weightTfIdf)
train.matrix <- as.matrix(train.dtm)
train.matrix <- Matrix(train.matrix, sparse=T)
dim(train.matrix)
train.matrix[1:5,1:25]
cv <- cv.glmnet(train.matrix,
y=as.factor(train.headlines$y), alpha=1,
family='binomial', nfolds=10, intercept=F,
type.measure='class')
plot(cv)
cv <- cv.glmnet(train.matrix,
y=as.factor(train.headlines$y), alpha=1,
family='binomial', nfolds=10, intercept=F,
type.measure='class')
train <- createDataPartition(headlines$y, p=0.5, list=F)
train.headlines <- headlines[train,]
test.headlines <- headlines[-train,]
clean.train <- headline.clean(train.headlines$headline)
train.dtm <- match.matrix(clean.train, weighting=tm::weightTfIdf)
train.matrix <- as.matrix(train.dtm)
train.matrix <- Matrix(train.matrix, sparse=T)
dim(train.matrix)
train.matrix[1:5,1:25]
cv <- cv.glmnet(train.matrix,
y=as.factor(train.headlines$y), alpha=1,
family='binomial', nfolds=10, intercept=F,
type.measure='class')
install.packages("openNLP")
install.packages("openNLPmodels.en", repos="http://datacube.wu.ac/", type='source')
options(stringsAsFactors=FALSE)
Sys.setlocale('LC_ALL','C')
library(gridExtra        )
library(gridExtra)
library(ggmap)
library(ggthemes)
library(NLP)
library(openNLP)
library("openNLPmodels.en", lib.loc='~/R/win-library/3.2')
library("openNLPmodels.en", lib.loc="~/R/win-library/3.2")
library(pbapply)
library(stringr)
library(rvest)
library(doBy)
library(tm)
library(cshapes)
install.packages("cshapes")
library(cshapes)
library(cshapes)
library(tm)
library(doBy)
library(rvest)
library(stringr)
library(gridExtra)
library(ggmap)
library(ggthemes)
library(NLP)
library(openNLP)
library("openNLPmodels.en", lib.loc="~/R/win-library/3.2")
library(pbapply)
library(stringr)
library(rvest)
temp <- list.files(pattern='*.txt')
for (i in 1:length(temp)) assign(tmp[i], readLines(temp[i]))
all.emails <- pblapply(temp,get)
library(rvest)
url<-'http://www.amazon.com/gp/help/customer/forums/ ref=cs_hc_g_tv?i.e.=UTF8&forumID=Fx1SKFFP8U1B6N5&cdThr ead=Tx3JJLVOS6N6YSD'
page<-read_html(url)
a <- read.csv('C:\Users\samsung\Desktop\kbo 예측대회\drive-download-20190215T051311Z-001\data')
a <- read.csv('C:\\Users\\samsung\\Desktop\\kbo 예측대회\\drive-download-20190215T051311Z-001\\data\\상반기.csv')
head(a)
a['batter_id']
a['batter_id'].unique
a['batter_id'].unique()
unique(a['batter_id'])
b <- unique(a['batter_id'])
b
상반 <- read.csv('C:\\Users\\samsung\\Desktop\\kbo 예측대회\\drive-download-20190215T051311Z-001\\data\\상반기.csv')
상반기 <- read.csv('C:\\Users\\samsung\\Desktop\\kbo 예측대회\\drive-download-20190215T051311Z-001\\data\\상반기.csv')
전체 <- read.csv('C:\\Users\\samsung\\Desktop\\kbo 예측대회\\drive-download-20190215T051311Z-001\\data\\yearly.csv')
head(전)
head(전체)
x <- data.frame('a'=전체['batter'])
x
head(전체)
전체[전체['year']=2017]
전체[전체['year']==2017]
전체2017 <- 전체[전체['year']==2017]
head(전체2017)
전체2017
전체2017[1:5]
a = c(1,2,3,4,5)
a
a[a==5]
a[a==c(1,3)]
전체2017 <- 전체[전체['year']==2017]
전체2017
전체2017 <- 전체[전체['year']==2017,]
전체2017
전체2018 <- 전체[전체['year']==2018,]
head(전체2018)
head(전체2018)
전체2016 <-
x <- data.frame('id'=전체2018['batter_id'],'2014P'=전체2014['P'])
전체2018 <- 전체[전체['year']==2018,]
전체2017 <- 전체[전체['year']==2017,]
전체2016 <- 전체[전체['year']==2016,]
전체2014 <- 전체[전체['year']==2014,]
전체2015 <- 전체[전체['year']==2015,]
head(전체2017)
x <- data.frame('id'=전체2018['batter_id'],'2014P'=전체2014['P'])
len(전체2014['P'])
전체2014['P']
전체2014['P'].count
전체2014['P'].counts
count.fields(전체2014['P'])
x <- data.frame('id'=전체2018['batter_id'],'2014P'=전체2014['P'])
x <- data.frame('id'=전체2018['batter_id'])
x
전체2018
columns(전체2018)
colnames(전체2018)
전체2018_2 < -전체2018['batter_id','P','G','PA','AB','R','H','X2B','X3B','HR']
전체2018_2 <- 전체2018['batter_id','P','G','PA','AB','R','H','X2B','X3B','HR']
전체2018['batter_id']
전체2018['batter_id','P']
library(dplyr)
전체2018 %>% select(1:3)
전체2018 %>% select(1,3)
colnames(전체2018)
전체2018 %>% select(1,7:29)
전체2018_2 <- 전체2018 %>% select(1)
전체2018_2
전체2018_2 <- 전체2018 %>% select(1)
전체2017_2 <- 전체2017 %>% select(1,7:29)
전체2016_2 <- 전체2016 %>% select(1,7:29)
전체2015_2 <- 전체2015 %>% select(1,7:29)
전체2014_2 <- 전체2014 %>% select(1,7:29)
merge(전체2017_2,전체2016_2)
전체2014_2
for i in (2015,2016):
print(i)
for i in (2015,2016):
print(i)
for i in c(2015,2016):
print(i)
for k in c(2015,2016):
print(k)
for k in c(2015,2016):
print(k)
colnames(전체2017_2)
for (k in colnames(전체2014_2)){
for (i in paste0(k,2014:2017)){
d=get(i)
colnames(d)[10]=i
assign(i,d)
}
for (k in colnames(전체2014_2)){
for (i in paste0(k,2014:2017)){
d=get(i)
colnames(d)[10]=i
assign(i,d)
}
}
for (k in colnames(전체2014_2)){
print(k)
}
for (k in colnames(전체2014_2)){
for (k in colnames(전체2014_2)){
print(전체2017_2[k])
}
for (k in colnames(전체2014_2)){
colnames(전체2017_2) <- paste0(2017,k)
}
전체2017_2
for (k in colnames(전체2014_2)){
colnames(전체2017_2[k]) <- paste0(2017,k)
}
전체2017_2
전체2017_2 <- 전체2017 %>% select(1,7:29)
전체2017_2
colnames(전체2017_2)
colnames(전체2014_2)
for (k in colnames(전체2014_2)){
print(paste0(2017,k))
}
for (k in colnames(전체2014_2)){
names(전체2017_2)[names(전체2017_2)==k] <- paste0(2017,k)
names(전체2016_2)[names(전체2016_2)==k] <- paste0(2016,k)
names(전체2015_2)[names(전체2015_2)==k] <- paste0(2015,k)
names(전체2014_2)[names(전체2014_2)==k] <- paste0(2014,k)
}
전체2017_2
merge(전체2017_2,전체2016_2)
a <- merge(전체2017_2,전체2016_2)
a
names(전체2017_2)[names(전체2017_2)=='2017batter_id'] <- 'batter_id'
names(전체2017_2)[names(전체2017_2)=='2017batter_id'] <- 'batter_id'
names(전체2016_2)[names(전체2017_2)=='2017batter_id'] <- 'batter_id'
names(전체2014_2)[names(전체2017_2)=='2017batter_id'] <- 'batter_id'
names(전체2015_2)[names(전체2017_2)=='2017batter_id'] <- 'batter_id'
head(전체2018_2)
head(전체2018_2)
전체2018_2
전체2018_2
전체2017_2
3+5
print(전체2017_2)
# 텍스트 마이닝 할 때!!!!!!!!!!
# 뭔진 몰라도 그냥 해 ㅎㅎㅎ
Sys.setlocale(category='LC_ALL')
options(stringsAsFactors=F)
# 경로 설정 후 파일불러오기
setwd("C:\\Users\\samsung\\Desktop\\Bitamin\\Machine Learning with R\\4장. Naive Bayes")
sms_raw <- read.csv("sms_spam.csv",stringsAsFactors=FALSE)
# 구조확인
str(sms_raw)
# vector -> factor
sms_raw$type <- factor(sms_raw$type)
# 빈도 확인하기
# ( ham : spam 빈도수 )
table(sms_raw$type)
# Corpus 생성하기
library(tm)
sms_corpus <- VCorpus(VectorSource(sms_raw$text))
# 특정 message 자세히보기
sms_raw$text[1]
inspect(sms_corpus[1:2])
# 실제 message로 풀어서보기
lapply(sms_corpus[1:2], as.character)
# all 소문자로
sms_corpus_clean <- tm_map(sms_corpus, content_transformer(tolower))
# 숫자 없애기
sms_corpus_clean <- tm_map(sms_corpus_clean, removeNumbers)
# 쓸데없는 단어 없애기
sms_corpus_clean <- tm_map(sms_corpus_clean, removeWords, stopwords())
# punctuation 없애기
sms_corpus_clean <- tm_map(sms_corpus_clean, removePunctuation)
# wordstem (어근)
library(SnowballC)
sms_corpus_clean <- tm_map(sms_corpus_clean, stemDocument)
# stripWhitespace()
sms_corpus_clean <- tm_map(sms_corpus_clean, stripWhitespace)
# DTM (Document Term Matrix)
sms_dtm <- DocumentTermMatrix(sms_corpus_clean)
# 데이터셋 나누기
sms_dtm_train <- sms_dtm[1:4169,]
sms_dtm_test <- sms_dtm[4170:5559,]
sms_train_labels <- sms_raw[1:4169,]$type
sms_test_labels <- sms_raw[4170:5559,]$type
# train/test 데이터에 속한 ham & spam 비율
prop.table(table(sms_train_labels))
prop.table(table(sms_test_labels))
# Wordcloud 만들기
# ( 최소 50번 이상 등장한 단어들 )
library(wordcloud)
wordcloud(sms_corpus_clean, min.freq=50, random.order=FALSE)
spam <- subset(sms_raw, type=="spam")
ham <- subset(sms_raw, type=="ham")
wordcloud(spam$text, max.words=40,scale=c(3,0.5))
wordcloud(ham$text, max.words=40,scale=c(3,0.5))
# 자주나오는 단어들 ( top 5 )
sms_freq_words <- findFreqTerms(sms_dtm_train, 5)
sms_dtm_freq_train <- sms_dtm_train[,sms_freq_words]
sms_dtm_freq_test <- sms_dtm_test[,sms_freq_words]
# 수치형 -> 카테고리형
convert_counts <- function(x) {x<-ifelse(x>0,"Yes","No")}
# 적용
sms_train <- apply(sms_dtm_freq_train, MARGIN=2, convert_counts)
sms_test <- apply(sms_dtm_freq_test, MARGIN=2, convert_counts)
# 모델 생성
# e1071 library는 naiveBayes 위해서
library(e1071)
sms_classifier <- naiveBayes(sms_train, sms_train_labels)
# 자주나오는 단어들 ( top 5 )
sms_freq_words <- findFreqTerms(sms_dtm_train, 5)
sms_freq_words
sms_dtm_freq_train
# 수치형 -> 카테고리형
convert_counts <- function(x) {x<-ifelse(x>0,"Yes","No")}
sms_train
# 자주나오는 단어들 ( 5번 이상 등장  )
sms_freq_words <- findFreqTerms(sms_dtm_train, 5)
sms_freq_words
sms_dtm_freq_train <- sms_dtm_train[,sms_freq_words]
sms_dtm_freq_train
inspect(sms_dtm_freq_train)
# 자주나오는 단어들 ( 5번 이상 등장  )
sms_freq_words <- findFreqTerms(sms_dtm_train, 5)
sms_freq_words
sms_dtm_freq_train
sms_dtm_freq_test <- sms_dtm_test[,sms_freq_words]
sms_dtm_freq_test
inspect(sms_dtm_freq_train)
# 수치형 -> 카테고리형
convert_counts <- function(x) {x<-ifelse(x>0,"Yes","No")}
# 적용
sms_train <- apply(sms_dtm_freq_train, MARGIN=2, convert_counts)
sms_train
sms_dtm_freq_train
# 모델 생성
# e1071 library는 naiveBayes 위해서
library(e1071)
sms_classifier <- naiveBayes(sms_train, sms_train_labels)
# 예측하기
sms_test_pred <- predict(sms_classifier, sms_test)
# CrossTable 만들기
library(gmodels)
CrossTable(sms_test_pred,sms_test_labels, prop.chisq=FALSE, prop.t=FALSE, dnn=c("predicted","actual"))
# 모델 개선하기 (Laplace 사용)
sms_classifier2 <- naiveBayes(sms_train,sms_train_labels,laplace=1)
sms_test_pred2 <- predict(sms_classifier2,sms_test)
CrossTable(sms_test_pred2,sms_test_labels, prop.chisq=FALSE, prop.t=FALSE, prop.r=FALSE, dnn=c("predicted","actual"))
