fit <- lda.collapsed.gibbs.sampler(documents=documents$documents,
K=k,
vocab=documents$vocab,
num.iterations = num.iter,
alpha=alpha,
eta=eta,
initial=NULL,
burnin=0,
compute.log.likelihood = TRUE
)
eta <- 0.02
set.seed(1234)
# LDA 모델 만들기
k <- 4
num.iter <- 25
alpha <- 0.02
eta <- 0.02
set.seed(1234)
fit <- lda.collapsed.gibbs.sampler(documents=documents$documents,
K=k,
vocab=documents$vocab,
num.iterations = num.iter,
alpha=alpha,
eta=eta,
initial=NULL,
burnin=0,
compute.log.likelihood = TRUE
)
# LDA Topic Modeling Explained
library(tm)
library(qdap)
library(GuardianR)
library(lda)
library(pbapply)
library(LDAvis)
library(treemap)
library(car)
fit <- lda.collapsed.gibbs.sampler(documents=documents$documents,
K=k,
vocab=documents$vocab,
num.iterations = num.iter,
alpha=alpha,
eta=eta,
initial=NULL,
burnin=0,
compute.log.likelihood = TRUE
)
options(stringsAsFactors = F)
text <- read.csv('C:\\Users\\samsung\\Desktop\\Bitamin\\Textmining with R\\5장\\guardian.csv')
articles <- iconv(text$body, "latin1",'ASCII',sub="")
articles <- gsub('http\\S+\\s*','',articles)
articles <- bracketX(articles, bracket='all')
articles <- removeNumbers(articles)
articles <- tolower(articles)
articles <- removeWords(articles,c(stopwords('en'), 'pakistan','gmt','england'))
blank.removal <- function(x) {
x <- unlist(strsplit(x,' '))
x <- subset(x, nchar(x)>0)
x <- paste(x,collapse=' ')
}
articles <- pblapply(articles, blank.removal)
strsplit(ex.text[1],' ')
char.vec <- unlist(strsplit(ex.text[1],' '))
char.vec <- subset(char.vec, nchar(char.vec)>0)
char.vec <- paste(char.vec, collapse=' ')
char.vec
# example로 훈련하기 (2)
ex.text <- c('this is a text document')
ex.text.lex
ex.text.lex <- lexicalize(ex.text)
char.vec
char.vec
# example로 훈련하기 (1)
ex.text <- c('Text mining is a     good time', 'Text mining is a good time')
articles <- gsub("[[:punct:]]","",articles)
strsplit(ex.text[2],' ')
ex.text <- c('this is a text document','text mining a text document is great')
ex.text.lex <- lexicalize(ex.text)
ex.text.lex
# example 끝내고 다시 본문으로!
documents <- lexicalize(articles)
wc <- word.counts(documents$documents, documents$vocab)
doc.length <- document.lengths(documents$documents)
# LDA 모델 만들기
k <- 4
num.iter <- 25
alpha <- 0.02
eta <- 0.02
set.seed(1234)
fit <- lda.collapsed.gibbs.sampler(documents=documents$documents,
K=k,
vocab=documents$vocab,
num.iterations = num.iter,
alpha=alpha,
eta=eta,
initial=NULL,
burnin=0,
compute.log.likelihood = TRUE
)
str(fit)
plot(fit$log.likelihoods[1,])
top.topic.words(fit$topics,7,by.score=TRUE)
# 각 주제를 잘 대표하는 문서는 어떤 문서?
top.topic.documents(fit$document_sums,1)
theta <- t(pbapply(fit$document_sums + alpha, 2, function(x) x/sum(x)))
phi <- t(pbapply(t(fit$topics) + eta, 2, function(x) x/sum(x)))
article.json <- createJSON(phi=phi, theta=theta, doc.length=doc.length, vocab=document$vocab, term.frequency = as.vector(wc))
article.json <- createJSON(phi=phi, theta=theta, doc.length=doc.length, vocab=documents$vocab, term.frequency = as.vector(wc))
serVis(article.json)
article.json <- createJSON(phi=phi, theta=theta, doc.length=doc.length, vocab=documents$vocab, term.frequency = as.vector(wc))
# 각 주제를 잘 대표하는 문서는 어떤 문서?
# 주제1은 21번 문서가 잘 나타냄 / 주제2는 ....
top.topic.documents(fit$document_sums,1)
x <- table(x)
doc.assignment <- function(x) {
x <- table(x)
x <- as.matrix(x)
x <- t(x)
x <- max.col(x)
}
fit$assignments[[2]][1:10]
table(fit$assignments[[2]][1:10])
t(as.matrix(table(fit$assignments[[2]][1:10]))
t(as.matrix(table(fit$assignments[[2]][1:10])))
t(as.matrix(table(fit$assignments[[2]][1:10])))
max.col(t(as.matrix(table(fit$assignments[[2]][1:10]))))
assignments <- unlist(pblapply(fit$assignments,doc.assignment))
assignments <- recode(assignments, "1='Cricket1' ; 2='Paris Attacks'; 3='Cricket2', 4='Unknown'")
assignments <- recode(assignments, "1='Cricket1' ; 2='Paris Attacks'; 3='Cricket2'; 4='Unknown'")
article.ref <- seq(1:nrow(text))
article.pol <- polarity(articles)[[1]][3]
article.tree.df <- cbind(article.ref, article.pol, doc.length, assignments)
treemap(article.tree.df, index=c('assignments','article.ref'),
vSize='doc.length', vColor='polarity',type='value',
title='Guardian Articles mentioning Pakistan',
palette=c('red','white','green'))
# Text to Vectors using text2vec
library(data.table)
install.packages('text2vec')
library(text2vec)
library(tm)
text <- fread('Airbnb-boston_only.csv')
text <- fread('C:\\Users\\samsung\\Desktop\\Bitamin\\Textmining with R\\5장\\Airbnb-boston_only\\Airbnb-boston_only.csv')
airbnb <- data.table(review_id = text$review_id,
comments = text$comments,
review_scores_rating = text$review_scores_rating)
airbnb$comments <- removeWords(airbnb$comments, c(stopwords('en'),'Boston'))
airbnb$comments <- removePunctuation(airbnb$comments)
airbnb$comments <- stripWhitespace(airbnb$comments)
airbnb$comments <- removeNumbers(airbnb$comments)
airbnb$comments <- tolower(airbnb$comments)
airbnb$comments <- tolower(airbnb$comments)
airbnb$comments <- tolower(airbnb$comments)
options(stringsAsFactors = F)
text <- fread('C:\\Users\\samsung\\Desktop\\Bitamin\\Textmining with R\\5장\\Airbnb-boston_only\\Airbnb-boston_only.csv')
airbnb <- data.table(review_id = text$review_id,
comments = text$comments,
review_scores_rating = text$review_scores_rating)
airbnb$comments <- removeWords(airbnb$comments, c(stopwords('en'),'Boston'))
airbnb$comments <- removePunctuation(airbnb$comments)
airbnb$comments <- stripWhitespace(airbnb$comments)
airbnb$comments <- removeNumbers(airbnb$comments)
airbnb$comments <- tolower(airbnb$comments)
airbnb$comments
text <- read.csv('C:\\Users\\samsung\\Desktop\\Bitamin\\Textmining with R\\5장\\Airbnb-boston_only\\Airbnb-boston_only.csv')
airbnb <- data.table(review_id = text$review_id,
comments = text$comments,
review_scores_rating = text$review_scores_rating)
airbnb$comments <- removeWords(airbnb$comments, c(stopwords('en'),'Boston'))
airbnb$comments <- removePunctuation(airbnb$comments)
airbnb$comments <- stripWhitespace(airbnb$comments)
airbnb$comments <- removeNumbers(airbnb$comments)
airbnb$comments <- tolower(airbnb$comments)
text <- fread('C:\\Users\\samsung\\Desktop\\Bitamin\\Textmining with R\\5장\\Airbnb-boston_only\\Airbnb-boston_only.csv')
airbnb$comments <- removeWords(airbnb$comments, c(stopwords('en'),'Boston'))
airbnb$comments <- removePunctuation(airbnb$comments)
# 엔트로피 지수 (예시)
# 빨간색 60%, 파란색 40%로 이루어진 data
-0.60*log2(0.60) - 0.40*log2(0.40)
# 엔트로피 지수 곡선
curve(-x*log2(x)-(1-x)*log(1-x), col='red',xlab='x',ylab='Entropy',lwd=4)
# 경로 설정
setwd("C:\\Users\\samsung\\Desktop\\Bitamin\\Machine Learning with R\\5장. Decision Tree & Rules")
# 파일 불러오기
credit <- read.csv("credit.csv")
# 구조 & 타깃의 table 확인하기
str(credit)
table(credit$default)
table(credit$default)
# 구조 & 타깃의 table 확인하기
str(credit)
# (1) 데이터 나누기
set.seed(123)
train_sample <- sample(1000,900)
credit_train <- credit[train_sample,]
credit_test <- credit[-train_sample,]
# 비율 점검 (고르게 섞였는지)
prop.table(table(credit_train$default))
prop.table(table(credit_test$default))
# (2) 학습시키기
library(C50)
credit_model <- C5.0(credit_train[-17], credit_train$default)
# 들여다보기
summary(credit_model)
# (3) 평가하기
library(gmodels)
CrossTable(credit_test$default, credit_pred, prop.chisq=FALSE, prop.c=FALSE, prop.r=FALSE, dnn=c('actual default','predicted default'))
credit_pred <- predict(credit_model, credit_test)
CrossTable(credit_test$default, credit_pred, prop.chisq=FALSE, prop.c=FALSE, prop.r=FALSE, dnn=c('actual default','predicted default'))
# (4) 모델 개선 by 부스팅
credit_boost10 <- C5.0(credit_train[-17], credit_train$default, trials=10)
summary(credit_boost10)
credit_boost_pred10 <- predict(credit_boost10, credit_test)
CrossTable(credit_test$default, credit_boost_pred10, prop.chisq=FALSE, prop.c=FALSE, prop.r=FALSE, dnn=c('actual default','predicted default'))
CrossTable(credit_test$default, credit_pred, prop.chisq=FALSE, prop.c=FALSE, prop.r=FALSE, dnn=c('actual default','predicted default'))
CrossTable(credit_test$default, credit_pred, prop.chisq=FALSE, prop.c=FALSE, prop.r=FALSE, dnn=c('actual default','predicted default'))
CrossTable(credit_test$default, credit_pred, prop.chisq=FALSE, prop.c=FALSE, prop.r=FALSE, dnn=c('actual default','predicted default'))
CrossTable(credit_test$default, credit_boost_pred10, prop.chisq=FALSE, prop.c=FALSE, prop.r=FALSE, dnn=c('actual default','predicted default'))
# (5) cost matrix 작성 1
matrix_dimensions <- list(c('no','yes'),c('no','yes'))
names(matrix_dimensions) <- c('predicted','actual')
error_cost <- matrix(c(0,1,4,0),nrow=2, dimnames=matrix_dimensions)
# (5) cost matrix 작성 1
matrix_dimensions <- list(c('no','yes'),c('no','yes'))
names(matrix_dimensions) <- c('predicted','actual')
matrix_dimensions
error_cost <- matrix(c(0,1,4,0),nrow=2, dimnames=matrix_dimensions)
# (6) cost matrix 작성 2
credit_cost <- C5.0(credit_train[-17],credit_train$default, costs=error_cost)
credit_cost_pred <- predict(credit_cost, credit_test)
CrossTable(credit_test$default, credit_cost_pred, prop.chisq=FALSE, prop.c=FALSE, prop.r=FALSE, dnn=c('actual default','predicted default'))
# 파일 불러오기
mushrooms <- read.csv("mushrooms.csv", stringsAsFactors=TRUE)
mushrooms$veil_type <- NULL
# 비율 확인하기
table(mushrooms$type)
# (1) 학습시키기
library(RWeka)
mushroom_1R <- OneR(type~.,data=mushrooms)
# (2) 평가하기
summary(mushroom_1R)
# (3) 개선하기 (RIPPER 사용)
mushroom_JRip <- JRip(type~.,data=mushrooms)
mushroom_JRip
# 경로설정
setwd("C:\\Users\\samsung\\Desktop\\Bitamin\\Machine Learning with R\\6장. Regression")
# 파일 불러오기
launch <- read.csv("challenger.csv")
# 계수 & 절편(오차)
b <- cov(launch$temperature, launch$distress_ct) / var(launch$temperature)
a <- mean(launch$distress_ct) - b*mean(launch$temperature)
# 피어슨 상관계수
r <- cov(launch$temperature,launch$distress_ct) / (sd(launch$temperature)*sd(launch$distress_ct))
cor(launch$temperature, launch$distress_ct)
# 다중 선형회귀
reg <- function(y,x) {
x <- as.matrix(x)
x <- cbind(Intercept=1, x)
b <- solve(t(x)%*%x)%*%t(x)%*%y
colnames(b) <- 'estimate'
print(b)
}
# 구조 확인
str(launch)
# 분석하기
reg(y=launch$distress_ct,x=launch[2])
reg(y=launch$distress_ct,x=launch[2:4])
# 파일 불러오기
insurance <- read.csv("insurance.csv",stringsAsFactors=TRUE)
# 구조 확인
str(insurance)
# summary & histogram
summary(insurance$charges)
hist(insurance$charges)
# 비율 확인 (지역별)
table(insurance$region)
# 상관행렬
cor(insurance[c("age","bmi","children","charges")])
# 산점도행렬
pairs(insurance[c('age','bmi','children','charges')])
library(psych)
pairs.panels(insurance[c('age','bmi','children','charges')])
# 학습하기
ins_model <- lm(charges~.,data=insurance)
ins_model
# 평가하기
summary(ins_model)
# 개선하기 ( 비선형, 새로운 항, 이진데이터, 상호작용 .... )
insurance$age2 <- insurance$age^2
insurance$bmi30 <- ifelse(insurance$bmi>=30, 1, 0)
ins_model2 <- lm(charges~age+age2+children+bmi+sex+bmi30*smoker+region, data=insurance)
summary(ins_model2)
# 데이터 불러오기
wine <- read.csv("whitewines.csv")
# 구조 확인
str(wine)
# histogram
hist(wine$quality)
# 경로설정
setwd("C:\\Users\\samsung\\Desktop\\Bitamin\\Machine Learning with R\\6장. Regression")
# 데이터 나누기
wine_train <- wine[1:3750,]
wine_test <- wine[3751:4898,]
# 학습하기
library(rpart)
m.rpart <- rpart(quality~.,data=wine_train)
m.rpart
# 시각화
library(rpart.plot)
rpart.plot(m.rpart,digits=3)
rpart.plot(m.rpart,digits=4,fallen.leaves=TRUE,type=3,extra=101)
# 평가하기
p.rpart <- predict(m.rpart,wine_test)
cor(p.rpart, wine_test$quality)
# Mean Absolute Error (MAE)
MAE <- function(actual,predicted) {
mean(abs(actual-predicted))
}
MAE(p.rpart,wine_test$quality)
mean(wine_train$quality)
MAE(5.87,wine_test$quality)
# 개선하기
library(RWeka)
MAE(p.rpart,wine_test$quality)
mean(wine_train$quality)
MAE(5.88,wine_test$quality)
# 개선하기
library(RWeka)
m.m5p <- M5P(quality~.,data=wine_train)
summary(m.m5p)
cor(p.m5p,wine_test$quality)
MAE(wine_test$quality,p.m5p)
p.m5p <- predict(m.m5p,wine_test)
summary(p.m5p)
cor(p.m5p,wine_test$quality)
MAE(wine_test$quality,p.m5p)
# 파일 불러오기
setwd("C:\\Users\\samsung\\Desktop\\Bitamin\\Machine Learning with R\\7장 Neural Network&SVM")
# 파일 불러오기
setwd("C:\\Users\\samsung\\Desktop\\Bitamin\\Machine Learning with R\\7장. Neural Network & SVM")
# 파일 불러오기
setwd("C:\\Users\\samsung\\Desktop\\Bitamin\\Machine Learning with R\\7장. Neural Network & SVM")
concrete <- read.csv("concrete.csv")
str(concrete)
# 정규화하기
normalize <- function(x) {return((x-min(x))/(max(x)-min(x)))}
concrete_norm <- as.data.frame(lapply(concrete,normalize))
# 정규화 O <-> 정규화 X (비교해보기)
summary(concrete_norm$strength)
summary(concrete$strength)
# 데이터 나누기
concrete_train <- concrete_norm[1:773,]
concrete_test <- concrete_norm[774:1030,]
# 훈련시키기 (by neuralnet 함수)
library(neuralnet)
concrete_model <- neuralnet(strength~cement+slag+ash+water+superplastic+coarseagg+fineagg+age, data=concrete_train)
plot(concrete_model)
# 평가하기 (by compute 함수)
model_results <- compute(concrete_model, concrete_test[1:8])   # 모델과 독립변수(8개)를 인자로 compute
predicted_strength <- model_results$net.result  #net.result 이용하기
cor(predicted_strength, concrete_test$strength)   # 예측 & 실제의 상관관계 분석
# 개선하기 ( node를 5개로 -> hidden = n )
concrete_model2 <- neuralnet(strength~cement+slag+ash+water+superplastic+coarseagg+fineagg+age, data=concrete_train, hidden=5)
# SVM
letters <- read.csv("lettersdata.csv")
str(letters)
letters_train <- letters[1:13000,]
letters_test <- letters[13001:20000,]
library(kernlab)
letter_classifier <- ksvm(letter~., data=letters_train, kernel="vanilladot")
letter_classifier
letter_predictions <- predict(letter_classifier, letters_test)
head(letter_predictions)
table(letter_predictions, letters_test$letter)
agreement <- letter_predictions == letters_test$letter
table(agreement)
prop.table(table(agreement))
# SVM 개선시키기 ( 다른 종류의 kernel function )
letter_classifier_rbf <- ksvm(letter~.,data=letters_train, kernel="rbfdot")
setwd("C:\\Users\\samsung\\Desktop\\Bitamin\\Machine Learning with R\\8장. Association Rules (장바구니 분석)")
# 파일 불러오고 요약하기
library(arules)
groceries <- read.transactions("groceries.csv",sep=",")
summary(groceries)
# 자세히 들여다보기
inspect(groceries[1:5])
# 빈도수 확인 (알파벳 순)
itemFrequency(groceries[,1:3])
# 빈도수 그래프
itemFrequencyPlot(groceries,support=0.1)
# transaction data 시각화
image(groceries[1:5])
image(sample(groceries,100))
# transaction data 시각화
image(groceries[1:5])
groceryrules <- apriori(groceries, parameter=list(support=0.006, confidence=0.25, minlen=2))
groceryrules
# 평가하기
summary(groceryrules)
# 자세히 들여다보기
inspect(groceryrules[1:3])
# 모델 개선하기 1 (sort 통해 -> lift 높은 순대로 5개)
inspect(sort(groceryrules,by="lift")[1:5])
# 모델 개선하기 2 (subset 통해 -> 원하는 품목 고르기)
berryrules <- subset(groceryrules, items %in% "berries")
inspect(berryrules)
# 파일 저장하기
write(groceryrules,file='groceryrules.csv',sep=",",quote=TRUE, row.names=FALSE) #csv형태로
groceryrules_df <- as(groceryrules,"data.frame")#df형태로 만들기
str(groceryrules_df)
setwd("C:\\Users\\samsung\\Desktop\\Bitamin\\Machine Learning with R\\9장. Clustering with k-means")
# 데이터 불러오기
teens <- read.csv("snsdata.csv")
# 데이터 구조 확인
str(teens)
# 성비 확인
table(teens$gender)
table(teens$gender, useNA="ifany")
# summary
summary(teens$age)
# 나이 선별
teens$age <- ifelse(teens$age>=13 & teens$age<20, teens$age, NA)
summary(teens$age)
# 나이.성별 dummy
teens$female <- ifelse(teens$gender=="F"&!is.na(teens$gender),1,0)
teens$no_gender <- ifelse(is.na(teens$gender),1,0)
table(teens$gender,useNA="ifany")
table(teens$female,useNA="ifany") # 0 : 그 외 / 1 : 나이 되는 여
table(teens$no_gender,useNA="ifany")  # 0 : 성별 남or여 / 1 : 성별 모름
# NA값 대체하기 (Imputation)
mean(teens$age, na.rm=TRUE)
aggregate(data=teens, age~gradyear,mean,na.rm=TRUE)
ave_age <- ave(teens$age, teens$gradyear, FUN=function(x) mean(x,na.rm=TRUE))
teens$age <- ifelse(is.na(teens$age),ave_age,teens$age)
summary(teens$age)
table(teens$gender,useNA="ifany")
table(teens$female,useNA="ifany") # 0 : 그 외 / 1 : 나이 되는 여
table(teens$no_gender,useNA="ifany")  # 0 : 성별 남or여 / 1 : 성별 모름
# NA값 대체하기 (Imputation)
mean(teens$age, na.rm=TRUE)
aggregate(data=teens, age~gradyear,mean,na.rm=TRUE)
ave_age <- ave(teens$age, teens$gradyear, FUN=function(x) mean(x,na.rm=TRUE))
teens$age <- ifelse(is.na(teens$age),ave_age,teens$age)
summary(teens$age)
# 모델 훈련하기
interests <- teens[5:40]
interests_z <- as.data.frame(lapply(interests,scale))
set.seed(2345)
teen_clusters <- kmeans(interests_z,5)
# 모델 평가하기
teen_clusters$size
teen_clusters$centers
# 모델 개선하기
teens$cluster <- teen_clusters$cluster
teens[1:5,c("cluster","gender","age","friends")]
aggregate(data=teens,age~cluster,mean)
setwd("C:\\Users\\samsung\\Desktop\\Bitamin\\Machine Learning with R\\9장. Clustering with k-means")
# 데이터 불러오기
teens <- read.csv("snsdata.csv")
# 데이터 구조 확인
str(teens)
# 성비 확인
table(teens$gender)
table(teens$gender, useNA="ifany")
# summary
summary(teens$age)
# 나이 선별
# ( 나이 조건 충족하지 못하면 NA값으로 )
teens$age <- ifelse(teens$age>=13 & teens$age<20, teens$age, NA)
# 나이.성별 dummy
# teens$female : [1 : 나이O + 여자] [0 : 그 外]
teens$female <- ifelse(teens$gender=="F"&!is.na(teens$gender),1,0)
# teens$no_gender : [1: 성별 X] [0: 성별 O]
teens$no_gender <- ifelse(is.na(teens$gender),1,0)
table(teens$gender,useNA="ifany")
table(teens$female,useNA="ifany") # 0 : 그 외 / 1 : 나이 되는 여
table(teens$no_gender,useNA="ifany")  # 0 : 성별 남or여 / 1 : 성별 모름
# NA값 대체하기 (Imputation)
mean(teens$age, na.rm=TRUE)
# gradyear(졸엽년도)에 따른 나이 평균
aggregate(data=teens, age~gradyear,mean,na.rm=TRUE)
ave_age <- ave(teens$age, teens$gradyear, FUN=function(x) mean(x,na.rm=TRUE))
teens$age <- ifelse(is.na(teens$age),ave_age,teens$age)
summary(teens$age)
# 모델 훈련하기
interests <- teens[5:40]
interests_z <- as.data.frame(lapply(interests,scale))
set.seed(2345)
teen_clusters <- kmeans(interests_z,5)
# 모델 평가하기
teen_clusters$size
teen_clusters$centers
# 모델 개선하기
teens$cluster <- teen_clusters$cluster
teens[1:5,c("cluster","gender","age","friends")]
aggregate(data=teens,age~cluster,mean)
aggregate(data=teens,female~cluster,mean)
aggregate(data=teens,friends~cluster,mean)
setwd("C:\\Users\\samsung\\Desktop\\Bitamin\\Machine Learning with R\\9장. Clustering with k-means")
# 데이터 불러오기
teens <- read.csv("snsdata.csv")
# 데이터 구조 확인
str(teens)
# 성비 확인
table(teens$gender)
table(teens$gender, useNA="ifany")
# summary
summary(teens$age)
# 나이 선별
# ( 나이 조건 충족하지 못하면 NA값으로 )
teens$age <- ifelse(teens$age>=13 & teens$age<20, teens$age, NA)
head(teens)
