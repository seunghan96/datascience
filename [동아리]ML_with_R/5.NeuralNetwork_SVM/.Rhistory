data<-factor('a','b','c')
data
data<-factor('a',c('a','b'))
data
x<-list(name='a',foo='b')
x
x <- array( 1:12, dim(2,2,3) )
x<-array(1:12,dim=c(2,3,4)))
x<-array(1:12,dim=c(2,3,4))
,,1
, , 1
1
, , 1
,,1
(x<-array(1:12,dim=c(2,3,4)))
x[,,3]
q()
nreps <- 1--
da
fweq
nreps <- 100
l1 <- numeric(nreps)
u1 <- numeric(nreps)
n <- 100
mu <0 500
mu <- 500
sigma <- 100
for *i in 1 nreps) {}
for (i in 1 nreps) {}
for (i in 1 nreps) {
X1 <- 3
X2 = 6
8 -> X3
assign('X4',24)
Y1 <- (X1+X2)*X3-X4
Y1 > 50
Y2 <- ((X4-X2)/X1)**(X3/4)
Y2 > 50
Y1==Y2
remove('Y1','Y2')
rm(list=ls())
############
mode(pi)
Y3 <- as.character(pi)
is.character(Y3)
as.numeric(TRUE)
as.numeric(FALSE)
as.logical(0)
as.logical(1)
as.logical(-1)
as.logical(5)
as.logical(-5)
paste(month.name,month.abb,sep=' -> ')
paste('September',1:30,sep=' ')
paste('Lower case : ',letters,' / Upper case : ',LETTERS,sep=' ')
save.image("C:\\Users\\samsung\\Desktop\\daf.pdf")
X1 <- 3
X2 = 6
8 -> X3
assign('X4',24)
Y1 <- (X1+X2)*X3-X4
Y1 > 50
Y2 <- ((X4-X2)/X1)**(X3/4)
Y2 > 50
Y1==Y2
remove('Y1','Y2')
rm(list=ls())
############
mode(pi)
Y3 <- as.character(pi)
is.character(Y3)
as.numeric(TRUE)
as.numeric(FALSE)
as.logical(0)
as.logical(1)
as.logical(-1)
as.logical(5)
as.logical(-5)
paste(month.name,month.abb,sep=' -> ')
paste('September',1:30,sep=' ')
paste('Lower case : ',letters,' / Upper case : ',LETTERS,sep=' ')
X1 <- 3
X2 = 6
8 -> X3
assign('X4',24)
Y1 <- (X1+X2)*X3-X4
Y1 > 50
Y2 <- ((X4-X2)/X1)**(X3/4)
Y2 > 50
Y1==Y2
remove('Y1','Y2')
rm(list=ls())
############
mode(pi)
Y3 <- as.character(pi)
is.character(Y3)
as.numeric(TRUE)
as.numeric(FALSE)
as.logical(0)
as.logical(1)
as.logical(-1)
as.logical(5)
as.logical(-5)
paste(month.name,month.abb,sep=' -> ')
paste('September',1:30,sep=' ')
paste('Lower case : ',letters,' / Upper case : ',LETTERS,sep=' ')
q()
setwd("C:\\Users\\samsung\\Desktop\\Bitamin\\Machine Learning with R\\7장")
yacht <- read.csv("yacht.csv")
str(yacht)
yacht_norm <- as.data.frame(lapply(yacht,normalize))
normalize <- function(x) {return((x-min(x))/(max(x)-min(x)))}
yacht_norm <- as.data.frame(lapply(yacht,normalize))
summary(yacht$Residuary_Resist)
summary(yacht_norm$Residuary_Resist)
set.seed(123)
a <- sample(nrow(yacht),nrow(yacht)*0.75)
yacht_norm <- as.data.frame(lapply(yacht,normalize))
summary(yacht$Residuary_Resist)
summary(yacht_norm$Residuary_Resist)
set.seed(123)
a <- sample(nrow(yacht),nrow(yacht)*0.75)
yacht_train <- yacht_norm[a,]
yacht_test <- yacht_norm[-a,]
library(neuralnet)
yacht_model <- neuralnet(Residuary_Resist~., data=yacht_train)
str(yacht)
yacht_model <- neuralnet(Residuary_Resist~LongPos_COB+Prismatic_Coeff+Len_Disp_Ratio+Beam_Draut_Ratio+Length_Beam_Ratio+Froude_Naum, data=yacht_train)
plot(yacht_model)
model_results <- compute(yacht_model, yacht_test[1:6])   # 모델과 독립변수(8개)를 인자로 compute
predicted_RR <- model_results$net.result
cor(predicted_RR, yacht_test$Residuary_Resist)
yacht_model2 <- neuralnet(Residuary_Resist~LongPos_COB+Prismatic_Coeff+Len_Disp_Ratio+Beam_Draut_Ratio+Length_Beam_Ratio+Froude_Naum, data=yacht_train,hidden=5)
plot(yacht_model2)
model_results2 <- compute(yacht_model2, yacht_test[1:6])   # 모델과 독립변수(8개)를 인자로 compute
predicted_RR2 <- model_results2$net.result
cor(predicted_RR2, yacht_test$Residuary_Resist)
voice <- read.csv("voice.csv")
is.na(voice)
na.omit(voice)
voice <- na.omit(voice)
set.seed(123)
b<- sample(nrow(voice),nrow(voice)*0.75)
voice_train <- voice[b,]
voice_test <- voice[-b,]
library(kernlab)
str(voice)
voice_classifier <- ksvm(label ~., data=voice_train, kernel="vanilladot")
voice_classifier
voice_predictions <- predict(voice_classifier, voice_test)
head(voice_predictions)
table(voice_predictions, voice_test$label)
agreement <- voice_predictions == voice_test$label
table(agreement)
prop.table(table(agreement))
voice_classifier2 <- ksvm(label ~., data=voice_train, kernel="rbfdot")
voice_classifier2
voice_predictions2 <- predict(voice_classifier2, voice_test)
agreement2 <- voice_predictions == voice_test$label
prop.table(table(agreement2))
# 파일 불러오기
setwd("C:\\Users\\samsung\\Desktop\\Bitamin\\Machine Learning with R\\7장")
concrete <- read.csv("concrete.csv")
str(concrete)
# 정규화하기
normalize <- function(x) {return((x-min(x))/(max(x)-min(x)))}
concrete_norm <- as.data.frame(lapply(concrete,normalize))
# 정규화 O <-> 정규화 X (비교해보기)
summary(concrete_norm$strength)
summary(concrete$strength)
# 데이터 나누기
concrete_train <- concrete_norm[1:773,]
concrete_test <- concrete_norm[774:1030,]
# 훈련시키기
library(neuralnet)
concrete_model <- neuralnet(strength~cement+slag+ash+water+superplastic+coarseagg+fineagg+age, data=concrete_train)
plot(concrete_model)
# 평가하기 (by compute 함수)
model_results <- compute(concrete_model, concrete_test[1:8])   # 모델과 독립변수(8개)를 인자로 compute
predicted_strength <- model_results$net.result  #net.result 이용하기
cor(predicted_strength, concrete_test$strength)   # 예측 & 실제의 상관관계 분석
# 개선하기 ( node를 5개로 -> hidden=X )
concrete_model2 <- neuralnet(strength~cement+slag+ash+water+superplastic+coarseagg+fineagg+age, data=concrete_train, hidden=5)
plot(concrete_model2)
model_results2 <-compute(concrete_model2, concrete_test[1:8])
predicted_strength2 <- model_results2$net.result
cor(predicted_strength2,concrete_test$strength)
# SVM
letters <- read.csv("lettersdata.csv")
str(letters)
letters_train <- letters[1:13000,]
letters_test <- letters[13001:20000,]
library(kernlab)
letter_classifier <- ksvm(letter~., data=letters_train, kernel="vanilladot")
letter_classifier
letter_predictions <- predict(letter_classifier, letters_test)
head(letter_predictions)
agreement <- letter_predictions == letters_test$letter
table(agreement)
prop.table(table(agreement))
# SVM 개선시키기 ( 다른 종류의 kernel function )
letter_classifier_rbf <- ksvm(letter~.,data=letters_train, kernel="rbfdot")
