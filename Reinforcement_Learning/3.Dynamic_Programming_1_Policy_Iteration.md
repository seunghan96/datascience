# 3. Dynamic Programming 1
## 1) Dynamic Programming & MDP
Dynamic Programming(동적 계획법)은, 쉽게 말해서 풀고하자 하는 문제를 작은 여러 개의 하위 문제로 나눠서 푸는 것을 의미한다.
( https://github.com/seunghan96/datascience/tree/master/Data_Structure/2.Algorithm 참고 ) </br>
Dynamic Programming을 하기 위한 조건에는 다음과 같이 두 가지가 있다
- 1 ) 하나의 문제를 "여러 개의 작은 문제"로 나눌 수 있어야 한다.
- 2 ) 하나의 서브 문제를 풀고 난 뒤, 여기서 나온 솔루션을 "저장"할 수 있어야 한다.

는 그리고 앞에서 배운 MDP(Markov Decision Process)는 이 두 조건을 만족한다. MDP의 value function을 들여다 보면, v(s)를 구하기 위해 그 안에 있는 v(s+1)을 구하는 recursive 형태를 보이는 것을 알 수 있다. </br> </br>
<a href="https://www.codecogs.com/eqnedit.php?latex=v(s)&space;=&space;E[R_{t&plus;1}&plus;\gamma&space;v(S_{t&plus;1})|S_t=s]" target="_blank"><img src="https://latex.codecogs.com/gif.latex?v(s)&space;=&space;E[R_{t&plus;1}&plus;\gamma&space;v(S_{t&plus;1})|S_t=s]" title="v(s) = E[R_{t+1}+\gamma v(S_{t+1})|S_t=s]" /></a> </br> </br>
이렇게 Dynamic Programming을 이용하여 MDP문제를 풀 수 있는데, 여기에는 크게 2가지 방법이 있다. ( 1. Policy Iteration & 2. Value Iteration )
</br> </br>
## 2) Policy Iteration
Policy Iteration은 다음과 같은 방식으로 진행이 이루어진다. <br>
1) Initialize <a href="https://www.codecogs.com/eqnedit.php?latex=\pi" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\pi" title="\pi" /></a> randomly 
2) Repeat until converge
 - let <a href="https://www.codecogs.com/eqnedit.php?latex=V&space;=&space;V_\pi" target="_blank"><img src="https://latex.codecogs.com/gif.latex?V&space;=&space;V_\pi" title="V = V_\pi" /></a>
 - for each state s, let <a href="https://www.codecogs.com/eqnedit.php?latex=\pi(s)&space;=&space;\underset{a\in&space;A}{argmax}&space;\gamma&space;\sum_{s'\in&space;S}^{&space;}P_{sa}(s')V^*(s')" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\pi(s)&space;=&space;\underset{a\in&space;A}{argmax}&space;\gamma&space;\sum_{s'\in&space;S}^{&space;}P_{sa}(s')V^*(s')" title="\pi(s) = \underset{a\in A}{argmax} \gamma \sum_{s'\in S}^{ }P_{sa}(s')V^*(s')" /></a> </br>
 즉, 처음에 policy를 랜덤하게 초기값을 주고, return을 최대로 만드는 action을 선택하는 방향으로 계속 policy를 update해나가는 과정이다.
 여기서 Policy Iteration은 2가지로 진행이 된다 ( Policy Evaluation & Policy Improvement )
 
 ### a. Policy Evaluation
 policy evaluation, 말 그대로 '정책 평가'이다. 한 마디로, 현재 가지고 있는 policy에 따라 행동을 했을 경우, 각각의 state에서 agent가 얻게 되는 예상 return값들을 value로 계산하는 과정이다. 이를 통해, 최종적으로 이루고자 하는 것은 "각 state에서의 true value function 찾기"이다. 어떻게 그것을 이룰 수 있는지, 다음의 예시를 통해 알아보자.
 [ Example ]  ( 출처 : https://t1.daumcdn.net/cfile/tistory ) </br> </br>
 <img src="https://t1.daumcdn.net/cfile/tistory/99E206335A4770B42E" width="800" /> </br> </br>
 위와 같이 4x4의 grid의 state가 있다고 가정하자. 현재 Agent는 왼쪽 상단(0,0)이나 오른쪽 하단(3,3)에 도착하면 게임을 종료한다. 한번 action을 취할 때마다 받게 되는 reward는 '-1'로, 쉽게 말해서 최소한의 움직임으로 현재 있는 위치에서 (0,0)이나 (3,3)에 도착을 해야 하는 것이다. 그리고 policy의 초기값으로, uniform random policy를 주어서 상하좌우로 움직일 확률을 각각 동일하게 1/4로 지정해준다. ( discount factor는 없다고 하자 ) </br> </br>
이러한 environment에서, 만약 현재 Agent가 (1,2)에 있다고 했을 때, 이 Agent가 현재의 policy에 따라 행동을 했을 경우, 얻게 되는 value function V1(s)은 (위 수식 참고) -1이다. 이외 같은 방식으로 한번 iteration을 다 돈다. ( 즉, (0,0)~(3,3)까지의 이 과정을 계산한다 ) 단, iteration 중에, 하나의 state에서 다음 state로 넘어가면서 이전에 계산한 값은 지금 바로 update하지 않고, 그 값을 저장(기억)해 놓았다가 한 번의 iteration이 다 끝나고 나면 그때서야 전체적으로 update를 시행한다. 그렇게 되면, 최종 도착지인 (0,0)과 (3,3)을 제외한 나머지 14개의 state에서 value 값은 모두 -1이 되고, 이렇게 한번 iteration이 다 끝나면 이 값들을 -1(기존에는 0)로 update해준다. </br> </br>
<img src="https://t1.daumcdn.net/cfile/tistory/99CF5B495C345D4C39" width="800" />  </br> </br>
두 번째 iteration을 시행한다. (위 그림 참고) (1,2)위치에서는 V2(s)가 -1.75, (1,3)위치에서 V2(s)는 -2가 된다. ( 참고로, V 밑에 붙은 첨자는 현재 몇 번째 iteration을 진행하고 있는지를 의미한다 ) 이 iteration을 앞서서와 같은 방식으로 계속 반복하다 보면, 각 state에서의 value가 아래와 같이 update된다. </br> </br>
<img src="https://t1.daumcdn.net/cfile/tistory/99E632465C345D7303" width="800" />  </br> </br>

### b. Policy Improvement
policy improvement, 말 그대로 '정책 개선'이다. 앞선 예시에서, policy는 모든 state에서 상하좌우로 움직일 확률이 1/4이었다. 하지만 이는 당연히 최적의 policy일 리가 없고, 이를 optimal로 만들어 주기 위한 것이 바로 policy improvement이다. 그럼 어떻게 움직이는 것이 optimal일까?
우리는 앞선 단계 a.Policy Evaluation에서 각 state에서의 value function을 모두 구했다. 이것을 활용하여, 우리는 어떻게 다음 행동을 취해야 이것이 optimal하다고 할 수 있을지를 판단할 수 있다. 그 판단 기준은, 현재 agent가 속한 state에서 갈 수 있는 state들의 value이고, 이들 중 가장 value가 높은 곳으로 이동하는 것이바로 최적이 policy이다. 이 기준을 다음과 같은 식으로 나타낼 수 있다 (Q-value function,또는 action-value function) </br> </br>
<a href="https://www.codecogs.com/eqnedit.php?latex=q_\pi(s,a)&space;=&space;R_s^a&space;&plus;&space;\gamma&space;\sum_{s&space;\in&space;S}^{&space;}&space;P_{ss'}^aV_\pi&space;(s')" target="_blank"><img src="https://latex.codecogs.com/gif.latex?q_\pi(s,a)&space;=&space;R_s^a&space;&plus;&space;\gamma&space;\sum_{s&space;\in&space;S}^{&space;}&space;P_{ss'}^aV_\pi&space;(s')" title="q_\pi(s,a) = R_s^a + \gamma \sum_{s \in S}^{ } P_{ss'}^aV_\pi (s')" /></a> </br> </br>
방금 얘기한 것을 앞선 예제를 통해 이해해보자. </br> </br>
<img src="https://t1.daumcdn.net/cfile/tistory/995C024E5A47690334" width="500" />  </br> </br>
state 1 ( (0,1) 위치 )에 있을 때, 상/하/좌/우로 움직였을 때의 return은 -15,-19,-1,-21이다. 그러면 우리는 이 상황(state)에서 Agent가 '왼쪽'으로 움직이는 것이 최적이다는 사실을 알 수 있다. 이 방식을 마찬가지로 state 5에도 적용했을 때, 이 곳에서도 마찬가지로 return이 가장 큰 '왼쪽'으로 움직여야 한다는 것을 알 수 있다. </br> </br>

이처럼, Policy Iteration은 (1) Policy Evaluation(각 state에서의 true value function 찾기)와 (2) Policy Improvement (각 state에서의 optimal policy 찾기)로 이루어진다. 지금까지 설명한 내용은 다음과 같이 아래 사진으로 요약할 수 있다. </br> </br>
<img src="https://miro.medium.com/max/2624/1*udhphWhqjadT-osAQhL6AQ.png" width="500" />  </br> 
( 출처 : https://miro.medium.com/max/2624/1*udhphWhqjadT-osAQhL6AQ.png )
