# 6. Time Difference Learning

## 1. Introduction
Monte Carlo Method에서는, update가 매 episode마다 일어났다. 하지만 episode가 길면 길수록 update가 늦게 이루어지게 된다. 이를 보대 online으로 하기
위해서 나온 것이 Time Difference Learning인데, 이 방법에서는 value function에 대한 update가 매 episode가 아닌 '매 time step'마다 이루어진다.
</br>
Monte Carlo방법에서는 아래와 같은 updating equation으로 value function이 update되었다. 여기서에서 Gt는 episode의 종료시까지 얻은 모든 reward들을
현재가치로 discount한 것을 나타낸다. </br> </br>
<img src="https://t1.daumcdn.net/cfile/tistory/990D8B3A5A6347C803" width="300" /> </br>
( 출처 : https://t1.daumcdn.net/cfile/tistory/990D8B3A5A6347C803 )
</br></br>
하지만, Time Difference Learning 방법에서는 Monte Carlo처럼 epsiode 종료시까지 모든 reward들을 대상으로 구하지 않고, 아래와 같이 하나의 time step에
대한 것으로 reward를 설정하였다. 따라서 TD (Time Difference)의 updating 식은 아래와 같다.</br></br>
<img src="https://t1.daumcdn.net/cfile/tistory/996527445A6484CA24" width="300" /> </br>
( 출처 : https://t1.daumcdn.net/cfile/tistory/996527445A6484CA24 )
</br></br>
TD의 전체 process를 요약하면 다음과 같다. 매 step마다 업데이트가 이루어짐을 확인할 수 있다.
<img src="https://t1.daumcdn.net/cfile/tistory/99F78B405A63763B03" width="550" /> </br></br>
( 출처 : https://t1.daumcdn.net/cfile/tistory/99F78B405A63763B03 )
</br></br>

## 2. MC vs TD
결론부터 요약하자면, Monte Carlo Method의 target은 unbiased되어있지만 high variance라는 단점이 있고, Time Difference Learning의 경우에는 biased 되어 있지만
low variance라는 장점이 있다. </br>
- <a href="https://www.codecogs.com/eqnedit.php?latex=G_{t}&space;=&space;R_{t&plus;1}&space;&plus;&space;\gamma&space;R_{t&plus;2}&space;&plus;&space;\gamma&space;^2R_{t&plus;3}.." target="_blank"><img src="https://latex.codecogs.com/gif.latex?G_{t}&space;=&space;R_{t&plus;1}&space;&plus;&space;\gamma&space;R_{t&plus;2}&space;&plus;&space;\gamma&space;^2R_{t&plus;3}.." title="G_{t} = R_{t+1} + \gamma R_{t+2} + \gamma ^2R_{t+3}.." /></a>
는 unbiased estimate of <a href="https://www.codecogs.com/eqnedit.php?latex=v_\pi&space;(S_t)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?v_\pi&space;(S_t)" title="v_\pi (S_t)" /></a>
- TD의 true target인 <a href="https://www.codecogs.com/eqnedit.php?latex=R_{t&plus;1}&space;&plus;&space;\gamma&space;v_\pi(S_{t&plus;1})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?R_{t&plus;1}&space;&plus;&space;\gamma&space;v_\pi(S_{t&plus;1})" title="R_{t+1} + \gamma v_\pi(S_{t+1})" /></a>도 
unbiased estimate of <a href="https://www.codecogs.com/eqnedit.php?latex=v_\pi&space;(S_t)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?v_\pi&space;(S_t)" title="v_\pi (S_t)" /></a>
- 하지만, 하나의 time step만을 고려한 TD(0)의 target인 <a href="https://www.codecogs.com/eqnedit.php?latex=R_{t&plus;1}&space;&plus;&space;\gamma&space;V(S_{t&plus;1})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?R_{t&plus;1}&space;&plus;&space;\gamma&space;V(S_{t&plus;1})" title="R_{t+1} + \gamma V(S_{t+1})" /></a>는
biased estimate of <a href="https://www.codecogs.com/eqnedit.php?latex=v_\pi&space;(S_t)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?v_\pi&space;(S_t)" title="v_\pi (S_t)" /></a>
</br>
하지만 고려하는 time step의 수가 적다 보니, 당연히 variance는 줄어든다는 장점이 있다. ( Return depends on "many" random action,transitions,reward, but TD target epends on
"ONE" random action, transition, reward )
</br></br>

## 3. N-step TD
지금까지 위에서 이야기한 TD는, 엄연히 말하면 TD(0), 즉 "하나의 time step"만을 고려한 TD인 것이다. 이 time step를 무한으로 한다면, 이는 결국 MC와 동일해질 것이다.
앞서 보았듯 MC와 TD에는 각각의 장.단점이 있다. 그래서 이 둘을 적절히 섞은 TD(n)방법이 있다. time step을 n으로 잡는 것이다. 아래 식을 보면 쉽게 이해할 수 있다. </br></br>
<img src="https://programmingbeenet.files.wordpress.com/2019/07/n-step-td_math.png" width="500" /> </br>
( 출처 : https://programmingbeenet.files.wordpress.com/2019/07/n-step-td_math.png ) </br></br>

다양한 n값과 alpha값(=learning rate)을 Random Walk를 통해 시험해 보았고 time step의 수(n) 마다 최적의 alpha값이 다름을 확인할 수 있었다.
<img src="https://img1.daumcdn.net/thumb/R720x0.q80/?scode=mtistory2&fname=http%3A%2F%2Fcfile4.uf.tistory.com%2Fimage%2F99B32E335A1E3B7815C934" width="500" /> </br>
( 출처 : https://img1.daumcdn.net/thumb/R720x0.q80/?scode=mtistory2&fname=http%3A%2F%2Fcfile4.uf.tistory.com%2Fimage%2F99B32E335A1E3B7815C934 )
한마디로, 절대적으로 좋은 time step을 찾기는 어렵다. 그래서 여러 time step를 고려하여 Time Difference Learing을 한 것이 바로 <a href="https://www.codecogs.com/eqnedit.php?latex=TD(\lambda)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?TD(\lambda)" title="TD(\lambda)" /></a>이다.
