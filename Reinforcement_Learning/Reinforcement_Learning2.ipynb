{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tic Tac Toe 환경 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment():\n",
    "    \n",
    "    def __init__(self):\n",
    "    # 보드는 0으로 초기화된 9개의 배열로 준비\n",
    "    # 게임종료 : done = True\n",
    "        self.board_a = np.zeros(9)\n",
    "        self.done = False\n",
    "        self.reward = 0\n",
    "        self.winner = 0\n",
    "        self.print = False\n",
    "\n",
    "    def move(self, p1, p2, player):\n",
    "    # 각 플레이어가 선택한 행동을 표시 하고 게임 상태(진행 또는 종료)를 판단\n",
    "    # p1 = 1, p2 = -1로 정의\n",
    "    # 각 플레이어는 행동을 선택하는 select_action 메서드를 가짐\n",
    "        if player == 1:\n",
    "            pos = p1.select_action(env, player)\n",
    "        else:\n",
    "            pos = p2.select_action(env, player)\n",
    "        \n",
    "        # 보드에 플레이어의 선택을 표시\n",
    "        self.board_a[pos] = player\n",
    "        if self.print:\n",
    "            print(player)\n",
    "            self.print_board()\n",
    "        # 게임이 종료상태인지 아닌지를 판단\n",
    "        self.end_check(player)\n",
    "        \n",
    "        return  self.reward, self.done\n",
    " \n",
    "    # 현재 보드 상태에서 가능한 행동(둘 수 있는 장소)을 탐색하고 리스트로 반환\n",
    "    def get_action(self):\n",
    "        observation = []\n",
    "        for i in range(9):\n",
    "            if self.board_a[i] == 0:\n",
    "                observation.append(i)\n",
    "        return observation\n",
    "    \n",
    "    # 게임이 종료(승패 또는 비김)됐는지 판단\n",
    "    def end_check(self,player):\n",
    "        # 0 1 2\n",
    "        # 3 4 5\n",
    "        # 6 7 8\n",
    "        # 승패 조건은 가로, 세로, 대각선 이 -1 이나 1 로 동일할 때 \n",
    "        end_condition = ((0,1,2),(3,4,5),(6,7,8),(0,3,6),(1,4,7),(2,5,8),(0,4,8),(2,4,6))\n",
    "        for line in end_condition:\n",
    "            if self.board_a[line[0]] == self.board_a[line[1]] \\\n",
    "                and self.board_a[line[1]] == self.board_a[line[2]] \\\n",
    "                and self.board_a[line[0]] != 0:\n",
    "                # 종료됐다면 누가 이겼는지 표시\n",
    "                self.done = True\n",
    "                self.reward = player\n",
    "                return\n",
    "        # 비긴 상태는 더는 보드에 빈 공간이 없을때\n",
    "        observation = self.get_action()\n",
    "        if (len(observation)) == 0:\n",
    "            self.done = True\n",
    "            self.reward = 0            \n",
    "        return\n",
    "        \n",
    "    # 현재 보드의 상태를 표시 p1 = O, p2 = X    \n",
    "    def print_board(self):\n",
    "        print(\"+----+----+----+\")\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                if self.board_a[3*i+j] == 1:\n",
    "                    print(\"|  O\",end=\" \")\n",
    "                elif self.board_a[3*i+j] == -1:\n",
    "                    print(\"|  X\",end=\" \")\n",
    "                else:\n",
    "                    print(\"|   \",end=\" \")\n",
    "            print(\"|\")\n",
    "            print(\"+----+----+----+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Human_player():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.name = \"Human player\"\n",
    "        \n",
    "    def select_action(self, env, player):\n",
    "        while True:\n",
    "            # 가능한 행동을 조사한 후 표시\n",
    "            available_action = env.get_action()\n",
    "            print(\"possible actions = {}\".format(available_action))\n",
    "\n",
    "            # 상태 번호 표시\n",
    "            print(\"+----+----+----+\")\n",
    "            print(\"+  0 +  1 +  2 +\")\n",
    "            print(\"+----+----+----+\")\n",
    "            print(\"+  3 +  4 +  5 +\")\n",
    "            print(\"+----+----+----+\")\n",
    "            print(\"+  6 +  7 +  8 +\")\n",
    "            print(\"+----+----+----+\")\n",
    "                        \n",
    "            # 키보드로 가능한 행동을 입력 받음\n",
    "            action = input(\"Select action(human) : \")\n",
    "            action = int(action)\n",
    "            \n",
    "            # 입력받은 행동이 가능한 행동이면 반복문을 탈출\n",
    "            if action in available_action:\n",
    "                return action\n",
    "            # 아니면 행동 입력을 반복\n",
    "            else:\n",
    "                print(\"You selected wrong action\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 랜덤 플레이어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Random_player():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.name = \"Random player\"\n",
    "        self.print = False\n",
    "        \n",
    "    def select_action(self, env, player):\n",
    "        # 가능한 행동 조사\n",
    "        available_action = env.get_action()\n",
    "        # 가능한 행동 중 하나를 무작위로 선택\n",
    "        action = np.random.randint(len(available_action))\n",
    "#         print(\"Select action(random) = {}\".format(available_action[action]))\n",
    "        return available_action[action]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 몬테카를로 플레이어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Monte_Carlo_player():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.name = \"MC player\"\n",
    "        self.num_playout = 1000\n",
    "        \n",
    "    def select_action(self, env, player):\n",
    "        # 가능한 행동 조사\n",
    "        available_action = env.get_action()\n",
    "        V = np.zeros(len(available_action))\n",
    "        \n",
    "        for i in range(len(available_action)):\n",
    "            # 플레이아웃을 100번 반복\n",
    "            for j in range(self.num_playout):\n",
    "                # 지금 상태를 복사해서 플레이 아웃에 사용\n",
    "                temp_env = copy.deepcopy(env)\n",
    "                # 플레이아웃의 결과는 승리 플레이어의 값으로 반환\n",
    "                # p1 이 이기면 1, p2 가 이기면 -1\n",
    "                self.playout(temp_env, available_action[i], player)\n",
    "                if player == temp_env.reward:\n",
    "                    V[i] += 1\n",
    "   \n",
    "        return available_action[np.argmax(V)]    \n",
    "\n",
    "    # 플레이아웃 재귀함수\n",
    "    # 게임이 종료상태 (승 또는 패 또는 비김) 가 될때까지 행동을 임의로 선택하는 것을 반복\n",
    "    # 플레이어는 계속 바뀌기 때문에 (-)를 곱해서 -1, 1, -1 이 되게함    \n",
    "    def playout(self, temp_env, action, player):\n",
    "        \n",
    "        temp_env.board_a[action] = player\n",
    "        temp_env.end_check(player)\n",
    "        # 게임 종료 체크\n",
    "        if temp_env.done == True:\n",
    "            return \n",
    "        else:\n",
    "            # 플레이어 교체\n",
    "            player = -player\n",
    "            # 가능한 행동 조사\n",
    "            available_action = temp_env.get_action()\n",
    "            # 무작위로 행동을 선택\n",
    "            action = np.random.randint(len(available_action))\n",
    "            self.playout(temp_env, available_action[action], player)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-learning 플레이어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_learning_player():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.name = \"Q_player\"\n",
    "        # Q-table을 딕셔너리로 정의\n",
    "        self.qtable = {}\n",
    "        # e-greedy 계수 정의\n",
    "        self.epsilon = 1\n",
    "        # 학습률 정의\n",
    "        self.learning_rate = 0.1\n",
    "        self.gamma=0.9\n",
    "        self.print = False\n",
    "\n",
    "    # policy에 따라 상태에 맞는 행동을 선택\n",
    "    def select_action(self, env, player):\n",
    "        # policy에 따라 행동을 선택\n",
    "        action = self.policy(env)\n",
    "        if self.print:\n",
    "            print(\"{} : select action\".format(action))\n",
    "        return action \n",
    "        \n",
    "    def policy(self, env):\n",
    "        if self.print:\n",
    "            print(\"-----------   policy start -------------\")\n",
    "        # 행동 가능한 상태를 저장\n",
    "        available_action = env.get_action()\n",
    "        # 행동 가능한 상태의 Q-value를 저장\n",
    "        qvalues = np.zeros(len(available_action))\n",
    "\n",
    "        if self.print:\n",
    "            print(\"{} : available_action\".format(available_action))\n",
    "\n",
    "        # 행동 가능한 상태의 Q-value를 조사\n",
    "        for i, act in enumerate(available_action):\n",
    "\n",
    "            key = (tuple(env.board_a),act)\n",
    "\n",
    "            # 현재 상태를 경험한 적이 없다면(딕셔너리에 없다면) 딕셔너리에 추가(Q-value = 0)\n",
    "            if self.qtable.get(key) ==  None:                \n",
    "                self.qtable[key] = 0\n",
    "            # 행동 가능한 상태의 Q-value 저장\n",
    "            qvalues[i] = self.qtable.get(key)\n",
    "\n",
    "        if self.print:\n",
    "            for key, val in self.qtable.items():\n",
    "                print(\"key = {key}, value={value}\".format(key=key,value=val))\n",
    "            print(\"policy state_value = {}\".format(state_qvalue))\n",
    "            for key, val in self.qtable1.items():\n",
    "                print(\"key = {key}, count={value}\".format(key=key,value=val))\n",
    "\n",
    "\n",
    "        # e-greedy\n",
    "        # 가능한 행동들 중에서 Q-value가 가장 큰 행동을 저장\n",
    "        greedy_action = np.argmax(qvalues)                    \n",
    "\n",
    "        pr = np.zeros(len(available_action))\n",
    "        if self.print:\n",
    "            print(\"{} : self.epsilon = {}\".format(self.epsilon))\n",
    "            print(\"{} : greedy_action\".format(greedy_action))\n",
    "            print(\"{} : qvalues\".format(greedy_action, qvalues[greedy_action]))\n",
    "\n",
    "        # max Q-value와 같은 값이 여러개 있는지 확인한 후 double_check에 상태를 저장\n",
    "        double_check = (np.where(qvalues == np.max(qvalues),1,0))\n",
    "\n",
    "        #  여러개 있다면 중복된 상태중에서 다시 무작위로 선택    \n",
    "        if np.sum(double_check) > 1:\n",
    "            if self.print:\n",
    "                print(\"{} : double_check\".format(np.round(double_check,2)))\n",
    "            double_check = double_check/np.sum(double_check)\n",
    "            greedy_action =  np.random.choice(range(0,len(double_check)), p=double_check)\n",
    "            if self.print:\n",
    "                print(\"{} : greedy_action\".format(greedy_action))\n",
    "                print(\"{} : double_check\".format(np.round(double_check,2)))\n",
    "                print(\"{} : selected state\".format(available_state[greedy_action]))\n",
    "        # e-greedy로 행동들의 선택 확률을 계산\n",
    "        pr = np.zeros(len(available_action))\n",
    "\n",
    "        for i in range(len(available_action)):\n",
    "            if i == greedy_action:\n",
    "                pr[i] = 1 - self.epsilon + self.epsilon/len(available_action)\n",
    "                if pr[i] < 0:\n",
    "                    print(\"{} : - pr\".format(np.round(pr[i],2)))\n",
    "            else:\n",
    "                pr[i] = self.epsilon / len(available_action)\n",
    "                if pr[i] < 0:\n",
    "                    print(\"{} : - pr\".format(np.round(pr[i],2)))\n",
    "\n",
    "        action = np.random.choice(range(0,len(available_action)), p=pr)\n",
    "\n",
    "        if self.print:\n",
    "            print(\"pr = {}\".format(np.round(pr,2)))\n",
    "            print(\"action = {}\".format(action))\n",
    "            print(\"state[action] = {}\".format(state[action]))\n",
    "            print(\"-----------   policy end -------------\")\n",
    "\n",
    "        return available_action[action]    \n",
    "    \n",
    "    def learn_qtable(self,board_bakup, action_backup, env, reward):\n",
    "        # 현재 상태와 행동을 키로 저장\n",
    "        key = (board_bakup,action_backup)\n",
    "        if self.print:\n",
    "            print(\"-----------   learn_qtable start -------------\")\n",
    "            print(\"{} : board_bakup, {} : action_backup, {} : reward\".format(board_bakup, action_backup, reward))\n",
    "            print(\"{} : key\".format(key))\n",
    "        # Q-table 학습\n",
    "        if env.done == True:\n",
    "            # 게임이 끝났을 경우 학습\n",
    "            if self.print:\n",
    "                print(\"{} : before self.qtable[key]\".format(self.qtable[key]))\n",
    "\n",
    "            self.qtable[key] += self.learning_rate*(reward - self.qtable[key])\n",
    "            \n",
    "            if self.print:\n",
    "                print(\"{} : after self.qtable[key]\".format(self.qtable[key]))\n",
    "        else:\n",
    "            # 게임이 진행중일 경우 학습\n",
    "            # 다음 상태의 max Q 값 계산\n",
    "            available_action = env.get_action()        \n",
    "            qvalues = np.zeros(len(available_action))\n",
    "\n",
    "            for i, act in enumerate(available_action):\n",
    "                next_key = (tuple(env.board_a),act)\n",
    "                # 다음 상태를 경험한 적이 없다면(딕셔너리에 없다면) 딕셔너리에 추가(Q-value = 0)\n",
    "                if self.qtable.get(next_key) ==  None:                \n",
    "                    self.qtable[next_key] = 0\n",
    "                qvalues[i] = self.qtable.get(next_key)\n",
    "\n",
    "            # maxQ 조사\n",
    "            maxQ = np.max(qvalues)  \n",
    "            \n",
    "            if self.print:\n",
    "                print(\"{} : before self.qtable[key]\".format(self.qtable[key]))\n",
    "            # 게임이 진행중일 때 학습\n",
    "            self.qtable[key] += self.learning_rate*(reward + self.gamma * maxQ - self.qtable[key])\n",
    "            \n",
    "            if self.print:\n",
    "                print(\"{} : after self.qtable[key]\".format(self.qtable[key]))\n",
    "                \n",
    "        if self.print:\n",
    "            print(\"-----------   learn_qtable end -------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-learning 플레이어 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 100000/100000 [01:34<00:00, 1056.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p1 = 54938 p2 = 25843 draw = 19219\n",
      "end train\n"
     ]
    }
   ],
   "source": [
    "p1_Qplayer = Q_learning_player()\n",
    "p2_Qplayer = Q_learning_player()\n",
    "\n",
    "# 입실론은 0.5로 설정\n",
    "p1_Qplayer.epsilon = 0.5\n",
    "p2_Qplayer.epsilon = 0.5\n",
    "\n",
    "p1_score = 0\n",
    "p2_score = 0\n",
    "draw_score = 0\n",
    "\n",
    "# printer = True\n",
    "print()\n",
    "max_learn = 100000\n",
    "\n",
    "for j in tqdm(range(max_learn)):\n",
    "    np.random.seed(j)\n",
    "    env = Environment()\n",
    "    \n",
    "    for i in range(10000):\n",
    "        \n",
    "        # p1 행동 선택\n",
    "        player = 1\n",
    "        pos = p1_Qplayer.policy(env)\n",
    "        # 현재 상태 s, 행동 a를 저장\n",
    "        p1_board_backup = tuple(env.board_a)\n",
    "        p1_action_backup = pos\n",
    "#         print(\"p1_board_backup = {} p1_action_backup = {}\".format(p1_board_backup, p1_action_backup))\n",
    "        env.board_a[pos] = player\n",
    "        env.end_check(player)\n",
    "        \n",
    "        # 게임이 종료상태라면 각 플레이어의 Q-table을 학습\n",
    "        if env.done == True:\n",
    "            # 비겼으면 보수 0\n",
    "            if env.reward == 0:\n",
    "                p1_Qplayer.learn_qtable(p1_board_backup, p1_action_backup, env, 0)\n",
    "                p2_Qplayer.learn_qtable(p2_board_backup, p2_action_backup, env, 0)\n",
    "                draw_score += 1\n",
    "                break\n",
    "            # p1이 이겼으므로 보상 +1\n",
    "            # p2이 졌으므로 보상 -1\n",
    "            else:\n",
    "                p1_Qplayer.learn_qtable(p1_board_backup, p1_action_backup, env, 1)\n",
    "                p2_Qplayer.learn_qtable(p2_board_backup, p2_action_backup, env, -1)\n",
    "                p1_score += 1\n",
    "                break\n",
    "            \n",
    "        # 게임이 끌나지 않았다면 p2의 Q-talble을 학습 (게임 시작직후에는 p2 는 학습할수 없음)\n",
    "        if i != 0:\n",
    "            p2_Qplayer.learn_qtable(p2_board_backup, p2_action_backup, env, -0.01)\n",
    "#         print(\"p1_board_backup = {} p1_action_backup = {}\".format(p1_board_backup, p1_action_backup))\n",
    "        \n",
    "        # p2 행동 선택\n",
    "        player = -1\n",
    "        pos = p2_Qplayer.policy(env)\n",
    "        p2_board_backup = tuple(env.board_a)\n",
    "        p2_action_backup = pos\n",
    "        env.board_a[pos] = player\n",
    "        env.end_check(player)\n",
    "    \n",
    "#         print(\"p1_board_backup = {} p1_action_backup = {}\".format(p1_board_backup, p1_action_backup))\n",
    "        \n",
    "        if env.done == True:\n",
    "            # 비겼으면 보수 0\n",
    "            if env.reward == 0:\n",
    "                p1_Qplayer.learn_qtable(p1_board_backup, p1_action_backup, env, 0)\n",
    "                p2_Qplayer.learn_qtable(p2_board_backup, p2_action_backup, env, 0)\n",
    "                draw_score += 1\n",
    "                break\n",
    "            # p2이 이겼으므로 보상 +1\n",
    "            # p1이 졌으므로 보상 -1\n",
    "            else:\n",
    "                p1_Qplayer.learn_qtable(p1_board_backup, p1_action_backup, env, -1)\n",
    "                p2_Qplayer.learn_qtable(p2_board_backup, p2_action_backup, env, 1)\n",
    "                p2_score += 1\n",
    "                break\n",
    "    \n",
    "        # 게임이 끌나지 않았다면 p1의  Q-talble 학습\n",
    "        p1_Qplayer.learn_qtable(p1_board_backup, p1_action_backup, env, -0.01)    \n",
    "    \n",
    "#     # 1000 게임마다 게임 결과 표시\n",
    "#     if j% 1000 == 0:\n",
    "#         print(\"j = {} p1 = {} p2 = {} draw = {}\".format(j, p1_score,p2_score,draw_score))\n",
    "print(\"p1 = {} p2 = {} draw = {}\".format(p1_score,p2_score,draw_score))\n",
    "print(\"end train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN 플레이어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mingk\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from keras import metrics\n",
    "from keras.layers import Dense, Flatten, Conv2D\n",
    "from keras.models import load_model\n",
    "import time\n",
    "\n",
    "class DQN_player():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.name = \"DQN_player\"\n",
    "        self.epsilon = 1\n",
    "        self.learning_rate = 0.1\n",
    "        self.gamma=0.9\n",
    "        \n",
    "        # 두개의 신경망을 생성\n",
    "        self.main_network = self.make_network()\n",
    "        self.target_network = self.make_network()\n",
    "        # 메인 신경망의 가중치를 타깃 신경망의 가중치로 복사\n",
    "        self.copy_network()\n",
    "        \n",
    "        self.print = False\n",
    "        self.print1 = False\n",
    "        self.count = np.zeros(9)\n",
    "        self.win = np.zeros(9)\n",
    "        self.begin = 0\n",
    "        self.e_trend = []\n",
    "        \n",
    "    # 신경망 생성\n",
    "    def make_network(self):\n",
    "\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Conv2D(16, (3, 3), padding='same', activation='relu', input_shape=(3,3,2)))\n",
    "        self.model.add(Conv2D(32, (3, 3), padding='same', activation='relu'))\n",
    "        self.model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "        self.model.add(Flatten())\n",
    "        self.model.add(Dense(256, activation='tanh'))\n",
    "        self.model.add(Dense(128, activation='tanh'))\n",
    "        self.model.add(Dense(64, activation='tanh'))\n",
    "        self.model.add(Dense(9))\n",
    "        print(self.model.summary())\n",
    "             \n",
    "        self.model.compile(optimizer = SGD(lr=0.01), loss = 'mean_squared_error', metrics=['mse'])\n",
    "        \n",
    "        return self.model\n",
    "    \n",
    "    # 신경망 복사\n",
    "    def copy_network(self):\n",
    "        self.target_network.set_weights(self.main_network.get_weights())\n",
    "        \n",
    "    def save_network(self, name):\n",
    "        filename = name + '_main_network.h5'\n",
    "        self.main_network.save(filename)\n",
    "        print(\"end save model\")\n",
    "\n",
    "        \n",
    "    # 1차원 배열의 보드상태를 2차원으로 변환\n",
    "    def state_convert(self, board_a):\n",
    "        d_state = np.full((3,3,2),0.1)\n",
    "        for i in range(9):\n",
    "            if board_a[i] == 1:\n",
    "                d_state[i//3,i%3,0] = 1\n",
    "            elif board_a[i] == -1:\n",
    "                d_state[i//3,i%3,1] = 1\n",
    "            else:\n",
    "                pass\n",
    "        return d_state\n",
    "    \n",
    "    \n",
    "    def select_action(self, env, player):\n",
    "        \n",
    "        action = self.policy(env)\n",
    "\n",
    "        if self.print1:\n",
    "            print(\"{} : policy state\".format(available_state))\n",
    "            print(\"{} : qvalues\".format(np.round(qvalues,3)))\n",
    "            print(\"{} : select action\".format(action))\n",
    "            \n",
    "        return action \n",
    "        \n",
    "    def policy(self, env):\n",
    "        \n",
    "        if self.print:\n",
    "            print(\"-----------   policy start -------------\")\n",
    "        \n",
    "        # 행동 가능한 상태를 저장\n",
    "        available_state = env.get_action()\n",
    "        \n",
    "        state_2d = self.state_convert(env.board_a)\n",
    "        x = np.array([state_2d],dtype=np.float32).astype(np.float32)\n",
    "        qvalues = self.main_network.predict(x)[0,:]\n",
    "        \n",
    "        if self.print:\n",
    "            print(\"{} : policy state\".format(available_state))\n",
    "            print(\"{} : qvalues\".format(np.round(qvalues,3)))\n",
    "        \n",
    "        # 행동 가능한 상태의 Q-value를 저장\n",
    "        available_state_qvalues = qvalues[available_state]\n",
    "\n",
    "        if self.print:\n",
    "            print(\"{} : available_state_qvalues\".format(np.round(available_state_qvalues,3)))\n",
    "        \n",
    "        # max Q-value를 탐색한 후 저장\n",
    "        greedy_action = np.argmax(available_state_qvalues)\n",
    "        if self.print:\n",
    "            print(\"{} : self.epsilon\".format(self.epsilon))\n",
    "            print(\"{} : greedy_action\".format(greedy_action))\n",
    "            print(\"{} : qvalue = {}\".format(available_state_qvalues[greedy_action]))\n",
    "        \n",
    "        # max Q-value와 같은 값이 여러개 있는지 확인한 후 double_check에 상태를 저장\n",
    "        double_check = (np.where(qvalues == np.max(available_state[greedy_action]),1,0))\n",
    "        \n",
    "        #  여러개 있다면 중복된 상태중에서 다시 무작위로 선택    \n",
    "        if np.sum(double_check) > 1:\n",
    "            if self.print:\n",
    "                print(\"{} : double_check\".format(np.round(double_check,2)))\n",
    "            double_check = double_check/np.sum(double_check)\n",
    "            greedy_action =  np.random.choice(range(0,len(double_check)), p=double_check)\n",
    "            if self.print:\n",
    "                print(\"{} : greedy_action\".format(greedy_action))\n",
    "                print(\"{} : double_check\".format(np.round(double_check,2)))\n",
    "                print(\"{} : selected state\".format(available_state[greedy_action]))\n",
    "        \n",
    "        # ε-greedy\n",
    "        pr = np.zeros(len(available_state))\n",
    "\n",
    "        for i in range(len(available_state)):\n",
    "            if i == greedy_action:\n",
    "                pr[i] = 1 - self.epsilon + self.epsilon/len(available_state)\n",
    "                if pr[i] < 0:\n",
    "                    print(\"{} : - pr\".format(np.round(pr[i],2)))\n",
    "            else:\n",
    "                pr[i] = self.epsilon / len(available_state)\n",
    "                if pr[i] < 0:\n",
    "                    print(\"{} : - pr\".format(np.round(pr[i],2)))\n",
    "\n",
    "        action = np.random.choice(range(0,len(available_state)), p=pr)        \n",
    "        \n",
    "        if self.print:\n",
    "            print(\"{} : pr\".format(np.round(pr,2)))\n",
    "            print(\"{} : action\".format(action))\n",
    "            print(\"{} : state[action]\".format(available_state[action]))\n",
    "            print(\"-----------   policy end -------------\")\n",
    "\n",
    "        if len(available_state) == 9:\n",
    "            self.count[action] +=1\n",
    "            self.begin = action\n",
    "            \n",
    "        return available_state[action]        \n",
    "        \n",
    "    def learn_dqn(self,board_bakup, action_backup, env, reward):\n",
    "        \n",
    "        # 입력을 2차원으로 변환한 후, 메인 신경망으로 q-value를 계산\n",
    "        new_state = self.state_convert(board_bakup)\n",
    "        x = np.array([new_state],dtype=np.float32).astype(np.float32)\n",
    "        qvalues = self.main_network.predict(x)[0,:]\n",
    "        before_action_value = copy.deepcopy(qvalues)\n",
    "        delta = 0\n",
    "        \n",
    "        if self.print:\n",
    "            print(\"-----------   learn_qtable start -------------\")\n",
    "            print(\"{} : board_bakup\".format(board_bakup))\n",
    "            print(\"{} : action_backup\".format(action_backup))\n",
    "            print(\"{} : reward = {}\".format(reward))\n",
    "            \n",
    "        if env.done == True:\n",
    "            if reward == 1:\n",
    "                self.win[self.begin] += 1\n",
    "#                 print(\"winnn\")\n",
    "#                 print(\"{}\".format(self.win))\n",
    "            if self.print:\n",
    "                print(\"{} : delta\".format(delta))\n",
    "                print(\"{} : before update : actions[action_backup]\".format(np.round(qvalues[action_backup],3)))\n",
    "                print(\"1  : new_qvalue\")\n",
    "            \n",
    "            # 게임이 좀료됐을때 신경망의 학습을 위한 정답 데이터를 생성\n",
    "            qvalues[action_backup] = reward\n",
    "            y=np.array([qvalues],dtype=np.float32).astype(np.float32)\n",
    "            # 생성된 정답 데이터로 메인 신경망을 학습\n",
    "            self.main_network.fit(x, y, epochs=10, verbose=0)\n",
    "            \n",
    "            if self.print:\n",
    "                after_action_value = copy.deepcopy(self.main_network.predict(x)[0,:])\n",
    "                delta = after_action_value - before_action_value\n",
    "                print(\"{} : before_action_value id = {}\".format(np.round(before_action_value,3),id(before_action_value)))\n",
    "                print(\"{} : target_action_value id = {}\".format(np.round(target_action_value,3),id(target_action_value)))\n",
    "                print(\"{} : after_action_value id = {}\".format(np.round(after_action_value,3),id(after_action_value)))\n",
    "                print(\"{} : delta action value\".format(np.round(delta,3)))\n",
    "                state = ((0,0,0,0,0,0,0,0,0))\n",
    "                state_2d = self.state_convert(state)\n",
    "                x = np.array([state_2d],dtype=np.float32).astype(np.float32)\n",
    "                qvalues = self.main_network.predict(x)[0,:]\n",
    "                print(\"{} : initial state qvalues\".format(np.round(qvalues,3)))\n",
    "\n",
    "        else:\n",
    "            # 게임이 진행중일때  신경망의 학습을 위한 정답 데이터를 생성\n",
    "            # 현재 상태에서 최고 Q 값을 계산\n",
    "            new_state = self.state_convert(env.board_a)\n",
    "            next_x = np.array([new_state],dtype=np.float32).astype(np.float32)\n",
    "            next_qvalues = self.target_network.predict(next_x)[0,:]\n",
    "            available_state = env.get_action()\n",
    "            maxQ = np.max(next_qvalues[available_state])            \n",
    "            \n",
    "            if self.print:\n",
    "                print(\"{} : old_qvalue\".format(np.round(before_action_value[action_backup],3)))\n",
    "                print(\"{} : next_qvalue\".format(np.round(next_qvalues,3)))\n",
    "                print(\"{} : available_state\".format(np.round(available_state,3)))\n",
    "                print(\"{} : maxQ\".format(np.round(maxQ,3)))\n",
    "            \n",
    "            delta = self.learning_rate*(reward + self.gamma * maxQ - qvalues[action_backup])\n",
    "            \n",
    "            if self.print:\n",
    "                print(\"{} : delta\".format(np.round(delta,3)))\n",
    "                print(\"{} : before_update_qvalues\".format(np.round(qvalues,3)))\n",
    "                print(\"{} : before_update_qvalue\".format(np.round(qvalues[action_backup],3)))\n",
    "                \n",
    "            qvalues[action_backup] += delta\n",
    "            \n",
    "            if self.print:\n",
    "                print(\"{} : after_update_qvalue\".format(np.round(qvalues[action_backup],3)))            \n",
    "                print(\"{} : before_update_qvalues\".format(np.round(qvalues,3)))\n",
    "                target_action_value = copy.deepcopy(qvalues)\n",
    "                print(\"{} : new_qvalues\".format(np.round(qvalues,3)))            \n",
    "                print(\"{} : target_action_value id = {}\".format(np.round(target_action_value,3),target_action_value))            \n",
    "            # 생성된 정답 데이터로 메인 신경망을 학습\n",
    "            y=np.array([qvalues],dtype=np.float32).astype(np.float32)\n",
    "            self.main_network.fit(x, y, epochs = 10, verbose=0)\n",
    "            \n",
    "            if self.print:\n",
    "                after_action_value = copy.deepcopy(self.main_network.predict(x)[0,:])\n",
    "                delta = after_action_value - before_action_value\n",
    "                print(\"{} : before_action_value id = {}\".format(np.round(before_action_value,3),id(before_action_value)))\n",
    "                print(\"{} : target_action_value id = {}\".format(np.round(target_action_value,3),id(target_action_value)))\n",
    "                print(\"{} : after_action_value id = {}\".format(np.round(after_action_value,3),id(after_action_value)))\n",
    "                print(\"{} : delta action value\".format(np.round(delta,3)))\n",
    "                state = ((0,0,0,0,0,0,0,0,0))\n",
    "                state_2d = self.state_convert(state)\n",
    "                x = np.array([state_2d],dtype=np.float32).astype(np.float32)\n",
    "                qvalues = self.main_network.predict(x)[0,:]\n",
    "                print(\"{} : initial state qvalues\".format(np.round(qvalues,3)))\n",
    "\n",
    "            \n",
    "        if self.print:\n",
    "            print(\"-----------   learn_qtable end -------------\")\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN 플레이어 훈련 : p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 3, 3, 16)          304       \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 3, 3, 32)          4640      \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 3, 3, 64)          18496     \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 256)               147712    \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 9)                 585       \n",
      "=================================================================\n",
      "Total params: 212,889\n",
      "Trainable params: 212,889\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_10 (Conv2D)           (None, 3, 3, 16)          304       \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 3, 3, 32)          4640      \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 3, 3, 64)          18496     \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 256)               147712    \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 9)                 585       \n",
      "=================================================================\n",
      "Total params: 212,889\n",
      "Trainable params: 212,889\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "p2 player is Random player\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 20000/20000 [31:01<00:00, 10.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p1 = 14087 p2 = 3764 draw = 2149\n",
      "p2 player is MC player\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 20000/20000 [1:26:17<00:00,  3.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p1 = 20358 p2 = 17115 draw = 2527\n",
      "p2 player is Q_player\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 20000/20000 [31:39<00:00, 11.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p1 = 28745 p2 = 24429 draw = 6826\n",
      "end learn\n",
      "end save model\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "p1_DQN = DQN_player()\n",
    "\n",
    "print_opt = False\n",
    "p1_DQN.print = print_opt\n",
    "p1_DQN.print1 = print_opt\n",
    "\n",
    "p1_score = 0\n",
    "p2_score = 0\n",
    "draw_score = 0\n",
    "\n",
    "max_learn = 20000\n",
    "\n",
    "trend = []\n",
    "\n",
    "for k in range(3):\n",
    "    if k == 0:\n",
    "        p2 = Random_player()\n",
    "    elif k == 1:\n",
    "        p2 = Monte_Carlo_player()\n",
    "        p2.num_playout = 100\n",
    "    elif k == 2:\n",
    "        p2 = p2_Qplayer\n",
    "        p2.epsilon = 0.5\n",
    "        p2.print = False\n",
    "        p2.print1 = False\n",
    "        \n",
    "    print(\"p2 player is {}\".format(p2.name))\n",
    "\n",
    "    for j in tqdm(range(max_learn)):\n",
    "        np.random.seed(j)\n",
    "        env = Environment()\n",
    "        \n",
    "        # 시작할 때 메인 신경망의 가중치를 타깃 신경망의 가중치로 복사\n",
    "        p1_DQN.epsilon = 0.7\n",
    "        p1_DQN.copy_network()\n",
    "\n",
    "        for i in range(10000):\n",
    "            # p1 행동을 선택\n",
    "            player = 1\n",
    "            pos = p1_DQN.policy(env)\n",
    "\n",
    "            p1_board_backup = tuple(env.board_a)\n",
    "            p1_action_backup = pos\n",
    "\n",
    "            env.board_a[pos] = player\n",
    "            env.end_check(player)\n",
    "\n",
    "            # 게임 종료라면\n",
    "            if env.done == True:\n",
    "                # p1의 승리이므로 마지막 행동에 보상 +1\n",
    "                # p2는 마지막 행동에 보상 -1\n",
    "                # p1 행동의 결과는 이기거나 비기거나\n",
    "                if env.reward == 0:\n",
    "                    p1_DQN.learn_dqn(p1_board_backup, p1_action_backup, env, 0)\n",
    "                    draw_score += 1\n",
    "                    break\n",
    "                else:\n",
    "                    p1_DQN.learn_dqn(p1_board_backup, p1_action_backup, env, 1)\n",
    "                    p1_score += 1\n",
    "                    break\n",
    "\n",
    "            # p2 행동을 선택\n",
    "            player = -1\n",
    "            pos = p2.select_action(env, player)\n",
    "            env.board_a[pos] = player\n",
    "            env.end_check(player)\n",
    "\n",
    "            if env.done == True:\n",
    "                # p2승리 = p1 패배 마지막 행동에 보상 -1\n",
    "                # 비기면 보상 : 0\n",
    "                if env.reward == 0:\n",
    "                    p1_DQN.learn_dqn(p1_board_backup, p1_action_backup, env, 0)\n",
    "                    draw_score += 1\n",
    "                    break\n",
    "                else:\n",
    "                    # 지면 보상 : -1\n",
    "                    p1_DQN.learn_dqn(p1_board_backup, p1_action_backup, env, -1)\n",
    "                    p2_score += 1\n",
    "                    break\n",
    "\n",
    "            # 게임이 끝나지 않았다면 p1의 Q-talble 학습\n",
    "            p1_DQN.learn_dqn(p1_board_backup, p1_action_backup, env, 0)\n",
    "\n",
    "        # 5게임마다 메인 신경망의 가중치를 타깃 신경망의 가중치로 복사\n",
    "        if j%5 == 0:\n",
    "            p1_DQN.copy_network()\n",
    "\n",
    "    print(\"p1 = {} p2 = {} draw = {}\".format(p1_score,p2_score,draw_score))\n",
    "print(\"end learn\")\n",
    "\n",
    "p1_DQN.save_network(\"p1_DQN_0708\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 게임 진행 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pl player : Human player\n",
      "p2 player : MC player\n",
      "possible actions = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "+----+----+----+\n",
      "+  0 +  1 +  2 +\n",
      "+----+----+----+\n",
      "+  3 +  4 +  5 +\n",
      "+----+----+----+\n",
      "+  6 +  7 +  8 +\n",
      "+----+----+----+\n",
      "Select action(human) : 4\n",
      "+----+----+----+\n",
      "|    |    |    |\n",
      "+----+----+----+\n",
      "|    |  O |    |\n",
      "+----+----+----+\n",
      "|    |    |    |\n",
      "+----+----+----+\n",
      "+----+----+----+\n",
      "|    |    |    |\n",
      "+----+----+----+\n",
      "|    |  O |    |\n",
      "+----+----+----+\n",
      "|  X |    |    |\n",
      "+----+----+----+\n",
      "possible actions = [0, 1, 2, 3, 5, 7, 8]\n",
      "+----+----+----+\n",
      "+  0 +  1 +  2 +\n",
      "+----+----+----+\n",
      "+  3 +  4 +  5 +\n",
      "+----+----+----+\n",
      "+  6 +  7 +  8 +\n",
      "+----+----+----+\n",
      "Select action(human) : 1\n",
      "+----+----+----+\n",
      "|    |  O |    |\n",
      "+----+----+----+\n",
      "|    |  O |    |\n",
      "+----+----+----+\n",
      "|  X |    |    |\n",
      "+----+----+----+\n",
      "+----+----+----+\n",
      "|    |  O |    |\n",
      "+----+----+----+\n",
      "|    |  O |    |\n",
      "+----+----+----+\n",
      "|  X |  X |    |\n",
      "+----+----+----+\n",
      "possible actions = [0, 2, 3, 5, 8]\n",
      "+----+----+----+\n",
      "+  0 +  1 +  2 +\n",
      "+----+----+----+\n",
      "+  3 +  4 +  5 +\n",
      "+----+----+----+\n",
      "+  6 +  7 +  8 +\n",
      "+----+----+----+\n",
      "Select action(human) : 8\n",
      "+----+----+----+\n",
      "|    |  O |    |\n",
      "+----+----+----+\n",
      "|    |  O |    |\n",
      "+----+----+----+\n",
      "|  X |  X |  O |\n",
      "+----+----+----+\n",
      "+----+----+----+\n",
      "|  X |  O |    |\n",
      "+----+----+----+\n",
      "|    |  O |    |\n",
      "+----+----+----+\n",
      "|  X |  X |  O |\n",
      "+----+----+----+\n",
      "possible actions = [2, 3, 5]\n",
      "+----+----+----+\n",
      "+  0 +  1 +  2 +\n",
      "+----+----+----+\n",
      "+  3 +  4 +  5 +\n",
      "+----+----+----+\n",
      "+  6 +  7 +  8 +\n",
      "+----+----+----+\n",
      "Select action(human) : 5\n",
      "+----+----+----+\n",
      "|  X |  O |    |\n",
      "+----+----+----+\n",
      "|    |  O |  O |\n",
      "+----+----+----+\n",
      "|  X |  X |  O |\n",
      "+----+----+----+\n",
      "+----+----+----+\n",
      "|  X |  O |    |\n",
      "+----+----+----+\n",
      "|  X |  O |  O |\n",
      "+----+----+----+\n",
      "|  X |  X |  O |\n",
      "+----+----+----+\n",
      "winner is p2(MC player)\n",
      "final result\n",
      "+----+----+----+\n",
      "|  X |  O |    |\n",
      "+----+----+----+\n",
      "|  X |  O |  O |\n",
      "+----+----+----+\n",
      "|  X |  X |  O |\n",
      "+----+----+----+\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "p1 = Human_player()\n",
    "# p2 = Human_player()\n",
    "\n",
    "# p1 = Random_player()\n",
    "# p2 = Random_player()\n",
    "\n",
    "# p1 = Monte_Carlo_player()\n",
    "# p1.num_playout = 100\n",
    "p2 = Monte_Carlo_player()\n",
    "p2.num_playout = 1000\n",
    "\n",
    "# p1 = p1_Qplayer\n",
    "# p1.epsilon = 0\n",
    "\n",
    "# p2 = p2_Qplayer\n",
    "# p2.epsilon = 0\n",
    "\n",
    "# p1 = p1_DQN\n",
    "# p1.epsilon = 0\n",
    "\n",
    "# 지정된 게임 수를 자동으로 두게 할 것인지 한게임씩 두게 할 것인지 결정\n",
    "# auto = True : 지정된 판수(games)를 자동으로 진행 \n",
    "# auto = False : 한판씩 진행\n",
    "\n",
    "auto = False\n",
    "\n",
    "# auto 모드의 게임수\n",
    "games = 100\n",
    "\n",
    "print(\"pl player : {}\".format(p1.name))\n",
    "print(\"p2 player : {}\".format(p2.name))\n",
    "\n",
    "# 각 플레이어의 승리 횟수를 저장\n",
    "p1_score = 0\n",
    "p2_score = 0\n",
    "draw_score = 0\n",
    "\n",
    "\n",
    "if auto: \n",
    "    # 자동 모드 실행\n",
    "    for j in tqdm(range(games)):\n",
    "        \n",
    "        np.random.seed(j)\n",
    "        env = Environment()\n",
    "        \n",
    "        for i in range(10000):\n",
    "            # p1 과 p2가 번갈아 가면서 게임을 진행\n",
    "            # p1(1) -> p2(-1) -> p1(1) -> p2(-1) ...\n",
    "            reward, done = env.move(p1,p2,(-1)**i)\n",
    "            # 게임 종료 체크\n",
    "            if done == True:\n",
    "                if reward == 1:\n",
    "                    p1_score += 1\n",
    "                elif reward == -1:\n",
    "                    p2_score += 1\n",
    "                else:\n",
    "                    draw_score += 1\n",
    "                break\n",
    "\n",
    "else:                \n",
    "    # 한 게임씩 진행하는 수동 모드\n",
    "    np.random.seed(1)\n",
    "    while True:\n",
    "        \n",
    "        env = Environment()\n",
    "        env.print = False\n",
    "        for i in range(10000):\n",
    "            reward, done = env.move(p1,p2,(-1)**i)\n",
    "            env.print_board()\n",
    "            if done == True:\n",
    "                if reward == 1:\n",
    "                    print(\"winner is p1({})\".format(p1.name))\n",
    "                    p1_score += 1\n",
    "                elif reward == -1:\n",
    "                    print(\"winner is p2({})\".format(p2.name))\n",
    "                    p2_score += 1\n",
    "                else:\n",
    "                    print(\"draw\")\n",
    "                    draw_score += 1\n",
    "                break\n",
    "        \n",
    "        # 최종 결과 출력        \n",
    "        print(\"final result\")\n",
    "        env.print_board()\n",
    "\n",
    "        # 한게임 더?최종 결과 출력 \n",
    "        answer = input(\"More Game? (y/n)\")\n",
    "\n",
    "        if answer == 'n':\n",
    "            break           \n",
    "\n",
    "print(\"p1({}) = {} p2({}) = {} draw = {}\".format(p1.name, p1_score,p2.name, p2_score,draw_score))\n",
    "                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
