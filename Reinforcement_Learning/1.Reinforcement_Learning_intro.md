# [ 1. Reinforcement Learning Intro ]

## 1. What is Reinforcement Learning?
- 1 ) Supervised Learning : Labeled data -> prediction & classification
- 2 ) Unsupervised Learning : Unlabeled data -> clustering, recommendation system
- 3 ) Reinforcement Learning : Decision process, Reward Sytem </br>
"주어진 상황(State)에서, 받을 수 있는 보상(Reward)를 극대화 하는 행동(Action)을 선택하도록!"
</br></br>
## 2. Reinforcement Learning Components </br>
RL의 구성요소는? </br>
<img src="https://www.kdnuggets.com/images/reinforcement-learning-fig1-700.jpg" width="550" /> </br>
( 출처 : https://www.kdnuggets.com/images/reinforcement-learning-fig1-700.jpg ) </br>
- 1 ) Agent(에이전트) : 상태를 관찰, 행동을 선택하는 주체
- 2 ) Environment(환경) : Agent가 속해 있는 상황(현재 속해있는 시점의 상황 X.전체적인 상황)
 ( 쉽게 말해서, Agent를 제외한 나머지 )
- 3 ) State(상태) : 현재 Agent가 속해 있는 상태의 정보
- 4 ) Action(행동) : 현재 state에서 Agent가 하는 행동
- 5 ) Reward(보상) : Agent가 action을 했을 때 받게 되는 보상
- 6 ) Policy(정책) : 주어진 환경에서 어떠한 action을 해야한다는 행동 요령 </br> </br>

요약 : **"Agent(에이전트)가 Environment(환경)에서 자신의 state(상태)를 관찰하고, 어떠한 기준("가치 함수")에 따라 
행동을 선택한다. ( 여기서 "가치 함수"는, 현재 상태에서 미래에 받을 것으로 기대되는 Reward의 (discounted) 합을 
의미한다 ). 그리고 선택한 행동을 수행하면, 다음 state으로 넘어감과 함께 Reward를 받는다. 여기서 받는 Reward를 
통해, Agent는 가지고 있는 정보를 update한다"**
</br></br>
## 3. Value function & Q-Value function
#### 1) Value function 
여기서의 'value'는, 단기적인 보상(Reward)만을 이야기하는 것이 아니다. 장기적인 관점에서, 미래에 받게 될 보상
까지 고려한 reward를 의미한다. 여기서, 미래에 받을 reward를 할인율(감마)를 통해 discount하기도 한다. 이를 식으로
나타내면 다음과 같다. </br></br>
<a href="https://www.codecogs.com/eqnedit.php?latex=V_\pi&space;(s)&space;=&space;E[R(s0)&plus;\gamma&space;R(s1)&space;&plus;\gamma^2&space;R(s2)&plus;....|s0=s,\pi&space;]" target="_blank"><img src="https://latex.codecogs.com/gif.latex?V_\pi&space;(s)&space;=&space;E[R(s0)&plus;\gamma&space;R(s1)&space;&plus;\gamma^2&space;R(s1)&plus;....|s0=s,\pi&space;]" title="V_\pi (s) = E[R(s0)+\gamma R(s1) +\gamma^2 R(s1)+....|s0=s,\pi ]" /></a>
</br></br>
위 식은 value function <a href="https://www.codecogs.com/eqnedit.php?latex=V_\pi&space;(s)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?V_\pi&space;(s)" title="V_\pi (s)" /></a>
를 나타내는 식으로, 해석하자면 "현재 상태 s0에서 정책 <a href="https://www.codecogs.com/eqnedit.php?latex=\pi" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\pi" title="\pi" /></a>
에 따라 행동 했을 때 기대되는 보상"을 의미한다. </br> </br>

#### 2) Q-Value function
Q-value function이 value function과 다른 점은, action을 고려하는지의 유무이다. Value function에서는 
"현재의 상태과 정책"이 주어졌을 때의 Reward 관련 함수라면, Q-value function은 "현재의 상태,정책 그리고 현재의
상태에서 취할 행동"이 주어졌을 때의 Reward 관련된 함수이다. 이를 식으로 나타내면 다음과 같다. </br></br>
<a href="https://www.codecogs.com/eqnedit.php?latex=Q_\pi&space;(s,a)&space;=&space;E[R(s0,a0)&plus;\gamma&space;R(s1,a1)&space;&plus;\gamma^2&space;R(s2,a2)&plus;....|s0=s,a0=a,\pi&space;]" target="_blank"><img src="https://latex.codecogs.com/gif.latex?Q_\pi&space;(s,a)&space;=&space;E[R(s0,a0)&plus;\gamma&space;R(s1,a1)&space;&plus;\gamma^2&space;R(s2,a2)&plus;....|s0=s,a0=a,\pi&space;]" title="Q_\pi (s,a) = E[R(s0,a0)+\gamma R(s1,a1) +\gamma^2 R(s2,a2)+....|s0=s,a0=a,\pi ]" /></a>
</br></br>
## 4. Kinds of Reinforcement Learning
- 1 ) Model : agent가 존재하는 environment를 modeling한 것
- 2 ) Policy based agent : value function 없이, 오직 policy와 model만으로 구성
- 3 ) Value based agent : policy 없이, 오직 value function과 model만으로 구성
- 4 ) Model based agent & Model Free agent : model에 대한 정보가 곧 state transition의 정보
- 5 ) Actor Critic : policy + value function + model 모두 사용 
