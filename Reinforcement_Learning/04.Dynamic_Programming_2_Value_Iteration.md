# 4. Dynamic Programming 2
## 1) Value Iteration
앞서서는 Policy Iteration에 대해 알아보았다 ( Policy Iteration은 크게 '각 state에서의 value function을 찾아내는' **Policy Evaluation**과, '각 
state에서 optimal policy를 찾아내는' **Policy Improvement**로 이루어짐을 확인하였다 ) </br> </br>

Dynamic Programming으로 MDP를 푸는 또 다른 방법인 Value Iteration은, 다음과 같은 프로세스를 가진다.
1) Initialize V(s)=0, for all s </br>
2) Repeat until converge </br>
<a href="https://www.codecogs.com/eqnedit.php?latex=V(s)&space;=&space;R(s)&space;&plus;&space;\underset{a&space;\in&space;A}{max}&space;\gamma&space;\sum_{a'}^{&space;}P_{sa}(s')V(s')" target="_blank"><img src="https://latex.codecogs.com/gif.latex?V(s)&space;=&space;R(s)&space;&plus;&space;\underset{a&space;\in&space;A}{max}&space;\gamma&space;\sum_{a'}^{&space;}P_{sa}(s')V(s')" title="V(s) = R(s) + \underset{a \in A}{max} \gamma \sum_{a'}^{ }P_{sa}(s')V(s')" /></a>
  for all s. </br> </br>
 Policy Iteration과의 차이점은, improvement 과정이 없다는 것이다. Policy Iteration에서는 Evaluation을 통해 state의 value를 알아내고, improvement를 
 통해 최적의 policy를 찾아낸 반면, Value Iteration에서는 (value에 각 action을 취할 확률을 곱해서 summation을 하는 대신) max값을 바로 현재 state의
 value로 취한다. 아래 그림을 통해 확인해보자. (출처 : https://t1.daumcdn.net/cfile/tistory/99B8303A5A47731F10 )</br> </br>  
<img src="https://t1.daumcdn.net/cfile/tistory/99B8303A5A47731F10" width="500" />  </br> </br>
 위 그림을 통해서 알 수 있듯, 각각의 state는 해당 state에서 갈 수 있는 4개의 state들 중 max value를 자신의 value로 만든다. 이렇게 iteration을 여러 번
 반복할 경우, 아래 그림과 같이 value들이 update된다. </br> </br>
 <img src="https://t1.daumcdn.net/cfile/tistory/990D2B365A489D6C21" width="500" />  </br> </br>
 
 ## 2) 정리
 Policy Iteration과 Value Iteration과 같은 Dynamic Programming 방식은 엄청난 계산을 요한다. 위의 경우에는 4x4 grid이어서 간단해
 보였을 수 있지만, 실제 세상에서는 이 방법을 통해서 문제를 푸는데에는 제약이 있다. 이어지는 포스트에서는 이러한 문제점을 보완하는
 Monte Carlo Method에 대해서 알아보자.
 
