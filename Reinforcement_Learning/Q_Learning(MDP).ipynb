{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### https://stackoverflow.com/questions/52885901/how-to-save-python-script-as-py-file-on-jupyter-notebook/52886052"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import time\n",
    "\n",
    "from termcolor import colored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "#         #           #  Goal  #\n",
    "####################\n",
    "#         #           # Hole   #\n",
    "####################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make a Q world!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qworld():\n",
    "    def __init__(self):\n",
    "        # row&col\n",
    "        self.col=4 # col : number of actions\n",
    "        self.row=6 # 2x3 grid game\n",
    "        # tables\n",
    "        self.q_table=np.zeros([self.row,self.col]) # what we wil get when we do certain 'action' at certain 'state'\n",
    "        self.init_transition_table()\n",
    "        self.init_reward_table()\n",
    "        # parameteres\n",
    "        self.gamma=0.9 # discount rate\n",
    "        self.epsilon=0.9 # 90% explore, 10% exploit\n",
    "        self.epsilon_decay=0.9 # decaying exploration rate\n",
    "        self.epsilon_min=0.1\n",
    "        self.reset()\n",
    "        self.is_explore=True\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state=0\n",
    "        return self.state\n",
    "    \n",
    "    def isin_win(self):\n",
    "        return self.state ==2\n",
    "    \n",
    "    # what we get in the current state (x)\n",
    "    # what we get in the current state+action (o)\n",
    "    def init_reward_table(self): \n",
    "        self.reward_table[1,2] = 100. # in state1(=(0,1) of the grid), you get 100 when you do action2(=go right)\n",
    "        self.reward_table[4,2] = -100.\n",
    "    \n",
    "    def init_transition_table(self): # where the agent would be, when followed by the Q-table\n",
    "        self.transition_table = np.zeros([self.row, self.col], dtype=int)\n",
    "        self.transition_table[0,0]=0\n",
    "        self.transition_table[0,1]=3\n",
    "        self.transition_table[0,2]=1\n",
    "        self.transition_table[0,3]=0\n",
    "        \n",
    "        self.transition_table[1,0]=0\n",
    "        self.transition_table[1,1]=4\n",
    "        self.transition_table[1,2]=2\n",
    "        self.transition_table[1,3]=1\n",
    "        \n",
    "        # trainsition_table[2,n] : goal ( no state change )\n",
    "        self.transition_table[2,0]=2\n",
    "        self.transition_table[2,1]=2\n",
    "        self.transition_table[2,2]=2\n",
    "        self.transition_table[2,3]=2\n",
    "        \n",
    "        self.transition_table[3,0]=3\n",
    "        self.transition_table[3,1]=3\n",
    "        self.transition_table[3,2]=4\n",
    "        self.transition_table[3,3]=0\n",
    "        \n",
    "        self.transition_table[4,0]=3\n",
    "        self.transition_table[4,1]=4\n",
    "        self.transition_table[4,2]=5\n",
    "        self.transition_table[4,3]=1\n",
    "        \n",
    "        # trainsition_table[5,n] : hole ( no state change )\n",
    "        self.transition_table[5,0]=5\n",
    "        self.transition_table[5,1]=5\n",
    "        self.transition_table[5,2]=5\n",
    "        self.transition_table[5,3]=5\n",
    "    \n",
    "    def step(self,action)    :\n",
    "        next_state = self.transition_table\n",
    "        done = (next_state==2 or next_state==5)\n",
    "        reward = self.reward_table[self.state, action]\n",
    "        self.state = next_state\n",
    "        return next_state, reward, done\n",
    "    \n",
    "    def act(self):\n",
    "        # explore\n",
    "        if np.random.rand() <= self.epsilon: # return 0~1\n",
    "            self.is_explore = True\n",
    "            return np.random.choice(4,1)[0]\n",
    "        \n",
    "        # exploit\n",
    "        self.is_explore = False\n",
    "        return np.argmax(self.q_table[self.state]) # take the action with biggest return on the certain row(=certain state)\n",
    "    \n",
    "    def update_q_table(self,state,action,reward,next_state):\n",
    "        q_value = self.gamma*np.amax(self.q_table[next_state])\n",
    "        q_value = reward+q_value\n",
    "        self.q_table[state,action]=q_value\n",
    "        \n",
    "    def print_q_table(self):\n",
    "        print(\"Q-Table(Epsilon : %0.2f)\" % self.epsilon)\n",
    "        print(self.q_table)\n",
    "    \n",
    "    def update_epsilon(self):\n",
    "        if self.epsilon > self.epsilon_min: # no further decaying after the min value\n",
    "            self.epsilon *= self.epsilon_decay "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing Episode\n",
    "def print_episode(episode, delay=1):\n",
    "    os.system('clear')\n",
    "    for _ in range(13):\n",
    "        print('=', end='')\n",
    "    print(\"\")\n",
    "    print(\"Episode \", episode)\n",
    "    for _ in range(13):\n",
    "        print('=', end='')\n",
    "    print(\"\")\n",
    "    time.sleep(delay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing Status\n",
    "def print_status(q_world, done, step, delay=1):\n",
    "    os.system('clear')\n",
    "    q_world.print_world(action, step)\n",
    "    q_world.print_q_table()\n",
    "    if done:\n",
    "        print(\"-------EPISODE DONE--------\")\n",
    "        delay *= 2\n",
    "    time.sleep(delay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [-t]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\samsung\\AppData\\Roaming\\jupyter\\runtime\\kernel-53dbbf53-dbe9-4300-a00a-e8c996464f8d.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samsung\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3334: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    help_ = \"Trains and show final Q Table\"\n",
    "    parser.add_argument(\"-t\",\n",
    "                        \"--train\",\n",
    "                        help=help_,\n",
    "                        action='store_true')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.train:\n",
    "        maxwins = 2000\n",
    "        delay = 0\n",
    "    else:\n",
    "        maxwins = 10\n",
    "        delay = 1\n",
    "\n",
    "    wins = 0\n",
    "    episode_count = 10 * maxwins\n",
    "    # scores (max number of steps before goal) - good indicator of learning\n",
    "    scores = deque(maxlen=maxwins)\n",
    "    q_world = QWorld()\n",
    "    step = 1\n",
    "\n",
    "    # state, action, reward, next state iteration\n",
    "    for episode in range(episode_count):\n",
    "        state = q_world.reset()\n",
    "        done = False\n",
    "        print_episode(episode, delay=delay)\n",
    "        while not done:\n",
    "            action = q_world.act()\n",
    "            next_state, reward, done = q_world.step(action)\n",
    "            q_world.update_q_table(state, action, reward, next_state)\n",
    "            print_status(q_world, done, step, delay=delay)\n",
    "            state = next_state\n",
    "            # if episode is done, perform housekeeping\n",
    "            if done:\n",
    "                if q_world.is_in_win_state():\n",
    "                    wins += 1\n",
    "                    scores.append(step)\n",
    "                    if wins > maxwins:\n",
    "                        print(scores)\n",
    "                        exit(0)\n",
    "                # Exploration-Exploitation is updated every episode\n",
    "                q_world.update_epsilon()\n",
    "                step = 1\n",
    "            else:\n",
    "                step += 1\n",
    "\n",
    "    print(scores)\n",
    "    q_world.print_q_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
