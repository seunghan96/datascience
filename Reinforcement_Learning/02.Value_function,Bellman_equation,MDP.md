# [ 2. Value function, Bellman Equation, MDP ]

## 1. Value function
어떤 행동이 좋은 행동인가?이를 판단하기 위한 지표!
- 보상(reward) : Agent의 행동에 따라 받게 되는 것
- 할인율(discount factor, 감마) : 미래의 받게되는 보상을 현재로 당겼을 때 할인하는 비율!
- return(G, total discount reward) : 미래에 받게되는 모든 reward까지 현재가치로 표현한 것. 이 G를 식으로 표현
하면 다음과 같다. </br> </br>
<a href="https://www.codecogs.com/eqnedit.php?latex=G_t&space;=&space;R_{t&plus;1}&space;&plus;&space;\gamma&space;R_{t&plus;2}&space;&plus;&space;\gamma^2&space;R_{t&plus;3}&space;&plus;&space;...&space;=&space;\sum_{k=0}^{\infty&space;}\gamma^k&space;R_{t&plus;k&plus;1}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?G_t&space;=&space;R_{t&plus;1}&space;&plus;&space;\gamma&space;R_{t&plus;2}&space;&plus;&space;\gamma^2&space;R_{t&plus;3}&space;&plus;&space;...&space;=&space;\sum_{k=0}^{\infty&space;}\gamma^k&space;R_{t&plus;k&plus;1}" title="G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... = \sum_{k=0}^{\infty }\gamma^k R_{t+k+1}" /></a>
</br> </br>
- 가치(Value, Value Function) : 현재 상태(s)에서, (앞으로) 기대되는 모든 return들의 합의 . 이 V를 식으로 표현하면 다음과 같이 됨을, 앞의 
return값을 통해 알 수 있다.</br> </br>
<a href="https://www.codecogs.com/eqnedit.php?latex=V(s)&space;=&space;E[R(s0)&plus;&space;\gamma&space;R(s1)&space;&plus;&space;\gamma^2&space;R(s2)&space;&plus;&space;....&space;|s0=s]" target="_blank"><img src="https://latex.codecogs.com/gif.latex?V(s)&space;=&space;E[R(s0)&plus;&space;\gamma&space;R(s1)&space;&plus;&space;\gamma^2&space;R(s2)&space;&plus;&space;....&space;|s0=s]" title="V(s) = E[R(s0)+ \gamma R(s1) + \gamma^2 R(s2) + .... |s0=s]" /></a>
</br> </br>

## 2. Bellman Equation
Value function은 크게 두 부분으로 나뉠 수 있다.</br>
1) 지금(현재 t시점) 즉시 받는 보상값 (immediate reward, <a href="https://www.codecogs.com/eqnedit.php?latex=R_{t&plus;1}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?R_{t&plus;1}" title="R_{t+1}" /></a>)
2) 미래 (t+1 시점~)에 받게되는 보상값들 (discounted value of successor state, <a href="https://www.codecogs.com/eqnedit.php?latex=\gamma&space;v(S_{t&plus;1})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\gamma&space;v(S_{t&plus;1})" title="\gamma v(S_{t+1})" /></a>)

<img src="https://dnddnjs.gitbooks.io/rl/content/dfq.png" width="650" /> </br>
( 구체적인 유도과정은 위 수식을 참고 ) </br>
( 출처 : https://dnddnjs.gitbooks.io/rl/content/dfq.png )
</br> </br>

## 3. MDP (Markov Decision Process)
### 1) Markov Reward Process
Markov Reward Process는 Markov Process의 각 state에 'reward' 개념이 추가된 것이다. </br>
이 process는 다음과 같은 4개의 표현 <a href="https://www.codecogs.com/eqnedit.php?latex=<S,P,R,\gamma>" target="_blank"><img src="https://latex.codecogs.com/gif.latex?<S,P,R,\gamma>" title="<S,P,R,\gamma>" /></a>으로 
나타낼 수 있다.

- 1 ) S : State의 집합
- 2 ) P : transition Probability ( 즉, 상태 s에서 다음 상태인 s'으로 이동할 확률의 집합으로 보면 된다. ) </br>
이 집합의 각 요소는 <a href="https://www.codecogs.com/eqnedit.php?latex=p(s'|s)&space;=&space;Pr(S_{t&plus;1}=s'|S_{t}=s)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?p(s'|s)&space;=&space;Pr(S_{t&plus;1}=s'|S_{t}=s)" title="p(s'|s) = Pr(S_{t+1}=s'|S_{t}=s)" /></a>가 된다

- 3 ) R : Reward의 집합 </br>
이 집합의 각 요소는 <a href="https://www.codecogs.com/eqnedit.php?latex=r(s)&space;=&space;E[R_{t&plus;1}|S_t=s]" target="_blank"><img src="https://latex.codecogs.com/gif.latex?r(s)&space;=&space;E[R_{t&plus;1}|S_t=s]" title="r(s) = E[R_{t+1}|S_t=s]" /></a>가 된다.
- 4 ) <a href="https://www.codecogs.com/eqnedit.php?latex=\gamma" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\gamma" title="\gamma" /></a> : 할인율
</br>
Markov Reward Process의 예시를 한번 보자.
<img src="https://t1.daumcdn.net/cfile/tistory/99E9FD355AEB1E0927" width="550" /> </br>
( 출처 : https://t1.daumcdn.net/cfile/tistory/99E9FD355AEB1E0927 ) </br>
위 그림에서 Agent는 탐험을 통해 Room들을 지나서, Outside, Trap, 혹은 Treasure에 도착함으로서 모험을 종료한다.
이 그림을 보면, Markov Reward Process의 요소인 S(state의 집합, 즉 3개의 room, outside, trap, treasure)와,
R(reward. state별로 도착 시 얻게되는 reward가 차이가 있음을 알 수 있다), P(각 state에서 인접한 다음 state로 이동할 확률)을 알 수 있따.
( 할인율은 따로 표시되어 있지 않지만, 0.5라고 해보자 ) </br>
만약 이 탐험가가 현재 Room1에 있고, Room1 -> Room2 -> Outside 순으로 탐험하였다고 하자. 그러면 이 사람이 
받게 되는 discounted total reward는 -1 + (0.5)x(-2) + (0.5)x(0.5)x1 = -1.75이다.
이와 같은 방법으로 계속 value function( discounted total reward의 평균.기대값 )을 계산한다면, 그 경우의 수는
매우 많아서 (시간 복잡도 : O(n^3) ), 이를 해결하기 위한 다른 iterative solving method ( ex. Dynamic Programming, Temporal
Difference Learning, Monte-Carlo Method )등이 있다.

### 2) Markov Decision Process
MDP는 MRP에 action이라는 요소가 추가된 것이다. </br>
(MRP)<a href="https://www.codecogs.com/eqnedit.php?latex=<S,P,R,\gamma>" target="_blank"><img src="https://latex.codecogs.com/gif.latex?<S,P,R,\gamma>" title="<S,P,R,\gamma>" /></a> 
-> (MDP) <a href="https://www.codecogs.com/eqnedit.php?latex=<S,P,R,\gamma>" target="_blank"><img src="https://latex.codecogs.com/gif.latex?<S,A,P,R,\gamma>" title="<S,P,R,\gamma>" /></a>
</br>
더 진행하기에 앞서서, 이전에 말했던 정책(policy)에 대해 다시 얘기해보자. 정책은, agent가 각 상태에서 취할 수 있는
"action들에 대한 확률분포"라고 볼 수 있다. 그 식은 다음과 같이 표현할 수 있다. </br></br>
<a href="https://www.codecogs.com/eqnedit.php?latex=\pi(a|s)&space;=&space;P[A_t=a|S_t=s]" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\pi(a|s)&space;=&space;P[A_t=a|S_t=s]" title="\pi(a|s) = P[A_t=a|S_t=s]" /></a>
</br></br>
MDP의 (state) value function은 MRP와 마찬가지로 state s에서 기대되는 (미래 reward까지 고려한)모든 return들의 기대값이다. 하지만 MRP와의
차이점은, "주어진 policy에 따라" action을 결정한다는 것이다. 그렇기 때문에 MDP의 (state) value function와 (state) action-value function은 
다음과 같이 나타낼 수 있다.
<img src="https://image.slidesharecdn.com/reinforcementlearning-170904070210/95/introduction-of-deep-reinforcement-learning-22-638.jpg?cb=1504578048" width="650" /> </br>
( 출처 : https://image.slidesharecdn.com/reinforcementlearning-170904070210/95/introduction-of-deep-reinforcement-learning-22-638.jpg?cb=1504578048 )

</br> </br>
( (state) value function과 (state) action value function을 각각 'state value function','action value function'이라고 부르자 ) </br>
최적의 state value function과 action value function은, 위 식의 두 값을 각각 최대화 하는 policy <a href="https://www.codecogs.com/eqnedit.php?latex=\pi" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\pi" title="\pi" /></a>를 찾음으로서 알 수 있다. </br ></br>
<a href="https://www.codecogs.com/eqnedit.php?latex=v_{*}(s)&space;=&space;\underset{\pi}{max}v_\pi(s)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?v_{*}(s)&space;=&space;\underset{\pi}{max}v_\pi(s)" title="v_{*}(s) = \underset{\pi}{max}v_\pi(s)" /></a> </br>
<a href="https://www.codecogs.com/eqnedit.php?latex=q_{*}(s,a)&space;=&space;\underset{\pi}{max}q_\pi(s,a)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?q_{*}(s,a)&space;=&space;\underset{\pi}{max}q_\pi(s,a)" title="q_{*}(s,a) = \underset{\pi}{max}q_\pi(s,a)" /></a>
