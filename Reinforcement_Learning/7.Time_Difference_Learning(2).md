# 7. Time Difference Learning (2)
## 1. <a href="https://www.codecogs.com/eqnedit.php?latex=TD(\lambda)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?TD(\lambda)" title="TD(\lambda)" /></a>

### (1) Forward-view <a href="https://www.codecogs.com/eqnedit.php?latex=TD(\lambda)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?TD(\lambda)" title="TD(\lambda)" /></a> : n-step TD
각기 다른 n의 n-step return을 평균내는 방법이다. 여기서 평균을 낼 때, step별로 동일한 weight이 아닌, 다른 weight을 주어서 평균을 낸다.
</br> 
<img src="https://programmingbeenet.files.wordpress.com/2019/07/td-lambda.png" width="550" /> </br>
( 출처 : https://programmingbeenet.files.wordpress.com/2019/07/td-lambda.png ) </br> </br>
위 식을 보면, step 1에서는 (1-람다)만큼의 weight를, 그 이후로는 (1-람다)*람다, 또 그 이후로는 (1-람다)*람다^2...처럼 갈수록 weight가 줄어드는 
(람다는 0~1사이 값이다)식으로 가중치를 부여한다. 각 time step의 return 앞에 붙는 weight 값들의 합은 1이 된다.

이 방법은 TD(0)와는 다르게, 모든 time step의 return값들을 반영할 수 있다는 장점이 있다. 하지만, 이 방법은 결국 Monte Carlo처럼 episode가 한번 다 끝나야 update가 이루어지는
어떻게 보면 Time Difference가 본래 추구하고자 했던 online update가 불가능해진다는 단점이 있다. 

### (2) Backward-view <a href="https://www.codecogs.com/eqnedit.php?latex=TD(\lambda)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?TD(\lambda)" title="TD(\lambda)" /></a> : eligibility trace
Backward-view 방법은, Forward view와는 다르게, episode가 다 끝나지 않아도 incomplete sequence으로 부터 online으로 update할 수 있는 방법이다.
이를 이루기 위해, "Eligibility trace"를 도입한다. 이것이 무엇인지 알아 보기 전에, Frequency heuristic과 Recency heuristic에 대해 알아보자. 
</br> </br>
- Frequencey heuristic : "자주" 발생하는 state에 credit 부여!
- Recency heuristic : "최근"에 발생한 state에 credit 부여!
</br></br>
backward-view TD의 updating equation은 다음과 같다.
</br> </br>
<img src="https://t1.daumcdn.net/cfile/tistory/99FA92475A63529E31" width="290" /> </br>
<img src="https://t1.daumcdn.net/cfile/tistory/992F3D465A6352FE32" width="350" />
</br> </br>
위 식에서 Et(s)가 바로 eligibility trace인데, 이는 앞서 이야기한 frequency heuristc과 recency heuristic을 반영한 값이다.
</br> 
<img src="https://t1.daumcdn.net/cfile/tistory/995B2F385A634F7812" width="290" /> 
</br> 
위 식을 보면, frequency heuristic를 반영하기 위해 이전에 방문한 적이 있던 state면 '+1'을 주는 것을 확인할 수 있다. 또한, recency heuristic을 반영하는
gamma가 있음을 알 수 있다. 이렇게 해서 구한 eligiblity trace를 TD error에 곱한 뒤 update를 한다. 

## 2. Summary
Forward view 방법은, 말 그대로 "미래에 방문할 state들과 그 곳에서의 reward"를 사용하여 update를 했다면, Backward view는 "TD error와 과거의 정보들"을 사용하여 update를
하였다. Forward view 방법은, Monte Carlo method가 가지던 high variance 문제를, n-step으로 줄임으로써 variance를 줄이는게 초점이라면, Backward view는me Difference
가 가지던 high bias 문제를 eligibility trace를 통해 보완하고자 함을 확인할 수 있다.
