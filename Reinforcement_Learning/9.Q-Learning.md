# 9. Q-Learning

## 1. On & Off Policy
### (1) On Policy
- 학습하는 policy = 행동하는 policy
- 한 번의 policy improvement를 한 이후에는, 그 policy가 했던 과거의 경험들은 모두 이용 불가! ( 현재 policy하에서 최적이라고 생각하는 것을 선택한 이후,
그 이전의 경험들은 모두 잊어버림 )
- ex) SARSA

### (2) Off Policy
- 학습하는 policy != 행동하는 policy ( 반드시 같을 필요는 없다 )
- 현재 학습하는 policy가 과거에 했던 경험들도 모두 기억!
- 장점 ) 여러개의 agent, 심지어 사람으로부터 학습 가능!
- ex) Q-Learning

## 2. Q-Learning
### (1) 학습하는 Policy (target policy) : Q-function을 update
'학습'만 할 뿐, 실제로 이를 행동하는 것은 아님! 다음 state에서 가장 큰 Q-function을 가지고 update함.
</br></br>
<img src="https://image.slidesharecdn.com/flowchartpart2ofreinforcementlearning-180322085118/95/part-2-55-638.jpg?cb=1521708718" width="550" /> </br>
</br>
( 출처 : https://image.slidesharecdn.com/flowchartpart2ofreinforcementlearning-180322085118/95/part-2-55-638.jpg?cb=1521708718 )
</br>

### (2) 행동하는 Policy (behavior policy) : epsilon-greedy method
( 앞으로 '행동하는 policy(behavior policy)'를 <a href="https://www.codecogs.com/eqnedit.php?latex=\mu" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mu" title="\mu" /></a>,
'학습하는 policy(target policy)'를 <a href="https://www.codecogs.com/eqnedit.php?latex=\pi" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\pi" title="\pi" /></a>라고 하겠다 ) </br>

Algorithm
1) 현재 S(state)에서 <a href="https://www.codecogs.com/eqnedit.php?latex=\mu" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mu" title="\mu" /></a>에 따라 A(action) 선택
2) BUT 다음 S(state)에서 할 행동(A')은 <a href="https://www.codecogs.com/eqnedit.php?latex=\pi" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\pi" title="\pi" /></a>에 따라 선택 </br> </br>
<a href="https://www.codecogs.com/eqnedit.php?latex=\pi(S_{t&plus;1})&space;=&space;\underset{a'}{argmax}Q(S_{t&plus;1},a')" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\pi(S_{t&plus;1})&space;=&space;\underset{a'}{argmax}Q(S_{t&plus;1},a')" title="\pi(S_{t+1}) = \underset{a'}{argmax}Q(S_{t+1},a')" /></a>
</br>
3) Q-learning의 target 구하기 </br> 
<img src="https://t1.daumcdn.net/cfile/tistory/997CA13B5A649C642D" width="550" /> </br>
</br>
4) Q-function update하기 </br>
<img src="https://t1.daumcdn.net/cfile/tistory/99FA743A5A64A2CD29" width="550" /> </br>

## 3. Q-Learning Summary
SARSA는 "on-policy"방법의 TD이고, Q-learning은 "off-policy"방법의 TD이다. 이 둘의 차이점은, Q-learning의 경우에는 behavior policy(행동하는 policy)
와 target policy(학습하는 policy)가 따로 존재한다는 점이다. 아래의 두 사진 중, 밑에 사진인 Q-Learning을 보면, epsilon-greedy method로부터 얻어진 behavior policy에서 Action을 선택하여 행동한다. 
이렇게해서 Action을 하고 Reward와 다음 State를 받은 뒤, Q-function을 max로하는 action으로 Q-function을 update한다(여기서는 target policy로 동작)</br>
<img src="https://i.stack.imgur.com/wmFny.png" width="550" /> </br>
( 출처 : https://i.stack.imgur.com/wmFny.png )

