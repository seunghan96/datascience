# 8. SARSA
## 1. 복습
policy evaluation & improvement?
- evaluation : 각 state에서의 value function 찾기
- improvement : 각 state에서의 최적의 policy 찾기

how to find value function? (=evaluation)
- Planning 기법(환경에 대한 정보가 있을 때) : Dynamic Programming 
- Learning 기법(환경에 대한 정보가 있을 때) : Monte Carlo Method, Time Difference Learning (forward & backward view)

지금까지는 value function을 찾는 evaluation 방법들에 대해서 알아 보았고, 앞으로는 optimal policy를 찾는 policy improvement에 초점을 맞출 것이다.

## 2. TD Control : SARSA
SARSA : Time Difference Learning에서, value function 대신 action-value function을 사용하고, epsilon-greedy improvement를 적용한 것이다.
</br>
그 이름이 SARSA인 이유는, 아래 식에서 볼 수 있듯 S(state),A(action),R(reward)가 나오고 그 이후에 다음 S(state'), A(Action')이 나오기 때문이다.
<img src="https://t1.daumcdn.net/cfile/tistory/998852495A636E3F01" width="550" /> </br>
( 출처 : https://t1.daumcdn.net/cfile/tistory/998852495A636E3F01 )

SARSA의 process는 다음과 같다. action-value function이 사용되었고, 보다 나은 action을 하기 위한 (epsilon-greedy improvement를 사용한) policy improvement과정이 추가됨을 알 수 있다.
<img src="https://t1.daumcdn.net/cfile/tistory/9997F2425A64220127" width="550" /> </br>
( 출처 : https://t1.daumcdn.net/cfile/tistory/9997F2425A64220127 )
