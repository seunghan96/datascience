# 5. Monte Carlo Method

## 1. Monte Carlo Approximation
</br>  <img src="https://kr.mathworks.com/matlabcentral/mlc-downloads/downloads/submissions/65563/versions/2/screenshot.jpg" width="320" />  </br> </br>
(출처 : https://kr.mathworks.com/matlabcentral/mlc-downloads/downloads/submissions/65563/versions/2/screenshot.jpg)
위 그림 처럼 파란색 점과, 빨간색 점으로 이루어진 좌표를 보자. 여기서 우리는 파란색 점이 있는 부분의 넓이를 어떻게 구할까? y=x^2의 식으로 보여서,
적분을 통해서 쉽게 계산할 수 있겠지만, 확률을 이용하여 계산(엄밀히는 approximate)하는 방법이 있다. 임의의 점을 들어, 해당 좌표에 아무렇게나 찍었을 때
그 점이 파란색 부분에 속할 확률을 구하면 (그 횟수는 충분해야할 것이다) 25x25x(확률)을 통해 해당 부분의 넓이를 구할 수 있을 것이다. 이 쉬워보이는 개념이 Monte Carlo Approximation이다. </br>
요약 : **sampling을 통해 추정하고자 하는 값에 근사한다**
</br></br>
## 2. 수식
<img src="https://t1.daumcdn.net/cfile/tistory/99B101345A4A0DD617" width="320" /> </br>
위 식은, 각 state에서 받을 수 있는 Return값들을 나타낸다.

또한, 앞서 배웠던 Dynamic Programming에서는 다음과 같은 식에 따라서 각 state의 value를 구했었다.
</br> </br> 
<a href="https://www.codecogs.com/eqnedit.php?latex=v(s)&space;=&space;E[R_{t&plus;1}&plus;\gamma&space;v(S_{t&plus;1})|S_t&space;=&space;s]" target="_blank"><img src="https://latex.codecogs.com/gif.latex?v(s)&space;=&space;E[R_{t&plus;1}&plus;\gamma&space;v(S_{t&plus;1})|S_t&space;=&space;s]" title="v(s) = E[R_{t+1}+\gamma v(S_{t+1})|S_t = s]" /></a>
</br> </br>
하지만 환경/모델에 대한 정보가 없어서 위와 같은 식을 구할 수 없을 때, 우리는 Monte Carlo Approximation 방법을 사용하여, 아래와 같이 Return들의 random sample값들을 평균내어 value function을 근사한다.
</br> </br> 
<a href="https://www.codecogs.com/eqnedit.php?latex=V_{\pi}(s)&space;:=&space;\frac{1}{N(s)}\sum_{i=1}^{N(s)}G_{i}(s)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?V_{\pi}(s)&space;:=&space;\frac{1}{N(s)}\sum_{i=1}^{N(s)}G_{i}(s)" title="V_{\pi}(s) := \frac{1}{N(s)}\sum_{i=1}^{N(s)}G_{i}(s)" /></a>
</br> </br>
위 식을 전개해보면, 다음과 같은 식을 얻을 수 있다.
</br></br> 
<img src="https://t1.daumcdn.net/cfile/tistory/9964F84F5A4A186235" width="400" />
</br> </br> 
이를 통해, 우리는 value function에 대한 updating equation을 다음과 같이 구할 수 있다.
</br> </br> 
<img src="https://t1.daumcdn.net/cfile/tistory/99CB534B5A4A1D852F" width="280" />
</br></br>
## 3. First Visit Method vs Every Visit Method
한번의 episode의 과정 속에서, agent는 하나의 state에 여러 번 방문을 할 수도 있다. 이럴 경우 해당 state에 대한 update가 이루어지는 방식에는 두 가지가 있다. </br> </br>
(1) First Visit Method </br>
한 번의 episode에서 해당 state를 **처음 방문**했을 때만 update가 이루어진다.</br></br>
(2) Every Visit Method </br>
한 번의 episode에서 해당 state를 **방문하는 모든 때**에 update가 이루어진다.</br> </br>

각각의 update가 이루어질 때 마다, Incremental Counter인 N(s)는 N(s) <- N(s)+1로, Incremental total return인 S(s)는 S(s) <- S(s)+G(t)로 바뀐다. 그리고 마지막으로 최종 value (=V(s))는 S(s)/N(s)로 계산할 수 있는데, 누적된 Return값의 합을 (First Visit Method의 경우) episode 횟수로, (Every Visit Method의 경우) 총 방문횟수로 나눠준다. 여기서 N(s)이 무한대로 가면, V(s)는 <a href="https://www.codecogs.com/eqnedit.php?latex=V_{\pi}(s)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?V_{\pi}(s)" title="V_{\pi}(s)" /></a>에 근사한다.
</br></br>
## 4. Monte Carlo Control
앞서 배운 Dynamic Programming에서, Policy Iteration은 크게 (1)policy evaluation과 (2)policy improvement로 나눠졌다. 여기서 (1)policy evaluation (각 state의 value function을 구하는 과정)에, 위에서 배운 Monte Carlo Approximation을 적용하면 **Monte Carlo Policy Iteration**이 된다. </br> 

하지만 Monte Carlo 방법에도 여전히 다음과 같은 2가지 문제점이 존재한다. </br></br>
(1) **Value Function** </br>
기존에 policy iteration에서는 policy evaluation 과정에서 value function을 다음과 같이 MDP과정을 통해 구할 수 있었다. </br> </br>
<a href="https://www.codecogs.com/eqnedit.php?latex=v(s)&space;=&space;R_s&plus;\gamma&space;\sum_{s'\in&space;S}^{&space;}P_{ss'}v(s')" target="_blank"><img src="https://latex.codecogs.com/gif.latex?v(s)&space;=&space;R_s&plus;\gamma&space;\sum_{s'\in&space;S}^{&space;}P_{ss'}v(s')" title="v(s) = R_s+\gamma \sum_{s'\in S}^{ }P_{ss'}v(s')" /></a>). </br> </br>
하지만, 환경/모델에 대한 정보가 없는 상황(앞의 식에서의 <a href="https://www.codecogs.com/eqnedit.php?latex=P_{ss'}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P_{ss'}" title="P_{ss'}" /></a>)에서는 위 식을 풀 수가 없다. </br>

(2) **Local Optimum** </br>
MC에서는 모든 state에 대한 정보를 가지고 있는 것이 아니라, sampling을 통해 정보를 모으기 때문에 local optimum에 빠지게 될 가능성이 있다.

그래서 이 두 문제를 해결하기 위해서 나온 것이 Monte Carlo Control이다. Monte Carlo Control은 (1)번 문제를, value function 대신 action-value function ( = Q value function )을 이용하여 해결하고, (2)번 문제를 해결하기 위해 exploration을 이용하여 local optimum에 빠지게 될 위험을 줄인다. 자세한 내용에 대해서는 뒤에서 더 다룬다.

### 1. Q-value Function 사용</br>
<img src="https://docplayer.net/docs-images/42/17229441/images/page_8.jpg" width="500" /></br>
( 출처 : https://docplayer.net/docs-images/42/17229441/images/page_8.jpg ) </br> </br>
value function ( V(s) )에서는 policy improvement를 위해서 위 식처럼 MDP의 모델을 필요로 했다. 하지만, value function 대신 action-value function( Q(s,a) )를 이용할 경우, 위 식과 같이 model-free해져서, 이 문제를 해결할 수 있다.

### 2. Exploration </br>
<img src="https://miro.medium.com/max/3024/1*u52Y2-Z_nPHLhqVsuIdugw.jpeg" width="500" /> </br>
( 출처 : https://miro.medium.com/max/3024/1*u52Y2-Z_nPHLhqVsuIdugw.jpeg ) </br></br>
MC는 전체가 아닌 일부 sample로만 진행을 하기 때문에 local optimum에 빠질 가능성이 존재한다. 이를 방지하기 위해, agent로 하여금 늘 최적의 선택( 즉, Q(s,a)를 최대화하는 Action )만을 하는 것(=exploitation)이 아니라, 가끔은 돌발적인 행동( 즉, 다른 길을 시도해서 다른 optimum이 있는지 더 탐색해라! )을 하게끔 하는데, 이를 exploration(탐험) 이라고 한다. Agent가 얼마나 exploitation과 exploration을 나눠서 할 지에 대한 값이 바로 <a href="https://www.codecogs.com/eqnedit.php?latex=\epsilon" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\epsilon" title="\epsilon" /></a>이다.

## 5. Summary
지금 까지 배운 Monte Carlo를 요약하면, 다음과 같다. </br>
<img src="https://i.stack.imgur.com/Dvwsb.png" width="500" /> </br>
( 출처 : https://i.stack.imgur.com/Dvwsb.png ) </br></br>
우선 value-function대신 action-value function을 사용한다는 점과, Monte Carlo Approximation 방법을 이용하여 일부를 sampling하여 진행한다는 점, 그리고 마지막으로 agent가 action을 선택할 때 <a href="https://www.codecogs.com/eqnedit.php?latex=\epsilon" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\epsilon" title="\epsilon" /></a>의 확률로 exploration을 하여 local optimum에 빠질 위험을 줄인다는 것이 이 방법의 핵심이라 할 수 있다.
