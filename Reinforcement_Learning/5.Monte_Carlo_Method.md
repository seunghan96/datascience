# 5. Monte Carlo Method

## 1. Monte Carlo Approximation
</br> </br> <img src="https://kr.mathworks.com/matlabcentral/mlc-downloads/downloads/submissions/65563/versions/2/screenshot.jpg" width="320" />  </br> </br>
(출처 : https://kr.mathworks.com/matlabcentral/mlc-downloads/downloads/submissions/65563/versions/2/screenshot.jpg)
위 그림 처럼 파란색 점과, 빨간색 점으로 이루어진 좌표를 보자. 여기서 우리는 파란색 점이 있는 부분의 넓이를 어떻게 구할까? y=x^2의 식으로 보여서,
적분을 통해서 쉽게 계산할 수 있겠지만, 확률을 이용하여 계산(엄밀히는 approximate)하는 방법이 있다. 임의의 점을 들어, 해당 좌표에 아무렇게나 찍었을 때
그 점이 파란색 부분에 속할 확률을 구하면 (그 횟수는 충분해야할 것이다) 25x25x(확률)을 통해 해당 부분의 넓이를 구할 수 있을 것이다. 이 쉬워보이는 개념이 Monte Carlo Approximation이다. </br>
요약 : **sampling을 통해 추정하고자 하는 값에 근사한다**

## 2. 수식
<img src="https://t1.daumcdn.net/cfile/tistory/99B101345A4A0DD617" width="320" />
</br>
위 식은, 각 state에서 받을 수 있는 Return값들을 나타낸다.

앞서 배웠던 Dynamic Programming에서는 다음과 같은 식에 따라서 각 state의 value를 구했었다.
</br> 
<a href="https://www.codecogs.com/eqnedit.php?latex=v(s)&space;=&space;E[R_{t&plus;1}&plus;\gamma&space;v(S_{t&plus;1})|S_t&space;=&space;s]" target="_blank"><img src="https://latex.codecogs.com/gif.latex?v(s)&space;=&space;E[R_{t&plus;1}&plus;\gamma&space;v(S_{t&plus;1})|S_t&space;=&space;s]" title="v(s) = E[R_{t+1}+\gamma v(S_{t+1})|S_t = s]" /></a>
</br> </br>
하지만 환경/모델에 대한 정보가 없어서 위와 같은 식을 구할 수 없을 때, 우리는 Monte Carlo Approximation 방법을 사용하여, 아래와 같이 Return들의 random sample값들을 평균내어 value function을 근사한다.
</br>
<a href="https://www.codecogs.com/eqnedit.php?latex=V_{\pi}(s)&space;:=&space;\frac{1}{N(s)}\sum_{i=1}^{N(s)}G_{i}(s)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?V_{\pi}(s)&space;:=&space;\frac{1}{N(s)}\sum_{i=1}^{N(s)}G_{i}(s)" title="V_{\pi}(s) := \frac{1}{N(s)}\sum_{i=1}^{N(s)}G_{i}(s)" /></a>
</br> </br>
위 식을 전개해보면, 다음과 같은 식을 얻을 수 있다.
</br>
<img src="https://t1.daumcdn.net/cfile/tistory/9964F84F5A4A186235" width="320" />
이를 통해, 우리는 value function에 대한 updating equation을 다음과 같이 구할 수 있다.
<img src="https://t1.daumcdn.net/cfile/tistory/99CB534B5A4A1D852F" width="320" />

## 3. First Visit Method vs Every Visit Method
한번의 episode의 과정 속에서, agent는 하나의 state에 여러 번 방문을 할 수도 있다. 이럴 경우 해당 state에 대한 update가 이루어지는 방식에는 두 가지가 있다. </br>
(1) First Visit Method </br>
한 번의 episode에서 해당 state를 '처음 방문'했을 때만 update가 이루어진다.</br>
(2) Every Visit Method </br>
한 번의 episode에서 해당 state를 '방문하는 모든 때'에 update가 이루어진다.</br> </br>

각각의 update가 이루어질 때 마다, N(s) <- N(s)+1, S(s) <- S(s)+G(t)로 바뀐다. 그리고 마지막으로 최종 value는 S(s)/N(s)로, 누적 된 Return
