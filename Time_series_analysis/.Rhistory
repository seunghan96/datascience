apply(flm^2,2,sum) # Eigenvalue = How many variables Factor i explains
bp = data.frame(fifa,fit$scores)
cor(fit$scores) # uncorrelated
names(bp)[42:46] = c("Basic","Defense","Goalkeeping","Physical","Speed")
newdata=bp[42:46]
head(newdata)
## K-means
k_number<-NULL
for (i in 2:10){
set.seed(48167)
k_number<-c(k_number,kmeans(newdata,centers=i)$tot.withinss)
}
plot(2:10,k_number,type="b") ## I would go with 5 segments
set.seed(48167)
clus5 = kmeans(newdata, 5)
clus5
clus5$centers # Center of each cluster
clus5$cluster # cluster solution.
clus5$withinss # Within group variation
newdata$kmean=clus5$cluster
newdata$id = fifa$Name
head(newdata)
plot(newdata$Basic,newdata$Defense,type="n",xlab="mental",ylab="hot")
text(newdata$Basic[newdata$kmean==1],newdata$Defense[newdata$kmean==1],labels=newdata$id[newdata$kmean==1])#,cex=.7,col=1)
plot(newdata$Basic)
plot(newdata$Basic[newdata$kmean==1],newdata$Defense[newdata$kmean==1])
plot(newdata$Basic[newdata$kmean==1],newdata$Defense[newdata$kmean==1],labels=newdata$id[newdata$kmean==1])
plot(newdata$Basic,newdata$Defense,type="n",xlab="mental",ylab="hot")
text(newdata$Basic[newdata$kmean==1],newdata$Defense[newdata$kmean==1],labels=newdata$id[newdata$kmean==1],cex=.7,col=1)
text(0,0,label='a')
plot(newdata$Basic,newdata$Defense,type="n",xlab="mental",ylab="hot")
text(newdata$Basic[newdata$kmean==1],newdata$Defense[newdata$kmean==1],labels=newdata$id[newdata$kmean==1],cex=.1,col=1)
plot(newdata$Basic,newdata$Defense,type="n",xlab="Basic Skills",ylab="Defense")
text(newdata$Basic[newdata$kmean==1],newdata$Defense[newdata$kmean==1],labels=newdata$id[newdata$kmean==1],cex=.1,col=1)
text(newdata$Basic[newdata$kmean==2],newdata$Defense[newdata$kmean==2],labels=newdata$id[newdata$kmean==2],cex=.1,col=2)
text(newdata$Basic[newdata$kmean==3],newdata$Defense[newdata$kmean==3],labels=newdata$id[newdata$kmean==3],cex=.1,col=3)
text(newdata$Basic[newdata$kmean==4],newdata$Defense[newdata$kmean==4],labels=newdata$id[newdata$kmean==4],cex=.1,col=4)
text(newdata$Basic[newdata$kmean==5],newdata$Defense[newdata$kmean==5],labels=newdata$id[newdata$kmean==5],cex=.1,col=4)
text(newdata$Basic[newdata$kmean==5],newdata$Defense[newdata$kmean==5],labels=newdata$id[newdata$kmean==5],cex=.1,col=5)
plot(newdata$Basic,newdata$Defense,type="n",xlab="Basic Skills",ylab="Defense")
text(newdata$Basic[newdata$kmean==1],newdata$Defense[newdata$kmean==1],labels=newdata$id[newdata$kmean==1],cex=.1,col=1)
text(newdata$Basic[newdata$kmean==2],newdata$Defense[newdata$kmean==2],labels=newdata$id[newdata$kmean==2],cex=.1,col=2)
text(newdata$Basic[newdata$kmean==3],newdata$Defense[newdata$kmean==3],labels=newdata$id[newdata$kmean==3],cex=.1,col=3)
text(newdata$Basic[newdata$kmean==4],newdata$Defense[newdata$kmean==4],labels=newdata$id[newdata$kmean==4],cex=.1,col=4)
text(newdata$Basic[newdata$kmean==5],newdata$Defense[newdata$kmean==5],labels=newdata$id[newdata$kmean==5],cex=.1,col=5)
plot(newdata$Basic,newdata$Defense,type="n",xlab="Basic Skills",ylab="Defense")
text(newdata$Basic[newdata$kmean==1],newdata$Defense[newdata$kmean==1],labels=newdata$id[newdata$kmean==1],cex=.1,col=1)
clus5$centers # Center of each cluster
clus5$centers # Center of each cluster
clus5$cluster # cluster solution.
count(clus5$cluster)
table(clus5$cluster)
newdata$kmean=clus5$cluster
newdata$id = fifa$Name
head(newdata)
describe(newdata)
summary(newdata$Goalkeeping)
bp = data.frame(fifa,fit$scores)
cor(fit$scores) # uncorrelated
names(bp)[42:46] = c("Basic","Defense","Goalkeeping","Physical","Speed")
newdata=bp[42:46]
head(newdata)
newdata$Goalkeeping = -newdata$Goalkeeping
## K-means
k_number<-NULL
for (i in 2:10){
set.seed(48167)
k_number<-c(k_number,kmeans(newdata,centers=i)$tot.withinss)
}
plot(2:10,k_number,type="b") ## I would go with 5 segments
set.seed(48167)
clus5 = kmeans(newdata, 5)
clus5
clus5$centers # Center of each cluster
table(clus5$cluster)
newdata$kmean=clus5$cluster
newdata$id = fifa$Name
head(newdata)
# 1. make correlation matrix with numeric features
cor_mat = cor(fifa[,-c(1,2,3)])
# 2. take 5 factors
# ( filtering by selecting factors with eigen values bigger than 1)
# those 5 factors has 83.5% explained variance ration
ev = eigen(cor_mat)
ev$values[ev$values>1]
cumsum(ev$values/length(ev$values))[1:5]
# 3. Fitting FA and assess model fit
fit = factanal(fifa[,-c(1,2,3)], 5, rotation="varimax", scores="regression")
print(fit$loadings, digits=2, cutoff=.4, sort=TRUE)
flm = as.matrix(fit$loadings[1:38,])
apply(flm^2,2,sum) # Eigenvalue = How many variables Factor i explains
bp = data.frame(fifa,fit$scores)
cor(fit$scores) # uncorrelated
names(bp)[42:46] = c("Basic","Defense","Goalkeeping","Physical","Speed")
newdata=bp[42:46]
# Goalkeeping feature : how they do NOT do well in goal keeping
# change the direction of the factor!
newdata$Goalkeeping = -newdata$Goalkeeping
## K-means
k_number<-NULL
for (i in 2:10){
set.seed(48167)
k_number<-c(k_number,kmeans(newdata,centers=i)$tot.withinss)
}
plot(2:10,k_number,type="b") ## I would go with 5 segments
set.seed(48167)
clus5 = kmeans(newdata, 5)
clus5
clus5
clus5
clus5$centers # Center of each cluster
table(clus5$cluster)
newdata$kmean=clus5$cluster
newdata$id = fifa$Name
head(newdata)
clus5$centers # Center of each cluster
newdata[newdata$Basic==0.77108945,]
newdata[(newdata$Basic<0.77108946),]
newdata[(newdata$Basic<0.77108946)&(newdata$Basic>0.77108944),]
table(clus5$cluster)
clus5$centers # Center of each cluster
cbind(clus5$centers,newdata)
clus5$centers # Center of each cluster
merge(clus5$centers,newdata,by.x=c('Basic','Defense','Goalkeeping'))
merge(clus5$centers,newdata,by=c('Basic','Defense','Goalkeeping'))
head(newdata)
merge(clus5$centers,newdata,by.x=c('Basic','Defense','Goalkeeping'),by.y=c('Basic','Defense','Goalkeeping'))
clus5$centers # Center of each cluster
merge(clus5$centers,newdata,by='basic')
merge(clus5$centers,newdata,by='Basic')
load("C:/Users/samsung/Desktop/연세대/2019.2학기/경영/마틱스/takehome final/FIFA.RData")
# 1. make correlation matrix with numeric features
cor_mat = cor(fifa[,-c(1,2,3)])
# 2. take 5 factors
# ( filtering by selecting factors with eigen values bigger than 1)
# those 5 factors has 83.5% explained variance ration
ev = eigen(cor_mat)
ev$values[ev$values>1]
cumsum(ev$values/length(ev$values))[1:5]
windows()
plot(cumsum(ev$values/length(ev$values)))
# 3. Fitting FA and assess model fit
fit = factanal(fifa[,-c(1,2,3)], 5, rotation="varimax", scores="regression")
print(fit$loadings, digits=2, cutoff=.4, sort=TRUE)
flm = as.matrix(fit$loadings[1:38,])
apply(flm^2,2,sum) # Eigenvalue = How many variables Factor i explains
bp = data.frame(fifa,fit$scores)
cor(fit$scores) # uncorrelated
names(bp)[42:46] = c("Basic","Defense","Goalkeeping","Physical","Speed")
newdata=bp[42:46]
# Goalkeeping feature : how they do NOT do well in goal keeping
# change the direction of the factor!
newdata$Goalkeeping = -newdata$Goalkeeping
## K-means
k_number<-NULL
for (i in 2:10){
set.seed(48167)
k_number<-c(k_number,kmeans(newdata,centers=i)$tot.withinss)
}
plot(2:10,k_number,type="b") ## I would go with 5 segments
set.seed(48167)
clus5 = kmeans(newdata, 5)
clus5$centers # Center of each cluster
table(clus5$cluster)
newdata$kmean=clus5$cluster
newdata$id = fifa$Name
head(newdata)
plot(newdata$Basic,newdata$Defense,type="n",xlab="Basic Skills",ylab="Defense")
text(newdata$Basic[newdata$kmean==1],newdata$Defense[newdata$kmean==1],labels=newdata$id[newdata$kmean==1],cex=.1,col=1)
text(newdata$Basic[newdata$kmean==2],newdata$Defense[newdata$kmean==2],labels=newdata$id[newdata$kmean==2],cex=.1,col=2)
text(newdata$Basic[newdata$kmean==3],newdata$Defense[newdata$kmean==3],labels=newdata$id[newdata$kmean==3],cex=.1,col=3)
text(newdata$Basic[newdata$kmean==4],newdata$Defense[newdata$kmean==4],labels=newdata$id[newdata$kmean==4],cex=.1,col=4)
text(newdata$Basic[newdata$kmean==5],newdata$Defense[newdata$kmean==5],labels=newdata$id[newdata$kmean==5],cex=.1,col=5)
legend("topleft",c("에이스","동네축구","평범선수","수비형",'공격형'),pch=8,col=c(1,2,3,4,5))
# 1. make correlation matrix with numeric features
cor_mat = cor(fifa[,-c(1,2,3)])
# 2. take 5 factors
# ( filtering by selecting factors with eigen values bigger than 1)
# those 5 factors has 83.5% explained variance ration
ev = eigen(cor_mat)
ev$values[ev$values>1]
cumsum(ev$values/length(ev$values))[1:5]
load("C:/Users/samsung/Desktop/연세대/2019.2학기/경영/마틱스/takehome final/FIFA.RData")
# 1. make correlation matrix with numeric features
cor_mat = cor(fifa[,-c(1,2,3)])
# 2. take 5 factors
# ( filtering by selecting factors with eigen values bigger than 1)
# those 5 factors has 83.5% explained variance ration
ev = eigen(cor_mat)
ev$values[ev$values>1]
cumsum(ev$values/length(ev$values))[1:5]
# 3. Fitting FA and assess model fit
fit = factanal(fifa[,-c(1,2,3)], 5, rotation="varimax", scores="regression")
print(fit$loadings, digits=2, cutoff=.4, sort=TRUE)
flm = as.matrix(fit$loadings[1:38,])
apply(flm^2,2,sum) # Eigenvalue = How many variables Factor i explains
bp = data.frame(fifa,fit$scores)
cor(fit$scores) # uncorrelated
names(bp)[42:46] = c("Basic","Defense","Goalkeeping","Physical","Speed")
newdata=bp[42:46]
# Goalkeeping feature : how they do NOT do well in goal keeping
# change the direction of the factor!
newdata$Goalkeeping = -newdata$Goalkeeping
## K-means
k_number<-NULL
for (i in 2:10){
set.seed(48167)
k_number<-c(k_number,kmeans(newdata,centers=i)$tot.withinss)
}
plot(2:10,k_number,type="b") ## I would go with 5 segments
set.seed(48167)
clus5 = kmeans(newdata, 5)
clus5$centers # Center of each cluster
table(clus5$cluster)
newdata$kmean=clus5$cluster
newdata$id = fifa$Name
head(newdata)
plot(newdata$Basic,newdata$Defense,type="n",xlab="Basic Skills",ylab="Defense")
text(newdata$Basic[newdata$kmean==1],newdata$Defense[newdata$kmean==1],labels=newdata$id[newdata$kmean==1],cex=.1,col=1)
text(newdata$Basic[newdata$kmean==2],newdata$Defense[newdata$kmean==2],labels=newdata$id[newdata$kmean==2],cex=.1,col=2)
text(newdata$Basic[newdata$kmean==3],newdata$Defense[newdata$kmean==3],labels=newdata$id[newdata$kmean==3],cex=.1,col=3)
text(newdata$Basic[newdata$kmean==4],newdata$Defense[newdata$kmean==4],labels=newdata$id[newdata$kmean==4],cex=.1,col=4)
text(newdata$Basic[newdata$kmean==5],newdata$Defense[newdata$kmean==5],labels=newdata$id[newdata$kmean==5],cex=.1,col=5)
legend("topleft",c("에이스","동네축구","평범선수","수비형",'공격형'),pch=8,col=c(1,2,3,4,5))
newdata[newdata$kmean==1]
library(forecast)
library(ggplot2)
gs = read.csv('gs_ts.csv')
# 3. Decomposition
library(forecast)
library(ggplot2)
gs = read.csv('gs_ts.csv')
gs = read.csv('C:\Users\samsung\Desktop\datascience\Time_series_analysis\gs_ts.csv')
gs = read.csv('C:\\Users\\samsung\\Desktop\\datascience\\Time_series_analysis\\gs_ts.csv')
gs_price = ts(gs$price,frequency=250)
plot(decompose(gs_price))
## holt winters
gs_hw = HoltWinters(gs_price)
plot(gs_hw)
forecast(gs_hw,h=10)
# decomposition plot
is.list(gs_hw)
# decomposition plot
attributes(gs_hw)
str(gs_hw)
# decomposition plot
class(gs_hw)
is.list(gs_hw)
str(gs_hw)
gs_hw_fitted = gs_hw$fitted[,1:4]
gs_hw_fitted
colnames(gs_hw_fitted)
plot(gs_hw_fitted)
plot(gs_hw_fitted,main='Decomposition of Time series (Holt Winters)')
## 2) STL(Seasonal Decomposition of Time Series by LOESS)
# smoothing method -> HW : Exponential <-> STL : LOESS (local regression)
# locally weighted polynomial regression
gs_stl = stl(gs_price, s.window='periodic')
plot(gs_stl)
str(gs_stl)
gs_stl$time.series[,1] # seasonality
stl_fcst = forecast(gs_stl, h=100)
plot(stl_fcst, main= 'Forecast by STL')
# 4. Timeseries Forecasting Model
library(forecast)
AP = AirPassengers
dim(AP)
length(AP)
int(length(AP)*0.7)
round(length(AP)*0.7)
round(length(AP)*0.8)
round(length(AP)*0.85)
index = round(length(AP)*0.85)
train = AP[1:index]
test = AP[index:]
test = AP[index:,]
test = AP[index:length(AP)]
test
y = ts(train,frequency=12)
# 1) Exponential Moving Average
fit = HoltWinters(y, beta=False, gamma=False)
# 1) Exponential Moving Average
hw_fit = HoltWinters(y, beta=False, gamma=False)
hw_fc = forecast(hw_fit, h=12)
# 1) Exponential Moving Average
hw_fit = HoltWinters(y, beta=FALSE, gamma=FALSE)
hw_fc = forecast(hw_fit, h=12)
plot(hw_fc, main='Exponential MA Forecast')
hw_fc2 = forecast(hw_fit2, h=12)
# 2) Exponential MA 2
hw_fit2 = HoltWinters(y, alpha=0.1, beta=FALSE, gamma=FALSE)
hw_fc2 = forecast(hw_fit2, h=12)
plot(hw_fc2, main='Exponential MA Forecast 2')
# 1 - MAPE
mean(100 - 100*abs(as.numeric(fc$mean)-as.numeric(test)) / as.numeric(test))
# 1 - MAPE
mean(100 - 100*abs(as.numeric(hw_fc$mean)-as.numeric(test)) / as.numeric(test))
# 1 - MAPE
mean(100 - 100*abs(as.numeric(hw_fc$mean)-as.numeric(test)) / as.numeric(test))
hw2_mape = mean(100 - 100*abs(as.numeric(hw_fc2$mean)-as.numeric(test)) / as.numeric(test))
test
as.numeric(test)
hw1_mape
# 1 - MAPE
hw1_mape = mean(100 - 100*abs(as.numeric(hw_fc$mean)-as.numeric(test)) / as.numeric(test))
hw2_mape = mean(100 - 100*abs(as.numeric(hw_fc2$mean)-as.numeric(test)) / as.numeric(test))
hw1_mape
hw2_mape
# 3) Holt Winters
hw_fit = hw(y_
# 3) Holt Winters
hw_fit = hw(y)
# 3) Holt Winters
hw_fit = hw(y)
hw_fc = forecast(hw_fit, h=12)
plot(hw_fc, main='Holt Winters Forecast')
hw_mape = mean(100 - 100*abs(as.numeric(hw_fc$mean)-as.numeric(test)) / as.numeric(test))
hw_mape
ex1_mape, ex2_mape, hw_mape
ex1_mape
ex2_mape
# 4) Holt Winters 2    # parameter estimated by MSE
hw_fit2 = Holtwinters(y)
hw_fc2 = forecast(hw_fit2, h=12)
plot(hw_fc2, main='Holt Winters Forecast 2')
# 4) Holt Winters 2    # parameter estimated by MSE
hw_fit2 = HoltWinters(y)
hw_fc2 = forecast(hw_fit2, h=12)
plot(hw_fc2, main='Holt Winters Forecast 2')
hw2_mape = mean(100 - 100*abs(as.numeric(hw_fc2$mean)-as.numeric(test)) / as.numeric(test))
hw2_mape
stl_fc = forecast(stl_fit,h=12)
# 5) STL
stl_fit = stl(y,s.window='per')
stl_fc = forecast(stl_fit,h=12)
plot(stl_fc, main='STL Forecast')
stl_mape = mean(100 - 100*abs(as.numeric(stl_fc$mean)-as.numeric(test)) / as.numeric(test))
stl_mape
ex1_mape ; ex2_mape ; hw_mape ; hw2_mape ; stl_mape
z = read.csv('pounds_nz.csv')
z = read.csv('C:\\Users\\samsung\\Desktop\\datascience\\Time_series_analysis\\pounds_nz.csv')
dim(Z)
dim(z)
dim(z)[1]
train = z[1:35]
test = z[36:39]
y = ts(train, frequency=4)
#### 2. Modeling ####
fit1 = HoltWinters(y, beta=False, gamma=False)
fc1 = forecast(fit1, h=4)
#### 2. Modeling ####
fit1 = HoltWinters(y, beta=FALSE, gamma=FALSE)
fc1 = forecast(fit1, h=4)
plot(fc1, main='Exponential MA')
# 2) Holt Winters
fit2 = hw(y)
fc2 = forecast(fit2,h=4)
plot(fc2, main='Holt Winters')
# 3) STL
fit3 = stl(y, s.window='periodic')
fc3 = forecast(fit3, h=4)
plot(fc3, main='STL')
par(mfrow=c(3,1))
#### 2. Modeling ####
# 1) Exponential MA
fit1 = HoltWinters(y, beta=FALSE, gamma=FALSE)
fc1 = forecast(fit1, h=4)
plot(fc1, main='Exponential MA')
# 2) Holt Winters
fit2 = hw(y)
fc2 = forecast(fit2,h=4)
plot(fc2, main='Holt Winters')
# 3) STL
fit3 = stl(y, s.window='periodic')
fc3 = forecast(fit3, h=4)
plot(fc3, main='STL')
#### 3. Evaluation ####
# 1 - MAPE
mape1 = mean(100 - 100*abs(as.numeric(fc1$mean)-as.numeric(test)) / as.numeric(test))
mape2 = mean(100 - 100*abs(as.numeric(fc2$mean)-as.numeric(test)) / as.numeric(test))
mape3 = mean(100 - 100*abs(as.numeric(fc3$mean)-as.numeric(test)) / as.numeric(test))
mape1;mape2;mape3
## white noise
set.seed(1234)
wn = rnorm(1000)
ts.plot(wn)
## stationarity
library(forecast)
library(tseries)
## white noise
set.seed(1234)
wn = rnorm(1000)
ts.plot(wn)
# random walk ( Non-stationarity ts )
set.seed(1234)
x = rnorm(1)
w = rnorm(1000)
for (t in 2:1000) {x[t] = x[t-1]+w[t]}
x
ts.plot(x)
# 비정상 시계열을 정상 시계열로
# for ARIMA
# by differencing(차분) -> trend 없앰
par(mfrow=c(2,1))
plot(AirPassengers)
plot(diff(AirPassengers))
## 2) Stabilizing Variance
## by Log Transformation -> variance 줄임
par(mfrow=c(3,1))
plot(AirPassengers)
plot(diff(AirPassengers))
plot(log(diff(AirPassengers)))
## 3) acf : Auto Correlation Function
## 시계열의 자기상관을 계
par(mfrow=c(2,1))
plot(AirPassengers)
plot(acf(log(diff(AirPassengers))))
plot(acf(log(diff(AirPassengers))))
plot(acf(diff(log(AirPassengers))))
## 3) acf : Auto Correlation Function
## 시계열의 자기상관을 계
par(mfrow=c(2,1))
plot(AirPassengers)
plot(acf(diff(log(AirPassengers))))
## 2) Stabilizing Variance
## by Log Transformation -> variance 줄임
par(mfrow=c(3,1))
plot(AirPassengers)
plot(diff(AirPassengers))
plot(diff(log(AirPassengers)))
## 3) acf : Auto Correlation Function
## 시계열의 자기상관을 계
par(mfrow=c(2,1))
plot(AirPassengers)
plot(acf(diff(log(AirPassengers))))
## 3) acf : Auto Correlation Function
## 시계열의 자기상관을 계산!
par(mfrow=c(2,1))
plot(acf(AirPassengers))
plot(acf(diff(log(AirPassengers))))
## 3) acf : Auto Correlation Function
## 시계열의 자기상관을 계산!
par(mfrow=c(2,1))
plot(acf(AirPassengers))
plot(acf(diff(log(AirPassengers))))
library(forecast)
library(tseries)
## 1) simulate AR(1), phi=0.9
set.seed(12345)
ar1 = arima.sim(n=1000,list=(ar=c(0.9)))
ts.plot(ar1)
library(tseries)
## 1) simulate AR(1), phi=0.9
set.seed(12345)
ar1 = arima.sim(n=1000,list=(ar=c(0.9)))
ar1 = arima.sim(n=1000,list(ar=c(0.9)))
ts.plot(ar1)
## 1) simulate AR(1), phi=0.9
set.seed(12345)
ar1 = arima.sim(n=1000,list(ar=c(0.9)))
ts.plot(ar1)
eps = rnorm(1000)
eps2 = rnorm(1000)
y = eps
x = eps2
a = c(1,2,3)
b = a
b[3] = 4
b
a
a[3] = 5
b
for (t in 2:1000) {
y[t] = y[t-1] + eps[t]
x[t] = 1.05*x[t-1] + eps2[t]
}
eps
head(eps)
head(y)
head(eps2)
head(x)
par(mfrow=c(2,2))
par(mfrow=c(2,2))
plot(y[1:40], type='l', main='a=1')
plot(x[1:40], type='l', main='a=1.05')
plot(y, type='l', main='a=1')
plot(x, type='l', main='a=1.05')
## 2) Unit Root Test
# 시계열이 정상성인지 확인
# unit root를 가지면 비정상(non Stationary) ( p value 작아서 가설 기각 시, unit root 비존 -> 정상성 시계열 )
# phi < 1 : stationary
# phi = 1 : random walk
# phi > 1 : explode
adf.test(AirPassengers)
## should difference first!
adf.test(diff(AirPassengers))
library(forecast)
library(tseries)
plot(AirPassengers)
par(mfrow=c(2,1))
acf(AirPassengers)
pacf(AirPassengers)
## Example
library(forecast)
## Example
setwd('C:\\Users\\samsung\\Desktop\\datascience\\Time_series_analysis')
library(forecast)
sales = read/csv('sales_s1.csv')
sales = read.csv('sales_s1.csv')
sales_ms = sales[1:1097]
sales_ms = sales$total[1:1097]
sales_ts = sales$total[1098:1127]
