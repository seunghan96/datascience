{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back Propagation 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. define function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.0001\n",
    "\n",
    "def _t(x):\n",
    "    return np.transpose(x)\n",
    "\n",
    "def _m(A,B):\n",
    "    return np.matmul(A,B)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def mse(h,y):\n",
    "    return 1/(2*np.mean(np.square(h-y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self,W,b,a):\n",
    "        # Model Parameter\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.a = a\n",
    "        \n",
    "        # Gradients\n",
    "        self.dW = np.zeros_like(self.W)\n",
    "        self.db = np.zeros_like(self.b)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.a(_m(_t(self.W),x) + self.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. DNN ( Deep Neural Network )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN:\n",
    "    def __init__(self,hidden_depth, num_neuron, num_input, num_output, activation=sigmoid):\n",
    "        def init_var(i,o):\n",
    "            return np.random.normal(0.0, 0.01, (i,o)), np.zeros((o,)) # initialize W & b\n",
    "        \n",
    "        self.sequence = list()\n",
    "        \n",
    "        # 1st Hidden Layer\n",
    "        W,b = init_var(num_input,num_neuron)\n",
    "        self.sequence.append(Neuron(W,b,activation))\n",
    "        \n",
    "        # 2nd~last Hidden Layer\n",
    "        for _ in range(hidden_depth - 1):\n",
    "            W,b = init_var(num_neuron,num_neuron)\n",
    "            self.sequence.append(Neuron(W,b,activation))\n",
    "        \n",
    "        # Output Layer\n",
    "        W,b = init_var(num_neuron,num_output)\n",
    "        self.sequence.append(Neuron(W,b,activation))\n",
    "        \n",
    "    def __call__(self,x):\n",
    "        for layer in self.sequence:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def calc_grad(self,x,y,loss_func):\n",
    "        def get_new_sequence(layer_index, new_neuron):\n",
    "            new_sequence = list()\n",
    "            for i,layer in enumerate(self.sequence):\n",
    "                if i == layer_index :\n",
    "                    new_sequence.append(new_neuron) # change\n",
    "                else :\n",
    "                    new_sequence.append(layer) # not change\n",
    "            return new_sequence\n",
    "        \n",
    "        def eval_sequence(x,sequence):\n",
    "            for layer in sequence:\n",
    "                x = layer(x)\n",
    "            return x\n",
    "        \n",
    "        loss = loss_func(self(x), y)\n",
    "        \n",
    "        for layer_id, layer in enumerate(self.sequence):\n",
    "            for w_i, w in enumerate(layer.W): # iterate W (row)\n",
    "                for w_j, ww in enumerate(w): # iterate W (col)\n",
    "                    W = np.copy(layer.W)\n",
    "                    W[w_i][w_j] = ww + epsilon\n",
    "                    \n",
    "                    new_neuron = Neuron(W,layer.b,layer.a)\n",
    "                    new_seq = get_new_sequence(layer_id, new_neuron)\n",
    "                    h = eval_sequence(x,new_seq)\n",
    "                    \n",
    "                    num_grad = (loss_func(h,y) - loss) / epsilon # f(x+eps)-f(x) / eps\n",
    "                    layer.dW[w_i][w_j] = num_grad\n",
    "                \n",
    "                for b_i, bb in enumerate(layer.b):\n",
    "                    b = np.copy(layer.b)\n",
    "                    b[b_i] = bb + epsilon\n",
    "                    \n",
    "                    new_neuron = Neuron(layer.W, b,layer.a)\n",
    "                    new_seq = get_new_sequence(layer_id, new_neuron)\n",
    "                    h = eval_sequence(x,new_seq)\n",
    "                    \n",
    "                    num_grad = (loss_func(h,y) - loss) / epsilon # f(x+eps)-f(x) / eps\n",
    "                    layer.db[b_i] = num_grad\n",
    "                    \n",
    "            return loss                \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(network, x, y, loss_obj, alpha=0.01):\n",
    "    loss = network.calc_grad(x,y,loss_obj)\n",
    "    for layer in network.sequence:\n",
    "        layer.W += -alpha*layer.dW\n",
    "        layer.b += -alpha*layer.db\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 : Test Loss 2.2755415869729543\n",
      "Epoch 1 : Test Loss 2.2755415869729543\n",
      "Epoch 2 : Test Loss 2.2755415869729543\n",
      "Epoch 3 : Test Loss 2.2755415869729543\n",
      "Epoch 4 : Test Loss 2.2755415869729543\n",
      "Epoch 5 : Test Loss 2.2755415869729543\n",
      "Epoch 6 : Test Loss 2.2755415869729543\n",
      "Epoch 7 : Test Loss 2.2755415869729543\n",
      "Epoch 8 : Test Loss 2.2755415869729543\n",
      "Epoch 9 : Test Loss 2.2755415869729543\n",
      "Epoch 10 : Test Loss 2.2755415869729543\n",
      "Epoch 11 : Test Loss 2.2755415869729543\n",
      "Epoch 12 : Test Loss 2.2755415869729543\n",
      "Epoch 13 : Test Loss 2.2755415869729543\n",
      "Epoch 14 : Test Loss 2.2755415869729543\n",
      "Epoch 15 : Test Loss 2.2755415869729543\n",
      "Epoch 16 : Test Loss 2.2755415869729543\n",
      "Epoch 17 : Test Loss 2.2755415869729543\n",
      "Epoch 18 : Test Loss 2.2755415869729543\n",
      "Epoch 19 : Test Loss 2.2755415869729543\n",
      "Epoch 20 : Test Loss 2.2755415869729543\n",
      "Epoch 21 : Test Loss 2.2755415869729543\n",
      "Epoch 22 : Test Loss 2.2755415869729543\n",
      "Epoch 23 : Test Loss 2.2755415869729543\n",
      "Epoch 24 : Test Loss 2.2755415869729543\n",
      "Epoch 25 : Test Loss 2.2755415869729543\n",
      "Epoch 26 : Test Loss 2.2755415869729543\n",
      "Epoch 27 : Test Loss 2.2755415869729543\n",
      "Epoch 28 : Test Loss 2.2755415869729543\n",
      "Epoch 29 : Test Loss 2.2755415869729543\n",
      "Epoch 30 : Test Loss 2.2755415869729543\n",
      "Epoch 31 : Test Loss 2.2755415869729543\n",
      "Epoch 32 : Test Loss 2.2755415869729543\n",
      "Epoch 33 : Test Loss 2.2755415869729543\n",
      "Epoch 34 : Test Loss 2.2755415869729543\n",
      "Epoch 35 : Test Loss 2.2755415869729543\n",
      "Epoch 36 : Test Loss 2.2755415869729543\n",
      "Epoch 37 : Test Loss 2.2755415869729543\n",
      "Epoch 38 : Test Loss 2.2755415869729543\n",
      "Epoch 39 : Test Loss 2.2755415869729543\n",
      "Epoch 40 : Test Loss 2.2755415869729543\n",
      "Epoch 41 : Test Loss 2.2755415869729543\n",
      "Epoch 42 : Test Loss 2.2755415869729543\n",
      "Epoch 43 : Test Loss 2.2755415869729543\n",
      "Epoch 44 : Test Loss 2.2755415869729543\n",
      "Epoch 45 : Test Loss 2.2755415869729543\n",
      "Epoch 46 : Test Loss 2.2755415869729543\n",
      "Epoch 47 : Test Loss 2.2755415869729543\n",
      "Epoch 48 : Test Loss 2.2755415869729543\n",
      "Epoch 49 : Test Loss 2.2755415869729543\n",
      "Epoch 50 : Test Loss 2.2755415869729543\n",
      "Epoch 51 : Test Loss 2.2755415869729543\n",
      "Epoch 52 : Test Loss 2.2755415869729543\n",
      "Epoch 53 : Test Loss 2.2755415869729543\n",
      "Epoch 54 : Test Loss 2.2755415869729543\n",
      "Epoch 55 : Test Loss 2.2755415869729543\n",
      "Epoch 56 : Test Loss 2.2755415869729543\n",
      "Epoch 57 : Test Loss 2.2755415869729543\n",
      "Epoch 58 : Test Loss 2.2755415869729543\n",
      "Epoch 59 : Test Loss 2.2755415869729543\n",
      "Epoch 60 : Test Loss 2.2755415869729543\n",
      "Epoch 61 : Test Loss 2.2755415869729543\n",
      "Epoch 62 : Test Loss 2.2755415869729543\n",
      "Epoch 63 : Test Loss 2.2755415869729543\n",
      "Epoch 64 : Test Loss 2.2755415869729543\n",
      "Epoch 65 : Test Loss 2.2755415869729543\n",
      "Epoch 66 : Test Loss 2.2755415869729543\n",
      "Epoch 67 : Test Loss 2.2755415869729543\n",
      "Epoch 68 : Test Loss 2.2755415869729543\n",
      "Epoch 69 : Test Loss 2.2755415869729543\n",
      "Epoch 70 : Test Loss 2.2755415869729543\n",
      "Epoch 71 : Test Loss 2.2755415869729543\n",
      "Epoch 72 : Test Loss 2.2755415869729543\n",
      "Epoch 73 : Test Loss 2.2755415869729543\n",
      "Epoch 74 : Test Loss 2.2755415869729543\n",
      "Epoch 75 : Test Loss 2.2755415869729543\n",
      "Epoch 76 : Test Loss 2.2755415869729543\n",
      "Epoch 77 : Test Loss 2.2755415869729543\n",
      "Epoch 78 : Test Loss 2.2755415869729543\n",
      "Epoch 79 : Test Loss 2.2755415869729543\n",
      "Epoch 80 : Test Loss 2.2755415869729543\n",
      "Epoch 81 : Test Loss 2.2755415869729543\n",
      "Epoch 82 : Test Loss 2.2755415869729543\n",
      "Epoch 83 : Test Loss 2.2755415869729543\n",
      "Epoch 84 : Test Loss 2.2755415869729543\n",
      "Epoch 85 : Test Loss 2.2755415869729543\n",
      "Epoch 86 : Test Loss 2.2755415869729543\n",
      "Epoch 87 : Test Loss 2.2755415869729543\n",
      "Epoch 88 : Test Loss 2.2755415869729543\n",
      "Epoch 89 : Test Loss 2.2755415869729543\n",
      "Epoch 90 : Test Loss 2.2755415869729543\n",
      "Epoch 91 : Test Loss 2.2755415869729543\n",
      "Epoch 92 : Test Loss 2.2755415869729543\n",
      "Epoch 93 : Test Loss 2.2755415869729543\n",
      "Epoch 94 : Test Loss 2.2755415869729543\n",
      "Epoch 95 : Test Loss 2.2755415869729543\n",
      "Epoch 96 : Test Loss 2.2755415869729543\n",
      "Epoch 97 : Test Loss 2.2755415869729543\n",
      "Epoch 98 : Test Loss 2.2755415869729543\n",
      "Epoch 99 : Test Loss 2.2755415869729543\n",
      "11.173606634140015 seconds elapsed.\n"
     ]
    }
   ],
   "source": [
    "x = np.random.normal(0.0, 1.0, (10,))\n",
    "y = np.random.normal(0.0,1.0, (2,))\n",
    "\n",
    "dnn = DNN(hidden_depth=5, num_neuron=32, num_input=10, num_output=2, activation=sigmoid)\n",
    "\n",
    "t = time.time()\n",
    "for epoch in range(100):\n",
    "    loss = gradient_descent(dnn,x,y,mse,0.01)\n",
    "    print('Epoch {} : Test Loss {}'.format(epoch, loss))\n",
    "\n",
    "print('{} seconds elapsed.'.format(time.time()-t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TOO LONG TIME... Back Propagation을 쓸 경우?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
